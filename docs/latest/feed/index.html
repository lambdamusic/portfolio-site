<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>The news feed of www.michelepasin.org</title><link>http://www.michelepasin.org/words/</link><description>Latest articles, blogs posts and news</description><atom:link href="http://127.0.0.1:8000/latest/feed/" rel="self"></atom:link><language>en-us</language><lastBuildDate>Tue, 07 Sep 2021 00:00:00 +0000</lastBuildDate><item><title>World date from the terminal</title><link>https://www.michelepasin.org/blog/2021/10/12/world-date-terminal/</link><description>
A little Bash script to show information about world time zones. Because I love my colleagues abroad, but I constantly struggle to remember how many hours ahead (or behind?) they are.

It basically scans the `zoneinfo` data already existing on your Mac and return the rows matching an input string.

&lt;script src="https://gist.github.com/lambdamusic/4a5b852c8f9051b1f29033619cd38e36.js"&gt;&lt;/script&gt;


NOTE On my Mac I found the `zoneinfo` data in this location:

`/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/pytz/zoneinfo/`

That path is needed for the script to work. You can customise it of course, see below how to do it. 


## Set up

Save the file above eg as `wdate` and then make it executable

```bash
chmod +x wdate
```

Then you can run it eg

```bash
$ wdate australia
/Australia/Melbourne               Tue 2021-10-12 20:47:48
/Australia/Queensland              Tue 2021-10-12 19:47:48
/Australia/North                   Tue 2021-10-12 19:17:48
/Australia/Lor ...</description><pubDate>Tue, 07 Sep 2021 00:00:00 +0000</pubDate><guid>https://www.michelepasin.org/blog/2021/10/12/world-date-terminal/</guid></item><item><title>Extempore functions explorer updated to latest release (v0.8.7)</title><link>https://www.michelepasin.org/blog/2021/02/01/extempore-functions-explorer-updated-to-latest-release-v0-8-7/</link><description>
The Extempore functions explorer has been updated with the latest version of the Extempore programming language: [v0.8.7](https://github.com/digego/extempore/tree/v0.8.7)

&gt; Try it out at: [http://hacks2019.michelepasin.org/extempore/](http://hacks2019.michelepasin.org/extempore/)

The Extempore functions explorer is a little webapp I built [a while ago](http://www.michelepasin.org/projects/impromptudocs/) in order to make it easier to browse (and learn about) the Extempore programming language.

[![demo](/media/static/blog_img/xtm-explorer.gif)](/media/static/blog_img/xtm-explorer.gif)


### What is Extempore?

[Extempore](https://extemporelang.github.io/) is a programming language and runtime environment designed by [Andrew Sorensen](https://twitter.com/digego?lang=en) to support livecoding and cyberphysical programming, where a human programmer operates as an active agent in the world.

&lt;iframe width="560" height="315" src="https://www.youtube.com/embed/yY1FSsUV-8c?controls=0" titl ...</description><pubDate>Mon, 01 Feb 2021 00:00:00 +0000</pubDate><guid>https://www.michelepasin.org/blog/2021/02/01/extempore-functions-explorer-updated-to-latest-release-v0-8-7/</guid></item><item><title>'The Kryos Noise' is available on Spotify</title><link>https://www.michelepasin.org/blog/2021/01/22/the-kryos-noise-is-available-on-spotify/</link><description>
The [prog rock](https://en.wikipedia.org/wiki/Progressive_rock) album I've worked on [years ago](http://www.michelepasin.org/sounds/?k=kryos) with the band Kryos Project is now available also on [Spotify](https://open.spotify.com/album/1uCLwvX24IFPp9g9SVoHqh) (and [Amazon](https://www.amazon.com/gp/product/B08SKXH1BW/?tag=distrokid06-20) too).

Why? Well it just feels good to be able to open up Spotify and _listen to your own music._ This is stuff we've made almost 20 years ago (!) but it still feels kinda relevant. Fresh. Well.. you know what I mean.

&gt; Check out [The Kryos' Noise on Spotify](https://open.spotify.com/album/1uCLwvX24IFPp9g9SVoHqh).

[![](/media/static/blog_img/Kryos-Noise-Cover.jpg)](https://distrokid.com/hyperfollow/kryosproject/the-kryos-noise)


### Cool. How do I get my band too on Spotify? 

It was surprisingly easy actually, if you don't mind spending a little money (around 20 dollars per year).

I used [Distrokid](https://distrokid.com/) to handle the distribut ...</description><pubDate>Fri, 22 Jan 2021 00:00:00 +0000</pubDate><guid>https://www.michelepasin.org/blog/2021/01/22/the-kryos-noise-is-available-on-spotify/</guid></item><item><title>A new livecoding project: 'The Musical Code'</title><link>https://www.michelepasin.org/blog/2020/11/23/a-new-livecoding-project-the-musical-code/</link><description>
I've started a new *livecoding* project on Github called [The Musical Code](https://github.com/lambdamusic/The-Musical-Code). Plan is to add experimental musical code/algorithms created via the amazing [Extempore](https://extemporelang.github.io/) programming language (as well as it precursor [Impromptu](http://impromptu.moso.com.au/)).

Background: I have accumulated *so much* musical-code ideas over the years... so I've finally resolved to clean it up, reorganise it and publish it somewhere. 

[Github](https://github.com/lambdamusic/The-Musical-Code) seemed the best option, these days.

[![](/media/static/blog_img/TheMusicalCodeGithub-1024x757.jpg)](https://github.com/lambdamusic/The-Musical-Code)

## Not just code

I soon realised that just **the code by itself won't do it**. Especially considering that the environments I used to 'run it' (and to make it go 'beep') could rapidly disappear: become obsolete, or get out of fashion!

Hence there's a [YouTube channel](https://www.youtub ...</description><pubDate>Mon, 23 Nov 2020 00:00:00 +0000</pubDate><guid>https://www.michelepasin.org/blog/2020/11/23/a-new-livecoding-project-the-musical-code/</guid></item><item><title>More Jupyter notebooks: pyvis and networkx</title><link>https://www.michelepasin.org/blog/2020/08/06/more-jupyter-notebooks-pyvis-and-networkx/</link><description>
&gt; Lately I've been spending more time creating Jupyter notebooks that demonstrate how to use the [Dimensions API for research analytics.](https://api-lab.dimensions.ai/) In this post I'll talk a little bit about two cool Python libraries I've recenlty discovered for working with **graph** data: **pyvis** and **networkx**.

### pyvis and networkx

The [networkx](https://networkx.github.io/documentation/stable/reference/introduction.html) and [pyvis](https://pyvis.readthedocs.io/en/latest/tutorial.html) libraries are used for _generating_ and _visualizing_ network data, respectively.

**Pyvis** is fundamentally a python wrapper around the popular [Javascript visJS library.](https://visjs.github.io/vis-network/examples/) 
**Networkx**, instead, of is a pretty sophisticated package for the creation, manipulation, and study of the structure, dynamics, and functions of complex networks.


```python
&gt;&gt;&gt; from pyvis.network import Network
&gt;&gt;&gt; import networkx as nx
# generate generic network gr ...</description><pubDate>Thu, 06 Aug 2020 00:00:00 +0000</pubDate><guid>https://www.michelepasin.org/blog/2020/08/06/more-jupyter-notebooks-pyvis-and-networkx/</guid></item><item><title>Getting to grips with Google Colab</title><link>https://www.michelepasin.org/blog/2020/01/30/getting-to-grips-with-google-colab/</link><description>
I've been using Google Colab on a regular basis during the last few months, as I was curious to see whether I could make the switch to it (from a more traditional Jupyter/Jupyterlab environment). As it turns out, Colab is pretty amazing in many respects but there are still situations where a local Jupyter notebook is my first choice. Keep reading to discover why!

### Google Colab VS Jupyter

[Google Colaboratory](https://colab.research.google.com/) (also known as Colab, see the [faqs](https://research.google.com/colaboratory/faq.html)) is a free [Jupyter](https://en.wikipedia.org/wiki/Project_Jupyter) notebook environment that runs in the cloud and stores its notebooks on Google Drive.

Colab has become extremely popular with data scientists and in particular people doing some kind of **machine learning tasks**. Party, I guess, that's because Colab has deep integration with Google's ML tools (eg [Tensorflow](https://en.wikipedia.org/wiki/TensorFlow)) and in fact Colab actually permit ...</description><pubDate>Thu, 30 Jan 2020 00:00:00 +0000</pubDate><guid>https://www.michelepasin.org/blog/2020/01/30/getting-to-grips-with-google-colab/</guid></item><item><title>Calculating Industry Collaborations via GRID</title><link>https://www.michelepasin.org/blog/2020/01/08/calculating-industry-collaborations-via-grid/</link><description>
A new [tutorial](https://api-lab.dimensions.ai/cookbooks/8-organizations/2-Industry-Collaboration.html) demostrating how to extract and visualize data about _industry collaborations_, by combining the Dimensions data with GRID.

Dimensions uses [GRID](https://grid.ac/) (the Global Research Identifiers Database) to unambiguously identify research organizations. GRID includes a wealth of data, for example whether an organization has type 'Education' or 'Industry'. So it's pretty easy to take advantage of these metadata in order to highlight collaboration patterns between a selected university and other organizations from the industry sector.

The [Identifying the Industry Collaborators of an Academic Institution](https://api-lab.dimensions.ai/cookbooks/8-organizations/2-Industry-Collaboration.html) notebook can be adapted so to focus on any research organization: many of us are linked to some university, hence it's quite interesting to explore what are the non-academic organizations rel ...</description><pubDate>Wed, 08 Jan 2020 00:00:00 +0000</pubDate><guid>https://www.michelepasin.org/blog/2020/01/08/calculating-industry-collaborations-via-grid/</guid></item><item><title>Analysing Grants and Patents using the Dimensions API: a Hands On Workshop</title><link>https://www.michelepasin.org/papers/2019/11/15/niaid-analysing-grants-and-patents-using-the-dimensions-api-a-hands-on-workshop/</link><description></description><pubDate>Fri, 15 Nov 2019 00:00:00 +0000</pubDate><guid>https://www.michelepasin.org/papers/2019/11/15/niaid-analysing-grants-and-patents-using-the-dimensions-api-a-hands-on-workshop/</guid></item><item><title>Analysing Grants and Patents using the Dimensions API: a Hands On Workshop</title><link>https://www.michelepasin.org/papers/2019/11/15/onr-analysing-grants-and-patents-using-the-dimensions-api-a-hands-on-workshop/</link><description></description><pubDate>Fri, 15 Nov 2019 00:00:00 +0000</pubDate><guid>https://www.michelepasin.org/papers/2019/11/15/onr-analysing-grants-and-patents-using-the-dimensions-api-a-hands-on-workshop/</guid></item><item><title>Analysing Grants and Patents using the Dimensions API: a Hands On Workshop</title><link>https://www.michelepasin.org/papers/2019/11/14/nsf-analysing-grants-and-patents-using-the-dimensions-api-a-hands-on-workshop/</link><description></description><pubDate>Thu, 14 Nov 2019 00:00:00 +0000</pubDate><guid>https://www.michelepasin.org/papers/2019/11/14/nsf-analysing-grants-and-patents-using-the-dimensions-api-a-hands-on-workshop/</guid></item><item><title>The Dimensions Analytics API: extracting journal researchers metrics</title><link>https://www.michelepasin.org/papers/2019/10/04/the-dimensions-analytics-api-extracting-journal-researchers-metrics/</link><description></description><pubDate>Fri, 04 Oct 2019 00:00:00 +0000</pubDate><guid>https://www.michelepasin.org/papers/2019/10/04/the-dimensions-analytics-api-extracting-journal-researchers-metrics/</guid></item><item><title>The Dimensions Analytics API: workshop</title><link>https://www.michelepasin.org/papers/2019/10/02/the-dimensions-analytics-api-workshop/</link><description></description><pubDate>Wed, 02 Oct 2019 00:00:00 +0000</pubDate><guid>https://www.michelepasin.org/papers/2019/10/02/the-dimensions-analytics-api-workshop/</guid></item><item><title>Introduction to the Dimensions Search Language (workshop)</title><link>https://www.michelepasin.org/papers/2019/09/02/introduction-to-the-dimensions-search-language-workshop/</link><description>The workshop provide a hands on overview of the Dimensions Search Language https://docs.dimensions.ai/dsl</description><pubDate>Mon, 02 Sep 2019 00:00:00 +0000</pubDate><guid>https://www.michelepasin.org/papers/2019/09/02/introduction-to-the-dimensions-search-language-workshop/</guid></item><item><title>Pypapers: a bare-bones, command line,  PDF manager</title><link>https://www.michelepasin.org/blog/2019/06/30/pypapers-a-bare-bones-command-line-pdf-manager/</link><description>
Ever felt like softwares like [Mendeley](https://www.mendeley.com/homepage-2-1?interaction_required=true&amp;mboxSession=ea3c06ad39f14ce29d625b9d3be138c5) or [Papers](https://www.papersapp.com/) are great, but somehow slow you down? Ever felt like none of the many [reference manager softwares](https://en.wikipedia.org/wiki/Comparison_of_reference_management_software) out there will ever cut it for you, cause you need something R E A L L Y SIMPLE? I did. Many times. So I've finally crossed the line and tried out building a simple commmand-line PDF manager. [PyPapers](https://github.com/lambdamusic/pypapers), is called.

Yes - that's right - [command line](https://en.wikipedia.org/wiki/Terminal_(macOS)). So not for everyone. Also: this is bare bones and pre-alpha. So don't expect wonders. It basically provides a simple interface for searching a folder full of PDFs. That's all for now!

&lt;iframe width="560" height="315" src="https://www.youtube.com/embed/o74Ct1EwZwI?controls=0" title="YouTube ...</description><pubDate>Sun, 30 Jun 2019 00:00:00 +0000</pubDate><guid>https://www.michelepasin.org/blog/2019/06/30/pypapers-a-bare-bones-command-line-pdf-manager/</guid></item><item><title>Modeling publications in SN SciGraph 2012-2019</title><link>https://www.michelepasin.org/papers/2019/06/03/modeling-publications-in-sn-scigraph-20122019/</link><description></description><pubDate>Mon, 03 Jun 2019 00:00:00 +0000</pubDate><guid>https://www.michelepasin.org/papers/2019/06/03/modeling-publications-in-sn-scigraph-20122019/</guid></item><item><title>Introducing DimCli: a Python CLI for the Dimensions API</title><link>https://www.michelepasin.org/blog/2019/05/24/introducing-dimcli-a-python-cli-for-dimensions-api/</link><description>
For the last couple of months I've been working on a new open source Python project. This is called **DimCli**  and it's a command-line library aimed at making it simpler to work with the Dimensions Analytics API.

The project is [available on Github](https://github.com/lambdamusic/dimcli). In a nutshell, DimCli helps people becoming productive with the powerful scholarly analytics API from Dimensions. See the video below for a quick taster of the functionalities available.

&lt;iframe width="560" height="315" src="https://www.youtube.com/embed/HbZPxJ7G_00?controls=0" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen&gt;&lt;/iframe&gt;

### Background

I recenlty joined the [Dimensions](https://www.dimensions.ai/) team, so needed a way to get to grips with their feature-rich API ([official docs](https://docs.dimensions.ai/dsl)). So, building DimCli has been a fun way for me to dig into the  ...</description><pubDate>Fri, 24 May 2019 00:00:00 +0000</pubDate><guid>https://www.michelepasin.org/blog/2019/05/24/introducing-dimcli-a-python-cli-for-dimensions-api/</guid></item><item><title>Interlinking SciGraph and DBpedia datasets using Link Discovery and Named Entity Recognition Techniques</title><link>https://www.michelepasin.org/papers/2019/05/22/interlinking-scigraph-and-dbpedia-datasets-using-link-discovery-and-named-entity-recognition-techniques/</link><description>In recent years we have seen a proliferation of Linked Open Data (LOD) compliant datasets becoming available on the web, leading to an increased number of opportunities for data consumers to build smarter applications which integrate data coming from disparate sources. However, often the integration is not easily achievable since it requires discovering and expressing associations across heterogeneous data sets. The goal of this work is to increase the discoverability and reusability of the scholarly data by integrating them to highly interlinked datasets in the LOD cloud. In order to do so we applied techniques that a) improve the identity resolution across these two sources using Link Discovery for the structured data (i.e. by annotating Springer Nature (SN) SciGraph entities with links to DBpedia entities), and b) enriching SN SciGraph unstructured text content (document abstracts) with links to DBpedia entities using Named Entity Recognition (NER). We published the results of this work using standard vocabularies and provided an interactive exploration tool which presents the discovered links w.r.t. the breadth and depth of the DBpedia classes.</description><pubDate>Wed, 22 May 2019 00:00:00 +0000</pubDate><guid>https://www.michelepasin.org/papers/2019/05/22/interlinking-scigraph-and-dbpedia-datasets-using-link-discovery-and-named-entity-recognition-techniques/</guid></item><item><title>Running interactive Jupyter demos with  mybinder.org</title><link>https://www.michelepasin.org/blog/2019/05/03/running-jupyter-demos-via-mybinder-org/</link><description>
The online tool [mybinder.org](https://mybinder.org/) allows to turn a Git repo into a collection of interactive notebooks with one click.

I played with it a little today and was pretty impressed! A very useful tool e.g. if you have a repository of Jupyter notebooks and want to showcase them to someone with no access to a Jupyter environment.

[![](/media/static/blog_img/Screenshot-2019-05-03-binder.png)](Screenshot-2019-05-03-binder.png)

I was able to run many of the [Dimensions API notebooks](https://github.com/digital-science/dimensions-api) I've been working on in the last months, with little or no changes (follow [this link to try them](https://hub.mybinder.org/user/digital-science-dimensions-api-y3409gua/tree) yourself). Dependencies can be loaded on the fly, and new files (eg local settings) create just as if you are working within a normal Jupyter notebook.

[![](/media/static/blog_img/Screenshot-2019-05-03-binder2.png)](Screenshot-2019-05-03-binder2.png)

&gt; See the official ...</description><pubDate>Fri, 03 May 2019 00:00:00 +0000</pubDate><guid>https://www.michelepasin.org/blog/2019/05/03/running-jupyter-demos-via-mybinder-org/</guid></item><item><title>Hands on with the Dimensions API: An introduction to Data Science</title><link>https://www.michelepasin.org/papers/2019/03/28/hands-on-with-the-dimensions-api-an-introduction-to-data-science/</link><description></description><pubDate>Thu, 28 Mar 2019 00:00:00 +0000</pubDate><guid>https://www.michelepasin.org/papers/2019/03/28/hands-on-with-the-dimensions-api-an-introduction-to-data-science/</guid></item><item><title>SN SciGraph Latest Release: Patents, Clinical Trials and many new features</title><link>https://www.michelepasin.org/blog/2019/03/22/sn-scigraph-latest-release-patents-clinical-trials-and-many-new-features/</link><description>
We are pleased to announce the **third release** of [Springer Nature Scigraph Linked Open Data](https://scigraph.springernature.com). SN SciGraph is a Linked Data platform that collates information from across the research landscape, i.e. the things, documents, people, places and relations of importance to the science and scholarly domain.

This release includes a complete **refactoring of the SN SciGraph data model**. Following up on users feedback, we have simplified it using [Schema.org](https://schema.org/) and [JSON-LD](https://en.wikipedia.org/wiki/JSON-LD), so to make it easier to understand and consume the data also for non-linked data specialists.  

Also, this release includes **two brand new datasets** - Patents and Clinical Trials linked to Springer Nature publications - which have been made available by our partner [Digital Science](https://www.digital-science.com/), and in particular the [Dimensions](http://dimensions.ai) team.

## What's new

- **New Datasets.** Data ab ...</description><pubDate>Fri, 22 Mar 2019 00:00:00 +0000</pubDate><guid>https://www.michelepasin.org/blog/2019/03/22/sn-scigraph-latest-release-patents-clinical-trials-and-many-new-features/</guid></item></channel></rss>