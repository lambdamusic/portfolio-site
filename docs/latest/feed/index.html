<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>The news feed of www.michelepasin.org</title><link>http://www.michelepasin.org/words/</link><description>Latest articles, blogs posts and news</description><atom:link href="http://127.0.0.1:8000/latest/feed/" rel="self"></atom:link><language>en-us</language><lastBuildDate>Fri, 05 Nov 2021 00:00:00 +0000</lastBuildDate><item><title>A static site generator using Django, Wget and Github Pages</title><link>https://www.michelepasin.org/blog/2021/10/29/django-wget-static-site/</link><description>
If you're a Django developer and want to publish a website without the hassle (and costs) of deploying a web app, then this post may give you some useful tips. 

I found myself in this situation several times, so have created a time-saving workflow/set of tools for extracting a dynamic [Django](https://www.djangoproject.com/) website into a [static](https://en.wikipedia.org/wiki/Static_web_page) website (= a website that does not require a web application, just plain simple HTML pages). 

&gt; Disclaimer: this method is not suited for all types of websites. EG if your Django application is updated frequently (e.g. more than once a day), or if it has keyword search (or faceted search) pages that inherently rely on dynamical queries to the Django back-end based on user input, then a static site won't cut it for you, most likely. 

In a nutshell - this is how it works:

1. On my computer, I create / edit the website contents using [Markdown](https://en.wikipedia.org/wiki/Markdown) as much a ...</description><pubDate>Fri, 05 Nov 2021 00:00:00 +0000</pubDate><guid>https://www.michelepasin.org/blog/2021/10/29/django-wget-static-site/</guid></item><item><title>Terminal script: getting the time in different world time zones</title><link>https://www.michelepasin.org/blog/2021/10/12/world-date-terminal/</link><description>
A little Bash script to show information about world time zones. Because I love my colleagues abroad, but I constantly struggle to remember how many hours ahead (or behind?) they are.

I am using the script on a Mac, but it should work on other systems too with little or no changes. Basically, it scans the `zoneinfo` database ([more info](https://en.wikipedia.org/wiki/Tz_database)) that most likely already exists on your computer, in order to return the rows matching an input string.

For example - what's the time in Australia right now?

```bash
$ wdate australia
/Australia/Melbourne               Tue 2021-10-12 20:47:48
/Australia/Queensland              Tue 2021-10-12 19:47:48
/Australia/North                   Tue 2021-10-12 19:17:48
/Australia/Lord_Howe               Tue 2021-10-12 20:47:48
/Australia/Adelaide                Tue 2021-10-12 20:17:48
/Australia/Yancowinna              Tue 2021-10-12 20:17:48
/Australia/Victoria                Tue 2021-10-12 20:47:48
/Australia/Canb ...</description><pubDate>Tue, 07 Sep 2021 00:00:00 +0000</pubDate><guid>https://www.michelepasin.org/blog/2021/10/12/world-date-terminal/</guid></item><item><title>More Jupyter notebooks: pyvis and networkx</title><link>https://www.michelepasin.org/blog/2020/08/06/more-jupyter-notebooks-pyvis-and-networkx/</link><description>
&gt; Lately I've been spending more time creating Jupyter notebooks that demonstrate how to use the [Dimensions API for research analytics.](https://api-lab.dimensions.ai/) In this post I'll talk a little bit about two cool Python libraries I've recenlty discovered for working with **graph** data: **pyvis** and **networkx**.

### pyvis and networkx

The [networkx](https://networkx.github.io/documentation/stable/reference/introduction.html) and [pyvis](https://pyvis.readthedocs.io/en/latest/tutorial.html) libraries are used for _generating_ and _visualizing_ network data, respectively.

**Pyvis** is fundamentally a python wrapper around the popular [Javascript visJS library.](https://visjs.github.io/vis-network/examples/) 
**Networkx**, instead, of is a pretty sophisticated package for the creation, manipulation, and study of the structure, dynamics, and functions of complex networks.


```python
&gt;&gt;&gt; from pyvis.network import Network
&gt;&gt;&gt; import networkx as nx
# generate generic network gr ...</description><pubDate>Thu, 06 Aug 2020 00:00:00 +0000</pubDate><guid>https://www.michelepasin.org/blog/2020/08/06/more-jupyter-notebooks-pyvis-and-networkx/</guid></item><item><title>Getting to grips with Google Colab</title><link>https://www.michelepasin.org/blog/2020/01/30/getting-to-grips-with-google-colab/</link><description>
I've been using Google Colab on a regular basis during the last few months, as I was curious to see whether I could make the switch to it (from a more traditional Jupyter/Jupyterlab environment). As it turns out, Colab is pretty amazing in many respects but there are still situations where a local Jupyter notebook is my first choice. Keep reading to discover why!

### Google Colab VS Jupyter

[Google Colaboratory](https://colab.research.google.com/) (also known as Colab, see the [faqs](https://research.google.com/colaboratory/faq.html)) is a free [Jupyter](https://en.wikipedia.org/wiki/Project_Jupyter) notebook environment that runs in the cloud and stores its notebooks on Google Drive.

Colab has become extremely popular with data scientists and in particular people doing some kind of **machine learning tasks**. Party, I guess, that's because Colab has deep integration with Google's ML tools (eg [Tensorflow](https://en.wikipedia.org/wiki/TensorFlow)) and in fact Colab actually permit ...</description><pubDate>Thu, 30 Jan 2020 00:00:00 +0000</pubDate><guid>https://www.michelepasin.org/blog/2020/01/30/getting-to-grips-with-google-colab/</guid></item><item><title>Calculating Industry Collaborations via GRID</title><link>https://www.michelepasin.org/blog/2020/01/08/calculating-industry-collaborations-via-grid/</link><description>
A new [tutorial](https://api-lab.dimensions.ai/cookbooks/8-organizations/2-Industry-Collaboration.html) demostrating how to extract and visualize data about _industry collaborations_, by combining the Dimensions data with GRID.

Dimensions uses [GRID](https://grid.ac/) (the Global Research Identifiers Database) to unambiguously identify research organizations. GRID includes a wealth of data, for example whether an organization has type 'Education' or 'Industry'. So it's pretty easy to take advantage of these metadata in order to highlight collaboration patterns between a selected university and other organizations from the industry sector.

The [Identifying the Industry Collaborators of an Academic Institution](https://api-lab.dimensions.ai/cookbooks/8-organizations/2-Industry-Collaboration.html) notebook can be adapted so to focus on any research organization: many of us are linked to some university, hence it's quite interesting to explore what are the non-academic organizations rel ...</description><pubDate>Wed, 08 Jan 2020 00:00:00 +0000</pubDate><guid>https://www.michelepasin.org/blog/2020/01/08/calculating-industry-collaborations-via-grid/</guid></item><item><title>Analysing Grants and Patents using the Dimensions API: a Hands On Workshop</title><link>https://www.michelepasin.org/papers/2019/11/15/niaid-analysing-grants-and-patents-using-the-dimensions-api-a-hands-on-workshop/</link><description></description><pubDate>Fri, 15 Nov 2019 00:00:00 +0000</pubDate><guid>https://www.michelepasin.org/papers/2019/11/15/niaid-analysing-grants-and-patents-using-the-dimensions-api-a-hands-on-workshop/</guid></item><item><title>Analysing Grants and Patents using the Dimensions API: a Hands On Workshop</title><link>https://www.michelepasin.org/papers/2019/11/15/onr-analysing-grants-and-patents-using-the-dimensions-api-a-hands-on-workshop/</link><description></description><pubDate>Fri, 15 Nov 2019 00:00:00 +0000</pubDate><guid>https://www.michelepasin.org/papers/2019/11/15/onr-analysing-grants-and-patents-using-the-dimensions-api-a-hands-on-workshop/</guid></item><item><title>Analysing Grants and Patents using the Dimensions API: a Hands On Workshop</title><link>https://www.michelepasin.org/papers/2019/11/14/nsf-analysing-grants-and-patents-using-the-dimensions-api-a-hands-on-workshop/</link><description></description><pubDate>Thu, 14 Nov 2019 00:00:00 +0000</pubDate><guid>https://www.michelepasin.org/papers/2019/11/14/nsf-analysing-grants-and-patents-using-the-dimensions-api-a-hands-on-workshop/</guid></item><item><title>The Dimensions Analytics API: extracting journal researchers metrics</title><link>https://www.michelepasin.org/papers/2019/10/04/the-dimensions-analytics-api-extracting-journal-researchers-metrics/</link><description></description><pubDate>Fri, 04 Oct 2019 00:00:00 +0000</pubDate><guid>https://www.michelepasin.org/papers/2019/10/04/the-dimensions-analytics-api-extracting-journal-researchers-metrics/</guid></item><item><title>The Dimensions Analytics API: workshop</title><link>https://www.michelepasin.org/papers/2019/10/02/the-dimensions-analytics-api-workshop/</link><description></description><pubDate>Wed, 02 Oct 2019 00:00:00 +0000</pubDate><guid>https://www.michelepasin.org/papers/2019/10/02/the-dimensions-analytics-api-workshop/</guid></item><item><title>Introduction to the Dimensions Search Language (workshop)</title><link>https://www.michelepasin.org/papers/2019/09/02/introduction-to-the-dimensions-search-language-workshop/</link><description>The workshop provide a hands on overview of the Dimensions Search Language https://docs.dimensions.ai/dsl</description><pubDate>Mon, 02 Sep 2019 00:00:00 +0000</pubDate><guid>https://www.michelepasin.org/papers/2019/09/02/introduction-to-the-dimensions-search-language-workshop/</guid></item><item><title>Pypapers: a bare-bones, command line,  PDF manager</title><link>https://www.michelepasin.org/blog/2019/06/30/pypapers-a-bare-bones-command-line-pdf-manager/</link><description>
Ever felt like softwares like [Mendeley](https://www.mendeley.com/homepage-2-1?interaction_required=true&amp;mboxSession=ea3c06ad39f14ce29d625b9d3be138c5) or [Papers](https://www.papersapp.com/) are great, but somehow slow you down? Ever felt like none of the many [reference manager softwares](https://en.wikipedia.org/wiki/Comparison_of_reference_management_software) out there will ever cut it for you, cause you need something R E A L L Y SIMPLE? I did. Many times. So I've finally crossed the line and tried out building a simple commmand-line PDF manager. [PyPapers](https://github.com/lambdamusic/pypapers), is called.

Yes - that's right - [command line](https://en.wikipedia.org/wiki/Terminal_(macOS)). So not for everyone. Also: this is bare bones and pre-alpha. So don't expect wonders. It basically provides a simple interface for searching a folder full of PDFs. That's all for now!

&lt;iframe width="560" height="315" src="https://www.youtube.com/embed/o74Ct1EwZwI?controls=0" title="YouTube ...</description><pubDate>Sun, 30 Jun 2019 00:00:00 +0000</pubDate><guid>https://www.michelepasin.org/blog/2019/06/30/pypapers-a-bare-bones-command-line-pdf-manager/</guid></item><item><title>Modeling publications in SN SciGraph 2012-2019</title><link>https://www.michelepasin.org/papers/2019/06/03/modeling-publications-in-sn-scigraph-20122019/</link><description></description><pubDate>Mon, 03 Jun 2019 00:00:00 +0000</pubDate><guid>https://www.michelepasin.org/papers/2019/06/03/modeling-publications-in-sn-scigraph-20122019/</guid></item><item><title>Introducing DimCli: a Python CLI for the Dimensions API</title><link>https://www.michelepasin.org/blog/2019/05/24/introducing-dimcli-a-python-cli-for-dimensions-api/</link><description>
For the last couple of months I've been working on a new open source Python project. This is called **DimCli**  and it's a command-line library aimed at making it simpler to work with the Dimensions Analytics API.

The project is [available on Github](https://github.com/lambdamusic/dimcli). In a nutshell, DimCli helps people becoming productive with the powerful scholarly analytics API from Dimensions. See the video below for a quick taster of the functionalities available.

&lt;iframe width="560" height="315" src="https://www.youtube.com/embed/HbZPxJ7G_00?controls=0" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen&gt;&lt;/iframe&gt;

### Background

I recenlty joined the [Dimensions](https://www.dimensions.ai/) team, so needed a way to get to grips with their feature-rich API ([official docs](https://docs.dimensions.ai/dsl)). So, building DimCli has been a fun way for me to dig into the  ...</description><pubDate>Fri, 24 May 2019 00:00:00 +0000</pubDate><guid>https://www.michelepasin.org/blog/2019/05/24/introducing-dimcli-a-python-cli-for-dimensions-api/</guid></item><item><title>Interlinking SciGraph and DBpedia datasets using Link Discovery and Named Entity Recognition Techniques</title><link>https://www.michelepasin.org/papers/2019/05/22/interlinking-scigraph-and-dbpedia-datasets-using-link-discovery-and-named-entity-recognition-techniques/</link><description>In recent years we have seen a proliferation of Linked Open Data (LOD) compliant datasets becoming available on the web, leading to an increased number of opportunities for data consumers to build smarter applications which integrate data coming from disparate sources. However, often the integration is not easily achievable since it requires discovering and expressing associations across heterogeneous data sets. The goal of this work is to increase the discoverability and reusability of the scholarly data by integrating them to highly interlinked datasets in the LOD cloud. In order to do so we applied techniques that a) improve the identity resolution across these two sources using Link Discovery for the structured data (i.e. by annotating Springer Nature (SN) SciGraph entities with links to DBpedia entities), and b) enriching SN SciGraph unstructured text content (document abstracts) with links to DBpedia entities using Named Entity Recognition (NER). We published the results of this work using standard vocabularies and provided an interactive exploration tool which presents the discovered links w.r.t. the breadth and depth of the DBpedia classes.</description><pubDate>Wed, 22 May 2019 00:00:00 +0000</pubDate><guid>https://www.michelepasin.org/papers/2019/05/22/interlinking-scigraph-and-dbpedia-datasets-using-link-discovery-and-named-entity-recognition-techniques/</guid></item><item><title>Running interactive Jupyter demos with  mybinder.org</title><link>https://www.michelepasin.org/blog/2019/05/03/running-jupyter-demos-via-mybinder-org/</link><description>
The online tool [mybinder.org](https://mybinder.org/) allows to turn a Git repo into a collection of interactive notebooks with one click.

I played with it a little today and was pretty impressed! A very useful tool e.g. if you have a repository of Jupyter notebooks and want to showcase them to someone with no access to a Jupyter environment.

[![](/media/static/blog_img/Screenshot-2019-05-03-binder.png)](Screenshot-2019-05-03-binder.png)

I was able to run many of the [Dimensions API notebooks](https://github.com/digital-science/dimensions-api) I've been working on in the last months, with little or no changes (follow [this link to try them](https://hub.mybinder.org/user/digital-science-dimensions-api-y3409gua/tree) yourself). Dependencies can be loaded on the fly, and new files (eg local settings) create just as if you are working within a normal Jupyter notebook.

[![](/media/static/blog_img/Screenshot-2019-05-03-binder2.png)](Screenshot-2019-05-03-binder2.png)

&gt; See the official ...</description><pubDate>Fri, 03 May 2019 00:00:00 +0000</pubDate><guid>https://www.michelepasin.org/blog/2019/05/03/running-jupyter-demos-via-mybinder-org/</guid></item><item><title>Hands on with the Dimensions API: An introduction to Data Science</title><link>https://www.michelepasin.org/papers/2019/03/28/hands-on-with-the-dimensions-api-an-introduction-to-data-science/</link><description></description><pubDate>Thu, 28 Mar 2019 00:00:00 +0000</pubDate><guid>https://www.michelepasin.org/papers/2019/03/28/hands-on-with-the-dimensions-api-an-introduction-to-data-science/</guid></item><item><title>SN SciGraph Latest Release: Patents, Clinical Trials and many new features</title><link>https://www.michelepasin.org/blog/2019/03/22/sn-scigraph-latest-release-patents-clinical-trials-and-many-new-features/</link><description>
We are pleased to announce the **third release** of [Springer Nature Scigraph Linked Open Data](https://scigraph.springernature.com). SN SciGraph is a Linked Data platform that collates information from across the research landscape, i.e. the things, documents, people, places and relations of importance to the science and scholarly domain.

This release includes a complete **refactoring of the SN SciGraph data model**. Following up on users feedback, we have simplified it using [Schema.org](https://schema.org/) and [JSON-LD](https://en.wikipedia.org/wiki/JSON-LD), so to make it easier to understand and consume the data also for non-linked data specialists.  

Also, this release includes **two brand new datasets** - Patents and Clinical Trials linked to Springer Nature publications - which have been made available by our partner [Digital Science](https://www.digital-science.com/), and in particular the [Dimensions](http://dimensions.ai) team.

## What's new

- **New Datasets.** Data ab ...</description><pubDate>Fri, 22 Mar 2019 00:00:00 +0000</pubDate><guid>https://www.michelepasin.org/blog/2019/03/22/sn-scigraph-latest-release-patents-clinical-trials-and-many-new-features/</guid></item><item><title>Zero Hunger Hack Day: surfacing research about the Sustainable Development Goals program</title><link>https://www.michelepasin.org/blog/2019/02/11/zero-hunger-hack-day/</link><description>
This post is about a little dashboard idea that aims at helping policy makers discover research relevant to the '[zero hunger](https://en.wikipedia.org/wiki/Sustainable_Development_Goals#Goal_2:_Zero_hunger)' topic, one of the themes of the [Sustainable Development Goals](https://sustainabledevelopment.un.org/sdgs) program.

&gt; [The 2030 Agenda for Sustainable Development,](https://sustainabledevelopment.un.org/post2015/transformingourworld) adopted by all United Nations Member States in 2015, provides a shared blueprint for peace and prosperity for people and the planet, now and into the future. At its heart are the 17 Sustainable Development Goals (SDGs), which are an urgent call for action by all countries - developed and developing - in a global partnership. They recognize that ending poverty and other deprivations must go hand-in-hand with strategies that improve health and education, reduce inequality, and spur economic growth – all while tackling climate change and working to pr ...</description><pubDate>Mon, 11 Feb 2019 00:00:00 +0000</pubDate><guid>https://www.michelepasin.org/blog/2019/02/11/zero-hunger-hack-day/</guid></item><item><title>Ontospy 1.9.8 released</title><link>https://www.michelepasin.org/blog/2019/01/03/ontospy-1-9-8-released/</link><description>
Ontospy version 1.9.8 has been just released and it contains tons of improvements and new features. [Ontospy](http://lambdamusic.github.io/Ontospy/) is a lightweight open-source Python library and command line tool for working with vocabularies encoded in the [RDF](https://en.wikipedia.org/wiki/Resource_Description_Framework) family of languages.

Over the past month I've been working on a new version of Ontospy, which is now available for download on [Pypi](https://pypi.org/project/ontospy/). It's been quite some time in the making, so pretty glad it's finally out now.

&lt;iframe width="560" height="315" src="https://www.youtube.com/embed/MkKrtVHi_Ks?controls=0" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen&gt;&lt;/iframe&gt;

### What's new in this version

- The library module used to generate **ontology documentation** (as html or markdown) is now included within the main Ontospy d ...</description><pubDate>Thu, 03 Jan 2019 00:00:00 +0000</pubDate><guid>https://www.michelepasin.org/blog/2019/01/03/ontospy-1-9-8-released/</guid></item></channel></rss>