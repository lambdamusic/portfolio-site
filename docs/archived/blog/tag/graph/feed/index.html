<?xml version="1.0" encoding="UTF-8"?><rss version="2.0"
	xmlns:content="http://purl.org/rss/1.0/modules/content/"
	xmlns:wfw="http://wellformedweb.org/CommentAPI/"
	xmlns:dc="http://purl.org/dc/elements/1.1/"
	xmlns:atom="http://www.w3.org/2005/Atom"
	xmlns:sy="http://purl.org/rss/1.0/modules/syndication/"
	xmlns:slash="http://purl.org/rss/1.0/modules/slash/"
	
	xmlns:georss="http://www.georss.org/georss"
	xmlns:geo="http://www.w3.org/2003/01/geo/wgs84_pos#"
	>

<channel>
	<title>graph &#8211; Parerga und Paralipomena</title>
	<atom:link href="http://www.michelepasin.org/blog/tag/graph/feed/" rel="self" type="application/rss+xml" />
	<link>http://www.michelepasin.org/blog</link>
	<description>At the core of all well-founded belief lies belief that is unfounded - Wittgenstein</description>
	<lastBuildDate>Thu, 06 Aug 2020 10:18:31 +0000</lastBuildDate>
	<language>en-US</language>
	<sy:updatePeriod>
	hourly	</sy:updatePeriod>
	<sy:updateFrequency>
	1	</sy:updateFrequency>
	<generator>https://wordpress.org/?v=5.2.11</generator>
<site xmlns="com-wordpress:feed-additions:1">13825966</site>	<item>
		<title>More Jupyter notebooks: pyvis and networkx</title>
		<link>http://www.michelepasin.org/blog/2020/08/06/more-jupyter-notebooks-pyvis-and-networkx/</link>
				<pubDate>Thu, 06 Aug 2020 09:55:12 +0000</pubDate>
		<dc:creator><![CDATA[mikele]]></dc:creator>
				<category><![CDATA[Data science]]></category>
		<category><![CDATA[graph]]></category>
		<category><![CDATA[jupyter]]></category>
		<category><![CDATA[python]]></category>
		<category><![CDATA[visualization]]></category>

		<guid isPermaLink="false">http://www.michelepasin.org/blog/?p=3368</guid>
				<description><![CDATA[Lately I&#8217;ve been spending more time creating Jupyter notebooks that demonstrate how to use the Dimensions API for research analytics. In this post I&#8217;ll talk a little bit about two cool Python technologies I&#8217;ve discovered for working with graph data: pyvis and networkx. pyvis and networkx The networkx and pyvis libraries are used for generating and visualizing network [&#8230;]]]></description>
								<content:encoded><![CDATA[<p>Lately I&#8217;ve been spending more time creating Jupyter notebooks that demonstrate how to use the <a href="https://api-lab.dimensions.ai/">Dimensions API for research analytics.</a> In this post I&#8217;ll talk a little bit about two cool Python technologies I&#8217;ve discovered for working with graph data: pyvis and networkx.</p>
<h3>pyvis and networkx</h3>
<p>The <a class="reference external" href="https://networkx.github.io/documentation/stable/reference/introduction.html" target="_blank" rel="noopener noreferrer">networkx</a> and <a class="reference external" href="https://pyvis.readthedocs.io/en/latest/tutorial.html" target="_blank" rel="noopener noreferrer">pyvis</a> libraries are used for <em>generating</em> and <em>visualizing</em> network data, respectively.</p>
<p>Pyvis is fundamentally a python wrapper around the popular <a href="https://visjs.github.io/vis-network/examples/">Javascript visJS library.</a> Networkx instead of is a pretty sophisticated package for the creation, manipulation, and study of the structure, dynamics, and functions of complex networks.</p>
<pre><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">pyvis.network</span> <span class="k">import</span> <span class="n">Network</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">networkx</span> <span class="k">as</span> <span class="nn">nx
</span># generate generic <span class="nn">network graph instance</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">nx_graph</span> <span class="o">=</span> <span class="n">nx</span><span class="o">.Graph()</span><span class="p">
</span># add some nodes and edges
<span class="gp">&gt;&gt;&gt; </span><span class="n">nx_graph</span><span class="o">.</span><span class="n">nodes</span><span class="p">[</span><span class="mi">1</span><span class="p">][</span><span class="s1">'title'</span><span class="p">]</span> <span class="o">=</span> <span class="s1">'Number 1'</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">nx_graph</span><span class="o">.</span><span class="n">nodes</span><span class="p">[</span><span class="mi">1</span><span class="p">][</span><span class="s1">'group'</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">nx_graph</span><span class="o">.</span><span class="n">nodes</span><span class="p">[</span><span class="mi">3</span><span class="p">][</span><span class="s1">'title'</span><span class="p">]</span> <span class="o">=</span> <span class="s1">'I belong to a different group!'</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">nx_graph</span><span class="o">.</span><span class="n">nodes</span><span class="p">[</span><span class="mi">3</span><span class="p">][</span><span class="s1">'group'</span><span class="p">]</span> <span class="o">=</span> <span class="mi">10</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">nx_graph</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s1">'couple'</span><span class="p">,</span> <span class="n">group</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">nx_graph</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="mi">21</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">15</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s1">'couple'</span><span class="p">,</span> <span class="n">group</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">nx_graph</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">21</span><span class="p">,</span> <span class="n">weight</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">nx_graph</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="mi">25</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">25</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">'lonely'</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s1">'lonely node'</span><span class="p">,</span> <span class="n">group</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<span class="gp"># instantiatet <span class="nn">pyvis network</span>
&gt;&gt;&gt; </span><span class="n">nt</span> <span class="o">=</span> <span class="n">Network</span><span class="p">(</span><span class="s2">"500px"</span><span class="p">,</span> <span class="s2">"500px"</span><span class="p">)</span>
<span class="go"># populates <span class="gp"><span class="nn">pyvis network from networkx instance</span></span></span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">nt</span><span class="o">.</span><span class="n">from_nx</span><span class="p">(</span><span class="n">nx_graph</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">nt</span><span class="o">.</span><span class="n">show</span><span class="p">(</span><span class="s2">"nx.html"</span><span class="p">)</span></pre>
<p>It took me a little to familiarise with the libraries&#8217; concepts and to generate some basic graphs. So, the tutorials linked below are meant to provide some reusable <em>code</em> building blocks for working with these tools.</p>
<p>Once you get the hang of it though, the fun part begins. What are the best data variables to represent in the graph? What color coding strategy is making it easier to explore the data? How many nodes/edges to display? Can we add some interactivity to the visualizations? Check out the resulting visualizations below for more ideas.</p>
<h3>Dataviz: concepts co-occurence network</h3>
<p>The <a href="https://api-lab.dimensions.ai/cookbooks/2-publications/Concepts-network-graph.html">Building a concepts co-occurence network</a> notebook shows how to turn document keywords extracted from &#8216;semantic web&#8217; publications into a simple topic map &#8211; by virtue of their co-occurrence within the same documents.</p>
<p>See also the standalone html version of the interactive visualization: <a href="http://api-sample-data.dimensions.ai/dataviz-exports/concets-cooccurence/concepts_network_2020-08-05.html">concepts_network_2020-08-05.html</a></p>
<p><a href="http://www.michelepasin.org/blog/wp-content/uploads/2020/08/graph2.jpg"><img class="aligncenter size-large wp-image-3426" src="http://www.michelepasin.org/blog/wp-content/uploads/2020/08/graph2-1024x513.jpg" alt="graph2" width="1024" height="513" srcset="http://www.michelepasin.org/blog/wp-content/uploads/2020/08/graph2-1024x513.jpg 1024w, http://www.michelepasin.org/blog/wp-content/uploads/2020/08/graph2-300x150.jpg 300w, http://www.michelepasin.org/blog/wp-content/uploads/2020/08/graph2-768x385.jpg 768w, http://www.michelepasin.org/blog/wp-content/uploads/2020/08/graph2.jpg 1421w" sizes="(max-width: 1024px) 100vw, 1024px" /></a></p>
<p>&nbsp;</p>
<h3>Dataviz: Organizations Collaboration Network</h3>
<p>The <a href="https://api-lab.dimensions.ai/cookbooks/8-organizations/3-Organizations-Collaboration-Network.html">Building an Organizations Collaboration Network Diagram</a> notebook shows how to use publications&#8217; authors and <a href="https://grid.ac/">GRID</a> data to generate a network of collaborating research organizations.</p>
<p>See also the standalone html version of the interactive visualization: <a href="http://api-sample-data.dimensions.ai/dataviz-exports/3-Organizations-Collaboration-Network/network_2_levels_grid.412125.1.html">network_2_levels_grid.412125.1.html</a></p>
<p><a href="http://www.michelepasin.org/blog/wp-content/uploads/2020/08/graph1.jpg"><img class="aligncenter size-large wp-image-3424" src="http://www.michelepasin.org/blog/wp-content/uploads/2020/08/graph1-1024x515.jpg" alt="graph1" width="1024" height="515" srcset="http://www.michelepasin.org/blog/wp-content/uploads/2020/08/graph1-1024x515.jpg 1024w, http://www.michelepasin.org/blog/wp-content/uploads/2020/08/graph1-300x151.jpg 300w, http://www.michelepasin.org/blog/wp-content/uploads/2020/08/graph1-768x386.jpg 768w, http://www.michelepasin.org/blog/wp-content/uploads/2020/08/graph1.jpg 1402w" sizes="(max-width: 1024px) 100vw, 1024px" /></a></p>
<p>&nbsp;</p>
<p>&nbsp;</p>
]]></content:encoded>
									<post-id xmlns="com-wordpress:feed-additions:1">3368</post-id>	</item>
		<item>
		<title>Exploring SciGraph data using JSON-LD, Elastic Search and Kibana</title>
		<link>http://www.michelepasin.org/blog/2017/04/06/exploring-scigraph-data-using-elastic-search-and-kibana/</link>
				<comments>http://www.michelepasin.org/blog/2017/04/06/exploring-scigraph-data-using-elastic-search-and-kibana/#comments</comments>
				<pubDate>Thu, 06 Apr 2017 14:12:05 +0000</pubDate>
		<dc:creator><![CDATA[mikele]]></dc:creator>
				<category><![CDATA[Information Architecture]]></category>
		<category><![CDATA[Semantic Web]]></category>
		<category><![CDATA[data exploration]]></category>
		<category><![CDATA[elasticsearch]]></category>
		<category><![CDATA[graph]]></category>
		<category><![CDATA[jsonld]]></category>
		<category><![CDATA[kibana]]></category>
		<category><![CDATA[linkeddata]]></category>
		<category><![CDATA[nature]]></category>
		<category><![CDATA[scigraph]]></category>
		<category><![CDATA[visualization]]></category>

		<guid isPermaLink="false">http://www.michelepasin.org/blog/?p=2844</guid>
				<description><![CDATA[Hello there data lovers! In this post you can find some information on how to download and make some sense of the scholarly dataset recently made available by the Springer Nature SciGraph project, by using the freely available Elasticsearch suite of software. A few weeks ago the SciGraph dataset was released (full disclosure: I&#8217;m part [&#8230;]]]></description>
								<content:encoded><![CDATA[<p>Hello there data lovers! In this post you can find some information on how to download and make some sense of the scholarly dataset recently made available by the Springer Nature <a href="http://www.springernature.com/scigraph">SciGraph project</a>, by using the freely available <a href="https://en.wikipedia.org/wiki/Elasticsearch">Elasticsearch</a> suite of software.</p>
<p>A few weeks ago the SciGraph dataset was <a href="http://www.springernature.com/gp/group/media/press-releases/springer-nature-scigraph--supporting-open-science-and-the-wider-understanding-of-research/12129614">released</a> (full disclosure: I&#8217;m part of the team who did that!). This is a high quality dataset containing metadata and abstracts about scientific articles published by <a href="https://en.wikipedia.org/wiki/Springer_Nature">Springer Nature</a>, research grants related to them plus other classifications of this content.</p>
<p><img class="alignnone size-full wp-image-3123" src="http://www.michelepasin.org/blog/wp-content/uploads/2017/04/scigraph.png?w=1116" alt="scigraph.png" width="558" height="142" srcset="http://www.michelepasin.org/blog/wp-content/uploads/2017/04/scigraph.png 1888w, http://www.michelepasin.org/blog/wp-content/uploads/2017/04/scigraph-300x76.png 300w, http://www.michelepasin.org/blog/wp-content/uploads/2017/04/scigraph-768x195.png 768w, http://www.michelepasin.org/blog/wp-content/uploads/2017/04/scigraph-1024x260.png 1024w" sizes="(max-width: 558px) 100vw, 558px" /></p>
<p>This release of the dataset includes the last 5 years of content &#8211; that&#8217;s already an impressive <strong>32 gigs of data</strong> you can get your hands on. So in this post I&#8217;m going to show how to do that, in particular by transforming the data from the <a href="https://en.wikipedia.org/wiki/Resource_Description_Framework">RDF graph</a> format they come with, into a <a href="https://en.wikipedia.org/wiki/JSON">JSON format</a> which is more suited for application development and analytics.</p>
<p>We will be using two free-to-download products, <a href="http://ontotext.com/products/graphdb/">GraphDB</a> and <a href="https://www.elastic.co/">Elasticsearch</a>, so you&#8217;ll have to install them if you haven&#8217;t got them already. But no worries, that&#8217;s pretty straighforward, as you&#8217;ll see below.</p>
<h2>1. Hello SciGraph Linked Data</h2>
<p>First things first, we want to get hold of the SciGraph RDF datasets of course. That&#8217;s pretty easy, just head over to the SciGraph <a href="https://github.com/springernature/scigraph/wiki#downloads">downloads page</a> and get the following datasets:</p>
<ul>
<li><strong>Ontologies</strong>: the main <a href="http://s3-service-broker-live-afe45d64-24d0-4a96-b6a8-23b79e885eb7.s3-website.eu-central-1.amazonaws.com/2017-02-15/springernature-scigraph-ontologies.2017-02-15.nt.bz2">schema</a> behind SciGraph.</li>
<li><strong>Articles</strong> <strong>&#8211; 2016</strong>: all the core <a href="http://s3-service-broker-live-afe45d64-24d0-4a96-b6a8-23b79e885eb7.s3-website.eu-central-1.amazonaws.com/2017-02-15/springernature-scigraph-2016-articles.2017-02-15.nt.bz2">articles</a> metadata for one year.</li>
<li><strong>Grants</strong>: <a href="http://s3-service-broker-live-afe45d64-24d0-4a96-b6a8-23b79e885eb7.s3-website.eu-central-1.amazonaws.com/2017-02-15/springernature-scigraph-grants.2017-02-15.nt.bz2">grants</a> metadata related to those articles.</li>
<li><strong>Journals</strong>: full list of Springer Nature <a href="http://s3-service-broker-live-afe45d64-24d0-4a96-b6a8-23b79e885eb7.s3-website.eu-central-1.amazonaws.com/2017-02-15/springernature-scigraph-journals.2017-02-15.nt.bz2">journal catalogue</a>.</li>
<li><strong>Subjects</strong>: classification of <a href="http://s3-service-broker-live-afe45d64-24d0-4a96-b6a8-23b79e885eb7.s3-website.eu-central-1.amazonaws.com/2017-02-15/springernature-scigraph-subjects.2017-02-15.nt.bz2">research areas</a> developed by Springer Nature.</li>
</ul>
<p>That&#8217;s pretty much everything, only thing we&#8217;re getting only one year worth of articles as that&#8217;s enough for the purpose of this exercise (~300k articles from 2016).</p>
<p>Next up, we want to get a couple of other datasets SciGraph depends on:</p>
<ul>
<li><strong>GRID</strong>: a catalogue of the world&#8217;s research organisations. Make sure you get both the <a href="http://www.grid.ac/ontology/">ontology</a> and one of the <a href="https://www.grid.ac/downloads">latest releases</a>, within which you can find an RDF implementation too.</li>
<li><strong>Field Of Research codes</strong>: another classification scheme used in SciGraph, developed by the <a href="https://vocabs.ands.org.au/anzsrc-for">Australian and New Zealand Standard Research Classification</a> organization.</li>
</ul>
<p>That&#8217;s it! Time for a cup of coffee.</p>
<h2>2. Python to the help</h2>
<p>We will be doing a bit of data manipulation  in the next sections and Python is a great language for that sort of thing. Here&#8217;s what we need to get going:</p>
<ol>
<li><strong>Python</strong>. Make sure you have <a href="https://wiki.python.org/moin/BeginnersGuide/Download">Python installed</a> and also <a href="https://packaging.python.org/installing/">Pip</a>, the Python package manager (any Python version <strong>above 2.7</strong> should be ok).</li>
<li><strong>GitHub project</strong>. I&#8217;ve created a few scripts for this tutorial, so head over to the <a href="https://github.com/lambdamusic/hello-scigraph">hello-scigraph project on GitHub</a> and download it to your computer. Note: the project contains all the Python scripts needed to complete this tutorial, but of course you should feel free to modify them or write from scratch if you fancy it!</li>
<li><strong>Libraries</strong>. Install all the dependencies for the hello-scigraph project to run. You can do that by cd-ing into the project folder and running <code style="display: inline; padding: 0px;">pip install -r requirements.txt</code> (ideally within a <a href="http://python-guide-pt-br.readthedocs.io/en/latest/dev/virtualenvs/">virtual environment</a>, but that&#8217;s up to you).</li>
</ol>
<h2>3. Loading the data into GraphDB</h2>
<p>So, you should have by now 8 different files containing data (after step 1 above). Make sure they&#8217;re all in the same folder and that all of them have been unzipped (if needed), then head over to the <a href="http://ontotext.com/graphdb-8-enterprise-linked-data/">GraphDB website</a> and download the free version of the triplestore (you may have to sign up first).</p>
<p>The <a href="http://graphdb.ontotext.com/documentation/free/quick-start-guide.html#run-graphdb-as-a-stand-alone-server">online documentation</a> for GraphDB is pretty good, so it should be easy to get it up and running. In essence, you have to do the following steps:</p>
<ol>
<li><strong>Launch the application</strong>: for me, on a mac, I just had to double click the GraphDB icon &#8211; nice!</li>
<li><strong>Create a</strong> <a href="http://graphdb.ontotext.com/documentation/free/quick-start-guide.html#create-a-repository">new repository</a>: this is the equivalent of a database within the triplestore. Call this repo <span class="pl-pds">&#8220;</span><strong>scigraph-2016</strong><span class="pl-pds">&#8221; so that we&#8217;re all synced for the following steps.</span></li>
</ol>
<p>Next thing, we want a script to load our RDF files into this empty repository. So cd into the directory containg the GitHub project (from step 2) and run the following command:</p>
<pre style="background: lightgray; padding: 5px;">python -m hello-scigraph.loadGraphDB ~/scigraph-downloads/</pre>
<p>The &#8220;loadGraphDB&#8221; script goes through all RDF files in the &#8220;scigraph-downloads&#8221; directory and loads them into the <strong>scigraph-2016 </strong>repository (note: you must replace &#8220;scigraph-downloads&#8221; with the actual path to the folder you downloaded content in step 1 above).</p>
<p><strong>So, to recap:</strong> this script is now loading more than 35 million triples into your local graph database. Don&#8217;t be surprised if it&#8217;ll take some time (in particular the &#8216;articles-2016&#8217; dataset, by far the biggest) so it&#8217;s time to take a break or do something else.</p>
<p>Once the process it&#8217;s finished, you should be able to <a href="http://graphdb.ontotext.com/documentation/free/quick-start-guide.html#explore-your-data-and-class-relationships">explore your data via the GraphDB workbench</a>.  It&#8217;ll look something like this:</p>
<p><img class="aligncenter size-large wp-image-2954" src="http://www.michelepasin.org/blog/wp-content/uploads/2017/04/GraphDB-class-hierarchy-1024x525.png" alt="GraphDB-class-hierarchy" width="1024" height="525" srcset="http://www.michelepasin.org/blog/wp-content/uploads/2017/04/GraphDB-class-hierarchy-1024x525.png 1024w, http://www.michelepasin.org/blog/wp-content/uploads/2017/04/GraphDB-class-hierarchy-300x154.png 300w, http://www.michelepasin.org/blog/wp-content/uploads/2017/04/GraphDB-class-hierarchy-768x394.png 768w" sizes="(max-width: 1024px) 100vw, 1024px" /></p>
<h2>4. Creating an Elasticsearch index</h2>
<p>We&#8217;re almost there. Let&#8217;s head over to the <a href="https://www.elastic.co/downloads/elasticsearch">Elasticsearch website</a> and <strong>download</strong> it. Elasticsearch is a powerful, distributed, JSON-based search and analytics engine so we&#8217;ll be using it to build an analytics dashboard for the SciGraph data.</p>
<p>Make sure Elastic is running (run <code style="display: inline; padding: 0px;">bin/elasticsearch</code> (or <code style="display: inline; padding: 0px;">bin\elasticsearch.bat</code> on Windows), then cd into the hello-scigraph Python project (from step 2) in order to run the following script:</p>
<pre style="background: lightgray; padding: 5px;">python -m hello-scigraph.loadElastic</pre>
<p>If you <a href="https://github.com/lambdamusic/hello-scigraph/blob/master/hello-scigraph/loadElastic.py">take a look at the source code</a>, you&#8217;ll see that the script does the following:</p>
<ol>
<li><strong>Articles loading:</strong> extracts articles references from GraphDB in batches of 200.</li>
<li><strong>Articles metadata extraction:</strong> for each article, we <a href="https://github.com/lambdamusic/hello-scigraph/blob/master/hello-scigraph/queries.py">pull out all relevant metadata</a> (e.g. title, DOI, authors) plus related information (e.g. author GRID organizations, geo locations, funding info etc..).</li>
<li><strong>Articles metadata simplification: </strong> some intermediate nodes coming from the orginal RDF graph are dropped and replaced with a flatter structure which uses a a temporary dummy schema (<code style="display: inline; padding: 0px;">prefix es: &lt;http://elastic-index.scigraph.com/&gt;</code> It doesn&#8217;t matter what we call that schema, but what&#8217;s important is to that we want to simplify the data we put into the Elastic search index. That&#8217;s because while the Graph layer is supposed to facilitate <em>data integration </em>and hence it benefits from a rich semantic representation of information, the <em>search layer</em> is more geared towards performance and retrieval hence a leaner information structure can dramatically speed things up there.</li>
<li><strong>JSON-LD transformation</strong>: the simplified RDF data structure is <a href="http://json-ld.org/">serialized as JSON-LD</a> &#8211; one of the many serializations available for RDF. JSON-LD is of course valid JSON, meaning that we can put that into Elastic right away. This is a bit of a shortcut actually, in fact for a more fine-grained control of how the JSON looks like,  it&#8217;s probably better to transform the data into JSON using some ad-hoc mechanism. But for the purpose of this tutorial it&#8217;s more than enough.</li>
<li><strong>Elastic index creation.</strong> Finally, we can load the data into an Elastic index called &#8211; guess what &#8211; &#8220;hello-scigraph&#8221;.</li>
</ol>
<p>Two more things to point out:</p>
<ul>
<li><strong>Long queries.</strong> The Python script enforces a 60 seconds <a href="https://github.com/lambdamusic/hello-scigraph/blob/master/hello-scigraph/timeout.py">time-out </a>on the GraphDB queries, so in case things go wrong with some articles data the script should keep running.</li>
<li><strong>Memory issues.</strong> The script stops for 10 seconds after each batch of 200 articles (<code style="display: inline; padding: 0px;">time.sleep(10)</code>). Had to do this to prevent GraphDB on my laptop from running out of memory. Time to catch some breath!</li>
</ul>
<p>That&#8217;s it! <strong>Time for another break  now.</strong> A pretty long one actually &#8211; loading all the data took around 10 hours on my (rather averaged spec&#8217;ed) laptop so you may want to do that overnight or get hold of a faster machine/server.</p>
<p>Eventually, once the loading script is finished, you can issue this command from the command line to see how much data you&#8217;ve loaded into the Elastic index  &#8220;hello-scigraph&#8221;. Bravo!</p>
<pre style="background: lightgray; padding: 5px;">curl -XGET 'localhost:9200/_cat/indices/'</pre>
<h2>5. Analyzing the data with Kibana</h2>
<p>Loading the data in Elastic already opens up a number of possibilites &#8211; check out the <a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/search.html">search APIs </a>for some ideas &#8211; however there&#8217;s an even quicker way to analyze the data: <strong>Kibana</strong>. <a href="https://www.elastic.co/products/kibana">Kibana</a> is another free product in the Elastic Search suite, which provides an extensible user interface for configuring and managing all aspects of the Elastic Stack.</p>
<p>So let&#8217;s get started with Kibana: <a href="https://www.elastic.co/downloads/kibana">download it</a> and set it up using the online instructions, then point your browser at <a href="http://localhost:5601">http://localhost:5601 </a>.</p>
<p>You&#8217;ll get to the Kibana dashboard which shows the index we just created. Here you can perform any kind of searches and see the raw data as JSON.</p>
<p>What&#8217;s even more interesting is the <strong>visualization tab</strong>. Results of searches can be rendered as line chart, pie charts etc.. and more dimensions can be added via &#8216;buckets&#8217;. See below for some quick examples, but really, the possibilities are endless!</p>

<a href='http://www.michelepasin.org/blog/2017/04/06/exploring-scigraph-data-using-elastic-search-and-kibana/kibana-super-collider-1/'><img width="300" height="153" src="http://www.michelepasin.org/blog/wp-content/uploads/2017/04/Kibana-super-collider-1-300x153.png" class="attachment-medium size-medium" alt="" srcset="http://www.michelepasin.org/blog/wp-content/uploads/2017/04/Kibana-super-collider-1-300x153.png 300w, http://www.michelepasin.org/blog/wp-content/uploads/2017/04/Kibana-super-collider-1-768x391.png 768w, http://www.michelepasin.org/blog/wp-content/uploads/2017/04/Kibana-super-collider-1-1024x521.png 1024w" sizes="(max-width: 300px) 100vw, 300px" /></a>
<a href='http://www.michelepasin.org/blog/2017/04/06/exploring-scigraph-data-using-elastic-search-and-kibana/kibana-super-collider-2/'><img width="300" height="153" src="http://www.michelepasin.org/blog/wp-content/uploads/2017/04/Kibana-super-collider-2-300x153.png" class="attachment-medium size-medium" alt="" srcset="http://www.michelepasin.org/blog/wp-content/uploads/2017/04/Kibana-super-collider-2-300x153.png 300w, http://www.michelepasin.org/blog/wp-content/uploads/2017/04/Kibana-super-collider-2-768x391.png 768w, http://www.michelepasin.org/blog/wp-content/uploads/2017/04/Kibana-super-collider-2-1024x521.png 1024w" sizes="(max-width: 300px) 100vw, 300px" /></a>
<a href='http://www.michelepasin.org/blog/2017/04/06/exploring-scigraph-data-using-elastic-search-and-kibana/kibana-super-collider-3/'><img width="300" height="171" src="http://www.michelepasin.org/blog/wp-content/uploads/2017/04/Kibana-super-collider-3-300x171.png" class="attachment-medium size-medium" alt="" srcset="http://www.michelepasin.org/blog/wp-content/uploads/2017/04/Kibana-super-collider-3-300x171.png 300w, http://www.michelepasin.org/blog/wp-content/uploads/2017/04/Kibana-super-collider-3-768x438.png 768w, http://www.michelepasin.org/blog/wp-content/uploads/2017/04/Kibana-super-collider-3-1024x584.png 1024w" sizes="(max-width: 300px) 100vw, 300px" /></a>
<a href='http://www.michelepasin.org/blog/2017/04/06/exploring-scigraph-data-using-elastic-search-and-kibana/kibana-super-collider-4/'><img width="300" height="173" src="http://www.michelepasin.org/blog/wp-content/uploads/2017/04/Kibana-super-collider-4-300x173.png" class="attachment-medium size-medium" alt="" srcset="http://www.michelepasin.org/blog/wp-content/uploads/2017/04/Kibana-super-collider-4-300x173.png 300w, http://www.michelepasin.org/blog/wp-content/uploads/2017/04/Kibana-super-collider-4-768x444.png 768w, http://www.michelepasin.org/blog/wp-content/uploads/2017/04/Kibana-super-collider-4-1024x592.png 1024w" sizes="(max-width: 300px) 100vw, 300px" /></a>

<h2>Conclusion</h2>
<p>This post should have given you enough to realise that:</p>
<ol>
<li>The <strong>SciGraph dataset</strong> contain an impressive amount of high-quality scholarly publications metadata which can be used for things like literature search, research statistics etc..</li>
<li>Even though you&#8217;re not familiar with Linked Data and the RDF family of languages, it&#8217;s not hard to get going with a triplestore and then <strong>transform the data</strong> into a more widely used format like JSON.</li>
<li>Finally, <strong>Elasticsearch</strong> and especially <strong>Kibana</strong> are fantastic tools for data analysis and exploration! Needless to say, in this post I&#8217;ve just scratched the surface of what could be done with it.</li>
</ol>
<p>Hope this was fun, any questions or comments, you know the drill :-)</p>
]]></content:encoded>
							<wfw:commentRss>http://www.michelepasin.org/blog/2017/04/06/exploring-scigraph-data-using-elastic-search-and-kibana/feed/</wfw:commentRss>
		<slash:comments>4</slash:comments>
						<post-id xmlns="com-wordpress:feed-additions:1">2844</post-id>	</item>
		<item>
		<title>Installing ClioPatria triplestore on mac os</title>
		<link>http://www.michelepasin.org/blog/2014/10/27/getting-started-with-a-triplestore-on-mac-os-cliopatria/</link>
				<comments>http://www.michelepasin.org/blog/2014/10/27/getting-started-with-a-triplestore-on-mac-os-cliopatria/#comments</comments>
				<pubDate>Mon, 27 Oct 2014 11:53:48 +0000</pubDate>
		<dc:creator><![CDATA[mikele]]></dc:creator>
				<category><![CDATA[Semantic Web]]></category>
		<category><![CDATA[TechLife]]></category>
		<category><![CDATA[cliopatria]]></category>
		<category><![CDATA[graph]]></category>
		<category><![CDATA[rdf]]></category>
		<category><![CDATA[triplestore]]></category>

		<guid isPermaLink="false">http://www.michelepasin.org/blog/?p=2521</guid>
				<description><![CDATA[ClioPatria is a &#8220;SWI-Prolog application that integrates SWI-Prolog&#8217;s the SWI-Prolog libraries for RDF and HTTP services into a ready to use (semantic) web server&#8221;. It is actively developed by the folks at the VU University of Amsterdam and is freely available online. While at a conference last week I saw a pretty cool demo (DIVE) [&#8230;]]]></description>
								<content:encoded><![CDATA[<p>ClioPatria is a &#8220;<a href="http://www.swi-prolog.org/">SWI-Prolog</a> application that integrates SWI-Prolog&#8217;s the SWI-Prolog libraries for RDF and HTTP services into a ready to use (semantic) web server&#8221;. It is actively developed by the folks at the <a href="http://www.cs.vu.nl/en/index.asp">VU University of Amsterdam</a> and is <a href="">freely available online</a>.</p>
<p>While at a conference last week I saw a pretty cool demo (<a href="http://dive.beeldengeluid.nl/">DIVE</a>) which, I later learned, is powered by the ClioPatria triplestore. So I thought I&#8217;d give it a try and by doing so write a follow up on my recent post on <a href="http://www.michelepasin.org/blog/2014/10/16/getting-started-with-a-triplestore-on-mac-os-graphdb-aka-owlim/">installing OWLIM on Mac OS</a>. </p>
<h3>1. Requirements</h3>
<p><strong>OSX</strong>: Mavericks 10.9.5<br />
<strong>XCode</strong>: latest version <a href="https://developer.apple.com/xcode/downloads/">available from Apple</a><br />
<strong>HOMEBREW</strong>: <span style="font-family:monospace;color:#000000; ">ruby -e &#8220;$(curl -fsSkL raw.github.com/mxcl/homebrew/go)&#8221;</span><br />
<strong>Prolog</strong>: build it from source <a href="http://www.swi-prolog.org/build/macos.html">using brew</a>: <span style="font-family:monospace;color:#000000; ">brew install swi-prolog</span><br />
<strong>ClioPatria</strong>: <span style="font-family:monospace;color:#000000; ">git clone https://github.com/ClioPatria/ClioPatria.git</span></p>
<h3>2. Setting up</h3>
<p>After you have downloaded and unpacked the archive, all you need to do is <a href="http://cliopatria.swi-prolog.org/help/source/doc/home/vnc/prolog/src/ClioPatria/web/help/CreateProject.html">start a new project</a> using the ClioPatria script. In short, this is done by creating a new directory and telling ClioPatria to <span style="font-family:monospace;color:#000000; ">configure</span> it as a project:</p>
<p><span style="font-family:monospace;color:#000000; ">[michele.pasin]:~/Documents/ClioPatriaProjects/firstproject> ../path/to/ClioPatria/configure</span></p>
<p>A bunch of files are created, including a script <span style="font-family:monospace;color:#000000; ">run.pl</span> which you can use later to run the server. </p>
<h3>3. Running ClioPatria</h3>
<p>I tried running the <span style="font-family:monospace;color:#000000; ">run.pl</span> as per documentation but that didn&#8217;t work: </p>
<pre style='color:#000020;background:#f6f8ff; overflow: auto; width: 800px; font-family: monospace; line-height: 1;'>
[michele.pasin]@Tartaruga:~/Documents/ClioPatriaProjects/firstproject>./run.pl 
./run.pl: line 3: :-: command not found
./run.pl: line 5: /Applications: is a directory
./run.pl: line 6: This: command not found
./run.pl: line 8: syntax error near unexpected token `('
./run.pl: line 8: `    % ./configure			(Unix)'
</pre>
<p>According to a thread on stack overflow, the <a href="http://stackoverflow.com/questions/25467090/how-to-run-swi-prolog-from-the-command-line">Prolog shebang line isn&#8217;t interpreted correctly by OSx</a>, meaning that Mac OS doesn&#8217;t recognise that script as a Prolog program. </p>
<p>That can be easily <strong>solved</strong> by calling the Prolog interpreter (<span style="font-family:monospace;color:#000000; ">swipl</span>) explicitly:</p>
<pre style='color:#000020;background:#f6f8ff; overflow: auto; width: 800px; font-family: monospace; line-height: 1;'>[michele.pasin]@Tartaruga:~/Documents/ClioPatriaProjects/firstproject>swipl run.pl 
ERROR: /Applications/-Other-Apps/8-Languages-IDEs/ClioPatria/rdfql/sparql_runtime.pl:1246:14: Syntax error: Operator expected
% run.pl compiled 1.64 sec, 25,789 clauses
% Started ClioPatria server at port 3020
% You may access the server at http://tartaruga.local:3020/
% Loaded 0 graphs (0 triples) in 0.00 sec. (0% CPU = 0.00 sec.)
Welcome to SWI-Prolog (Multi-threaded, 64 bits, Version 6.6.6)
Copyright (c) 1990-2013 University of Amsterdam, VU Amsterdam
SWI-Prolog comes with ABSOLUTELY NO WARRANTY. This is free software,
and you are welcome to redistribute it under certain conditions.
Please visit http://www.swi-prolog.org for details. </pre>
<p>You should be able to access the server with your browser on <strong>port 3020</strong> (ps: the previous command caused a syntax error too, but luckily that isn&#8217;t a game stopper).</p>
<p><a href="http://www.michelepasin.org/blog/wp-content/uploads/2014/10/cliopatria.png"><img src="http://www.michelepasin.org/blog/wp-content/uploads/2014/10/cliopatria.png"  alt="Cliopatria" border="0" width="600" style="width: 600px; margin-left:auto; margin-right:auto; " /></a></p>
<h4>First impression:</h4>
<p>Super-easy to install, <strong>clean</strong> and <strong>intuitive</strong> user interface. I subsequently added a couple of RDF datasets and it all went very very smoothy. </p>
<p>One cool feature is the fact that ClioPatria has a <a href="http://cliopatria.swi-prolog.org/help/source/doc/home/vnc/prolog/src/ClioPatria/web/help/cpack/index.txt">built-in package management system</a>, which allows you to easily install <strong>extensions</strong> to the application. For example what follows allows one to quickly extend the UI with a couple of &#8216;intelligent&#8217; SPARQL query interfaces (<a href="http://yasqe.yasgui.org/">Yasque</a> and <a href="http://openuplabs.tso.co.uk/demos/sparqleditor">Flint</a>): </p>
<pre style='color:#000020;background:#f6f8ff; overflow: auto; width: 800px; font-family: monospace; line-height: 1;'>
[michele.pasin]@Tartaruga:/Applications/ClioPatria>sudo git submodule update --init web/yasqe web/yasr
Password:


[michele.pasin]@Tartaruga:/Applications/ClioPatria>sudo git submodule update --init web/FlintSparqlEditor
</pre>
<p>&nbsp;</p>
<h3>4. Loading a big dataset</h3>
<p>As in my <a href="http://www.michelepasin.org/blog/2014/10/16/getting-started-with-a-triplestore-on-mac-os-graphdb-aka-owlim/">previous post</a>, I&#8217;ve tried loading the <a href="http://data.nature.com/downloads/2012-07-16/articles.2012-07-16.nq.tar.gz">NPG Articles</a> dataset available at nature.com&#8217;s legacy linked data site <a href="http://www.nature.com/developers/documentation/linked-data-platform/releases/snapshot-downloads/">data.nature.com</a>. The dataset contains around <strong>40M triples</strong> describing (at the metadata level) all that&#8217;s been published by NPG and Scientific American from 1845 till nowadays. The file size is <strong>~6 gigs</strong> so it&#8217;s not a huge dataset. Still, something big enough to pose a challenge to my macbook pro (8gigs RAM). </p>
<p>I used the <strong>web UI</strong> (&#8216;load local file&#8217;) to load the dataset but I quickly ran into a <strong>&#8216;not enough memory&#8217; error</strong>. Tried fiddling with the settings accessible via the web interface (<em>Stack limit</em>, <em>Time limit</em>), but that didn&#8217;t seem to do much.<br />
So I <strong>increased the memory allocated to the Prolog process</strong> (more info <a href="http://www.swi-prolog.org/FAQ/StackSizes.html">here</a>) however this wasn&#8217;t enough since after around 20mins the whole thing crashed again due to an <em>out of memory</em> error. </p>
<pre style='color:#000020;background:#f6f8ff; overflow: auto; width: 800px; font-family: monospace; line-height: 1;'>[michele.pasin]@Tartaruga:~/Documents/ClioPatriaProjects/firstproject>swipl -G6g run.pl</pre>
<p>In the end I got in touch with the ClioPatria creators via the <a href="http://mailman.few.vu.nl/pipermail/cliopatria-list/">mailing list</a>: in their (incredibly fast) reply they suggested to <strong>load the dataset manually using the server Prolog console</strong>. You&#8217;d do that simply by using the <span style="font-family:monospace;color:#000000; ">rdf_load</span> command after starting the ClioPatria server (as shown above):</p>
<pre style='color:#000020;background:#f6f8ff; overflow: auto; width: 800px; font-family: monospace; line-height: 1;'>
?- rdf_load('/Users/michele.pasin/Downloads/NPGcitationsGraph/articles.2012-07-16/articles.nq')
|    .
% Parsed "articles.nq" in 1149.71 sec; 0 triples
</pre>
<p><strong>That worked</strong>: the dataset was loaded in around 20 mins. Job done! </p>
<p><strong>However</strong> when I tried to run some queries the application became very slow and ultimately <strong>not responding</strong> (especially with queries like trying to retrieve all named classes from the graph). I tried <strong>restarting the triplestore</strong>, and realised that  once you do that, ClioPatria begins by re-loading all repositories previously created &#8211; which, in the case of my 40M triples repo, would take around 10-15 minutes. </p>
<p>After restarting the server, queries were a bit faster but in many cases still pretty slowish on my 8G ram laptop.</p>
<p>&nbsp;</p>
<h4>Conclusion:</h4>
<p>I am sure there are many more things which could be optimised, however I&#8217;m no Prolog expert nor I could figure out where to start just based on the online documentation. So I kind of gave up on using it to work on large datasets on my macbook for now.</p>
<p>On the other hand, I <strong>really liked</strong> ClioPatria&#8217;s intuitive and simple UI, its ease of installation and the fact you can perform operations transparently and interactively via a Prolog-console (assuming you know how to do that). </p>
<p>All in all, ClioPatria seems to me a really good option if you want to get up and running quickly e.g. in order to prototype linked data applications or explore small to medium-sized RDF datasets (10M triples or so I guess). For bigger datasets, you better <a href="http://mac.appstorm.net/how-to/hardware-how-to/how-and-why-to-upgrade-your-macs-ram/">equip your mac</a> with a few gigs of extra RAM! </p>
<h3>5. Useful resources</h3>
<p><strong>> Whitepaper with technical analysis</strong></p>
<li><a href="http://cliopatria.swi-prolog.org/help/whitepaper.html">http://cliopatria.swi-prolog.org/help/whitepaper.html</a></li>
<p><strong>> Mailing list</strong></p>
<li><a href="http://mailman.few.vu.nl/mailman/listinfo/cliopatria-list">http://mailman.few.vu.nl/mailman/listinfo/cliopatria-list</a></li>
<p>&nbsp;</p>
]]></content:encoded>
							<wfw:commentRss>http://www.michelepasin.org/blog/2014/10/27/getting-started-with-a-triplestore-on-mac-os-cliopatria/feed/</wfw:commentRss>
		<slash:comments>3</slash:comments>
						<post-id xmlns="com-wordpress:feed-additions:1">2521</post-id>	</item>
		<item>
		<title>Installing GraphDB (aka OWLIM) triplestore on mac os</title>
		<link>http://www.michelepasin.org/blog/2014/10/16/getting-started-with-a-triplestore-on-mac-os-graphdb-aka-owlim/</link>
				<pubDate>Thu, 16 Oct 2014 19:05:38 +0000</pubDate>
		<dc:creator><![CDATA[mikele]]></dc:creator>
				<category><![CDATA[Semantic Web]]></category>
		<category><![CDATA[TechLife]]></category>
		<category><![CDATA[database]]></category>
		<category><![CDATA[graph]]></category>
		<category><![CDATA[rdf]]></category>
		<category><![CDATA[sparql]]></category>
		<category><![CDATA[triplestore]]></category>

		<guid isPermaLink="false">http://www.michelepasin.org/blog/?p=2507</guid>
				<description><![CDATA[GraphDB (formerly called OWLIM) is an RDF triplestore which is used &#8211; among others &#8211; by large organisations like the BBC or the British Museum. I&#8217;ve recently installed the LITE release of this graph database on my mac, so what follows is a simple write up of the steps that worked for me. Haven&#8217;t played [&#8230;]]]></description>
								<content:encoded><![CDATA[<p><a href="http://www.ontotext.com/products/ontotext-graphdb/">GraphDB</a> (formerly called OWLIM) is an RDF triplestore which is used &#8211; among others &#8211; by large organisations like the <a href="http://www.ontotext.com/customers/">BBC or the British Museum</a>. I&#8217;ve recently installed the LITE release of this graph database on my mac, so what follows is a simple write up of the steps that worked for me. </p>
<p>Haven&#8217;t played much with the database yet, but all in all, the installation was much simpler than expected (ps: this old <a href="https://code.google.com/p/sgvizler/wiki/HowTo_MacOWLIMLiteTomcat7Apache">recipe on google code</a> was very helpful in steering me in the right direction with the whole Tomcat/Java setup).</p>
<h3>1. Requirements</h3>
<p><strong>OSX</strong>: Mavericks 10.9.5<br />
<strong>XCode</strong>: latest version <a href="https://developer.apple.com/xcode/downloads/">available from Apple</a><br />
<strong>HOMEBREW</strong>: <span style="font-family:monospace;color:#000000; ">ruby -e &#8220;$(curl -fsSkL raw.github.com/mxcl/homebrew/go)&#8221;</span><br />
<strong>Tomcat7</strong>: <span style="font-family:monospace;color:#000000; ">brew install tomcat</span><br />
<strong>JAVA</strong>: available <a href="http://support.apple.com/kb/dl1572">from Apple</a> </p>
<p>Finally &#8211; we obviously want to get a copy of <strong>OWLIM-Lite</strong> too: <a href="http://www.ontotext.com/owlim/downloads">http://www.ontotext.com/owlim/downloads</a></p>
<h3>2. Setting up</h3>
<p>After you have downloaded and unpacked the archive, you must simply <strong>copy</strong> these two files:</p>
<p><span style="font-family:monospace;color:#000000; ">owlim-lite/sesame_owlim/openrdf-sesame.war </span><br />
<span style="font-family:monospace;color:#000000; ">owlim-lite/sesame_owlim/openrdf-workbench.war </span></p>
<p>..to the Tomcat <span style="font-family:monospace;color:#000000; ">webapps</span> folder:</p>
<p><span style="font-family:monospace;color:#000000; ">/usr/local/Cellar/tomcat/7.0.29/libexec/webapps/ </span></p>
<p>Essentially that&#8217;s because OWLIM-Lite is packaged as a storage and inference layer for the <a href="http://rdf4j.org/">Sesame RDF framework</a>, which runs here as a component within the <a href="http://tomcat.apache.org/">Tomcat</a> server (note: there are <a href="https://confluence.ontotext.com/display/OWLIMv53/OWLIM-Lite+Installation#OWLIM-LiteInstallation-Usefulinformation">other ways to run OWLIM</a>, but this one seemed the quickest).</p>
<h3>3. Starting Tomcat</h3>
<p>First I created a symbolic link in my <span style="font-family:monospace;color:#000000; ">~/Library</span> folder, so to better manage new versions (as suggested <a href="http://wolfpaulus.com/jounal/mac/tomcat7">here</a>).</p>
<p><span style="font-family:monospace;color:#000000; ">sudo ln -s /usr/local/Cellar/tomcat/7.0.39 ~/Library/Tomcat</span></p>
<p>Then in order to start/stop Tomcat it&#8217;s enough to use the <span style="font-family:monospace;color:#000000; ">catalina</span> command:</p>
<pre style='color:#000020;background:#f6f8ff; overflow: auto; width: 800px; font-family: monospace; line-height: 1;'>
[michele.pasin]@here:~/Library/Tomcat/bin>./catalina start
Using CATALINA_BASE:   /usr/local/Cellar/tomcat/7.0.39/libexec
Using CATALINA_HOME:   /usr/local/Cellar/tomcat/7.0.39/libexec
Using CATALINA_TMPDIR: /usr/local/Cellar/tomcat/7.0.39/libexec/temp
Using JRE_HOME:        /System/Library/Java/JavaVirtualMachines/1.6.0.jdk/Contents/Home
Using CLASSPATH:       /usr/local/Cellar/tomcat/7.0.39/libexec/bin/bootstrap.jar:/usr/local/Cellar/tomcat/7.0.39/libexec/bin/tomcat-juli.jar

[michele.pasin]@here:~/Library/Tomcat/bin>./catalina stop
Using CATALINA_BASE:   /usr/local/Cellar/tomcat/7.0.39/libexec
Using CATALINA_HOME:   /usr/local/Cellar/tomcat/7.0.39/libexec
Using CATALINA_TMPDIR: /usr/local/Cellar/tomcat/7.0.39/libexec/temp
Using JRE_HOME:        /System/Library/Java/JavaVirtualMachines/1.6.0.jdk/Contents/Home
Using CLASSPATH:       /usr/local/Cellar/tomcat/7.0.39/libexec/bin/bootstrap.jar:/usr/local/Cellar/tomcat/7.0.39/libexec/bin/tomcat-juli.jar
</pre>
<blockquote><p>Tip: Tomcat runs by default on port 8080. That can be changed pretty easily by modifying a parameter in <span style="font-family:monospace;color:#000000; ">server.xml</span> in <span style="font-family:monospace;color:#000000; ">{Tomcat installation folder}/libexec/conf/</span> more <a href="http://www.mkyong.com/tomcat/how-to-change-tomcat-default-port/">details here</a>.
</p></blockquote>
<p>&nbsp;</p>
<h3>4. Testing the Graph database</h3>
<p>Start a browser and go to the Workbench Web application using a URL of this form: <span style="font-family:monospace;color:#000000; ">http://localhost:8080/openrdf-workbench/</span> (substituting localhost and the 8080 port number as appropriate). You should see something like this:</p>
<p><a href="http://www.michelepasin.org/blog/wp-content/uploads/2014/10/SesameWorkbench.png"><img src="http://www.michelepasin.org/blog/wp-content/uploads/2014/10/SesameWorkbench.png"  alt="SesameWorkbench" border="0" width="600" style="width: 600px; margin-left:auto; margin-right:auto; " /></a></p>
<p>After selecting a server, click ‘New repository’.</p>
<p>Select ‘<strong>OWLIM-Lite</strong>’ from the drop-down and enter the repository ID and description. Then click ‘next’.</p>
<p>Fill in the fields as required and click ‘create’.</p>
<p>That&#8217;s it! A message should be displayed that gives details of the new repository and this should also appear in the repository list (click ‘repositories’ to see this).</p>
<h3>5. Loading a big dataset</h3>
<p>I&#8217;ve set out to load the <a href="http://data.nature.com/downloads/2012-07-16/articles.2012-07-16.nq.tar.gz">NPG Articles</a> dataset available at nature.com&#8217;s legacy linked data site <a href="http://www.nature.com/developers/documentation/linked-data-platform/releases/snapshot-downloads/">data.nature.com</a>.</p>
<p>The dataset contains around <strong>40M triples</strong> describing (at the metadata level) all that&#8217;s been published by NPG and Scientific American from 1845 till nowadays. The file size is <strong>~6 gigs</strong> so it&#8217;s not a huge dataset. Still, something big enough to pose a challenge to my macbook pro (8gigs RAM). </p>
<p>First, I <strong>increased the memory</strong> allocated to the Tomcat application to 5G. It was enough to create a <span style="font-family:monospace;color:#000000; ">setenv.sh</span> file in the <span style="font-family:monospace;color:#000000; ">${tomcat-folder}\bin\</span> folder. The file contains this line: </p>
<p><span style="font-family:monospace;color:#000000; ">CATALINA_OPTS=&#8221;$CATALINA_OPTS -server -Xms5g -Xmx5g&#8221;</span></p>
<blockquote><p>More details on Tomcat&#8217;s and Java memory issues are<a href="http://www.mkyong.com/tomcat/tomcat-javalangoutofmemoryerror-permgen-space/"> available here</a>.</p></blockquote>
<p>Then I used OWLIM&#8217;s <strong>web interface</strong> to create a new graph repository and upload the dataset file into it (I previously downloaded a copy of the dataset to my computer so to work with local files only). </p>
<p>It took around 10 minutes for the application to upload the file into the triplestore, and 2-3 minutes for OWLIM to process it. <strong>Much much faster than what I expected</strong>. Only minor issue, the lack of notifications (in the UI) of what was going on. Not a big deal in my case, but with larger dataset uploads it might be a potential downer. </p>
<p>Note: I used the web form to upload the dataset, but there are also <a href="http://answers.ontotext.com/questions/1045/archive-loading-a-large-triple-store-using-owlim-se">ways to do that from the command line</a> (which will probably result in even faster uploads).</p>
<h3>6. Useful information</h3>
<p><strong>> Sparql endpoints</strong></p>
<p>All of your repositories come also with a handy SPARQL endpoint, which is available at this url: <span style="font-family:monospace;color:#000000; ">http://localhost:8080/openrdf-sesame/repositories/test1</span> (just change the last bit so that it matches your repository name).</p>
<p><strong>> Official documentation</strong></p>
<li><a href="https://confluence.ontotext.com/display/GraphDB6/">https://confluence.ontotext.com/display/GraphDB6</a></li>
<p><strong>> Ontotext&#8217;s Q&#038;A forum</strong></p>
<li><a href="http://answers.ontotext.com">http://answers.ontotext.com</a></li>
<p>&nbsp;</p>
]]></content:encoded>
									<post-id xmlns="com-wordpress:feed-additions:1">2507</post-id>	</item>
		<item>
		<title>An introduction to Neo4j</title>
		<link>http://www.michelepasin.org/blog/2013/04/10/an-introduction-to-neo4j/</link>
				<pubDate>Wed, 10 Apr 2013 11:04:48 +0000</pubDate>
		<dc:creator><![CDATA[mikele]]></dc:creator>
				<category><![CDATA[Information Architecture]]></category>
		<category><![CDATA[database]]></category>
		<category><![CDATA[graph]]></category>
		<category><![CDATA[neo4j]]></category>

		<guid isPermaLink="false">http://www.michelepasin.org/blog/?p=2339</guid>
				<description><![CDATA[Neo4j is a recent graph-database that is rapidly accumulating success stories, especially in areas such as &#8220;social applications, recommendation engines, fraud detection, resource authorization, network &#038; data center management and much more&#8220;. Here&#8217;s an interesting introductory lecture about by Ian Robinson at JavaZone 2013. Tip: Databasetube offers various other interesting articles about neo4j A few [&#8230;]]]></description>
								<content:encoded><![CDATA[<p><a href="http://www.neo4j.org/">Neo4j</a> is a recent graph-database that is rapidly accumulating success stories, especially in areas such as &#8220;<em>social applications, recommendation engines, fraud detection, resource authorization, network &#038; data center management and much more</em>&#8220;. Here&#8217;s an interesting introductory lecture about by Ian Robinson at <a href="http://jz13.java.no/">JavaZone 2013</a>. </p>
<blockquote><p>Tip: <a href="http://www.databasetube.com/tag/neo4j/">Databasetube</a> offers various other interesting articles about neo4j</p></blockquote>
<p><iframe src="http://player.vimeo.com/video/49377272" width="400" height="300" frameborder="0" webkitAllowFullScreen mozallowfullscreen allowFullScreen></iframe></p>
<p>A few notes from the presentation:</p>
<pre style="background: lightyellow; overflow: auto;">
Premises: 
	- Data today is more connected than ever before
	- Complexity = f(size, semi-structure, connectedness)
	- Graphs are the best abstractions we have to model connectedness

The data model in neo4j: "property graph model"
	- nodes have properties (eg key-value pairs)
	- relationships have a direction, and can have properties too (eg weighted associations)

Neo4j server has a built in UI (web-based)

When to consider using a graph database:
	- lots of join tables [connectedness]
	- lots of sparse tables [semi-structure]

Neo4j fully supports ACID transactions
	- durable, consistent data
	- uses a try/success syntax

Performance
	- millions of 'joins' per second [connections are pre-calculated at insert time!]
	- consistent query times as dataset grows

Cypher query language
	- syntax mirrors the graphic representation of a graph 
	- one dimensional, left-to-right
	
</pre>
<blockquote><p>For a comparison of various graph databases (including Neo4j) check out this <a href="http://www.labf.usb.ve/TUTORIAL2013/ESWC2013Tutorial/Home_files/SlidesTutorialGraphDatabases_MAC.pdf">tutorial from the ESWC&#8217;13 conference</a></p></blockquote>
<p>&nbsp;</p>
]]></content:encoded>
									<post-id xmlns="com-wordpress:feed-additions:1">2339</post-id>	</item>
	</channel>
</rss>
