<?xml version="1.0" encoding="UTF-8"?><rss version="2.0"
	xmlns:content="http://purl.org/rss/1.0/modules/content/"
	xmlns:wfw="http://wellformedweb.org/CommentAPI/"
	xmlns:dc="http://purl.org/dc/elements/1.1/"
	xmlns:atom="http://www.w3.org/2005/Atom"
	xmlns:sy="http://purl.org/rss/1.0/modules/syndication/"
	xmlns:slash="http://purl.org/rss/1.0/modules/slash/"
	
	xmlns:georss="http://www.georss.org/georss"
	xmlns:geo="http://www.w3.org/2003/01/geo/wgs84_pos#"
	>

<channel>
	<title>linkeddata &#8211; Parerga und Paralipomena</title>
	<atom:link href="http://www.michelepasin.org/blog/tag/linkeddata/feed/" rel="self" type="application/rss+xml" />
	<link>http://www.michelepasin.org/blog</link>
	<description>At the core of all well-founded belief lies belief that is unfounded - Wittgenstein</description>
	<lastBuildDate>Mon, 08 Apr 2019 11:31:02 +0000</lastBuildDate>
	<language>en-US</language>
	<sy:updatePeriod>
	hourly	</sy:updatePeriod>
	<sy:updateFrequency>
	1	</sy:updateFrequency>
	<generator>https://wordpress.org/?v=5.2.11</generator>
<site xmlns="com-wordpress:feed-additions:1">13825966</site>	<item>
		<title>SN SciGraph Latest Release: Patents, Clinical Trials and many new features</title>
		<link>http://www.michelepasin.org/blog/2019/03/22/sn-scigraph-latest-release-patents-clinical-trials-and-many-new-features/</link>
				<pubDate>Fri, 22 Mar 2019 12:49:22 +0000</pubDate>
		<dc:creator><![CDATA[mikele]]></dc:creator>
				<category><![CDATA[Semantic Web]]></category>
		<category><![CDATA[jsonld]]></category>
		<category><![CDATA[linkeddata]]></category>
		<category><![CDATA[opendata]]></category>
		<category><![CDATA[schema.org]]></category>

		<guid isPermaLink="false">http://www.michelepasin.org/blog/?p=3307</guid>
				<description><![CDATA[We are pleased to announce the third release of SN SciGraph Linked Open Data. SN SciGraph is Springer Nature’s Linked Data platform that collates information from across the research landscape, i.e. the things, documents, people, places and relations of importance to the science and scholarly domain. This release includes a complete refactoring of the SN [&#8230;]]]></description>
								<content:encoded><![CDATA[<p><span style="font-weight: 400;">We are pleased to announce the third release of </span><a href="https://scigraph.springernature.com"><span style="font-weight: 400;">SN SciGraph</span></a><span style="font-weight: 400;"> Linked Open Data. SN SciGraph is Springer Nature’s Linked Data platform that collates information from across the research landscape, i.e. the things, documents, people, places and relations of importance to the science and scholarly domain.</span></p>
<p><span style="font-weight: 400;">This release includes a complete refactoring of the SN SciGraph data model. Following up on users feedback, we have simplified it using </span><a href="https://schema.org/"><span style="font-weight: 400;">Schema.org</span></a><span style="font-weight: 400;"> and </span><a href="https://en.wikipedia.org/wiki/JSON-LD"><span style="font-weight: 400;">JSON-LD</span></a><span style="font-weight: 400;">, so to make it easier to understand and consume the data also for non-linked data specialists.  </span></p>
<p><span style="font-weight: 400;">This release includes two brand new datasets &#8211; Patents and Clinical Trials linked to Springer Nature publications &#8211; which have been made available by our partner </span><a href="https://www.digital-science.com/"><span style="font-weight: 400;">Digital Science</span></a><span style="font-weight: 400;">, and in particular the </span><a href="http://dimensions.ai"><span style="font-weight: 400;">Dimensions</span></a><span style="font-weight: 400;"> team. </span></p>
<p><b>Highlights:</b></p>
<ul>
<li style="font-weight: 400;"><b>New Datasets.</b><span style="font-weight: 400;"> Data about clinical trials and patents connected to Springer Nature publications have been added. This data is sourced from </span><a href="https://www.dimensions.ai/"><span style="font-weight: 400;">Dimensions.ai</span></a><span style="font-weight: 400;">.</span></li>
<li style="font-weight: 400;"><b>New Ontology.</b> <a href="https://schema.org/"><span style="font-weight: 400;">Schema.org</span></a><span style="font-weight: 400;"> is now the main model used to represent SN SciGraph data.</span></li>
<li style="font-weight: 400;"><b>References data.</b><span style="font-weight: 400;"> Publications data now include references as well (= outgoing citations).</span></li>
<li style="font-weight: 400;"><b>Simpler Identifiers.</b><span style="font-weight: 400;"> URIs for SciGraph objects have been dramatically simplified, reusing common identifiers whenever possible. In particular all articles and chapters use the URI format prefix (&#8216;pub.&#8217;) + DOI (eg </span><span style="font-weight: 400;">pub.10.1007/s11199-007-9209-1</span><span style="font-weight: 400;">).</span></li>
<li style="font-weight: 400;"><b>JSON-LD.</b> <a href="https://en.wikipedia.org/wiki/JSON-LD"><span style="font-weight: 400;">JSON-LD</span></a><span style="font-weight: 400;"> is now the primary serialization format used by SN SciGraph.</span></li>
<li style="font-weight: 400;"><b>Downloads.</b><span style="font-weight: 400;"> Data dumps are now managed externally on </span><a href="https://sn-scigraph.figshare.com/"><span style="font-weight: 400;">FigShare</span></a><span style="font-weight: 400;"> and are referenceable via DOIs.</span></li>
<li><b>Continuous updates.</b><span> New publications data is released on a daily basis. All the other datasets are refreshed on a monthly basis.</span></li>
</ul>
<p>&nbsp;</p>
<p>Note: crossposted on <a href="https://researchdata.springernature.com/users/82895-sn-scigraph/posts/45943-sn-scigraph-latest-release-patents-clinical-trials-and-many-new-features">https://researchdata.springernature.com</a></p>
<p>&nbsp;</p>
<p><a href="https://scigraph.springernature.com/explorer"><img class="alignnone size-full wp-image-3308" src="https://i1.wp.com/www.michelepasin.org/blog/wp-content/uploads/2019/03/Screenshot-2019-03-22-at-12.33.41.png?w=591" alt="Screenshot 2019-03-22 at 12.33.41.png" width="591" height="431" /></a></p>
<p>&nbsp;</p>
<p><a href="https://scigraph.springernature.com/explorer/datasets/data_at_a_glance/"><img class="alignnone  wp-image-3309" src="https://i1.wp.com/www.michelepasin.org/blog/wp-content/uploads/2019/03/Screenshot-2019-03-22-at-12.33.54.png?w=589" alt="Screenshot 2019-03-22 at 12.33.54.png" width="602" height="434" /></a></p>
]]></content:encoded>
									<post-id xmlns="com-wordpress:feed-additions:1">3307</post-id>	</item>
		<item>
		<title>Exploring scholarly publications using DBPedia concepts: an experiment</title>
		<link>http://www.michelepasin.org/blog/2018/11/23/exploring-scholarly-publications-via-dbpedia/</link>
				<pubDate>Fri, 23 Nov 2018 17:18:48 +0000</pubDate>
		<dc:creator><![CDATA[mikele]]></dc:creator>
				<category><![CDATA[Information Architecture]]></category>
		<category><![CDATA[Semantic Web]]></category>
		<category><![CDATA[d3.js]]></category>
		<category><![CDATA[dbpedia]]></category>
		<category><![CDATA[linkeddata]]></category>

		<guid isPermaLink="false">http://www.michelepasin.org/blog/?p=3254</guid>
				<description><![CDATA[This post is about a recent prototype I developed, which allows to explore a sample collection of Springer Nature publications using subject tags automatically extracted from DBPedia. DBpedia is a crowd-sourced community effort to extract structured content from the information created in various Wikimedia projects. This structured information resembles an open knowledge graph (OKG) which is available for everyone [&#8230;]]]></description>
								<content:encoded><![CDATA[<p>This post is about a recent <a class="jive-link-external-small" href="http://hacks2019.michelepasin.org/dbpedialinks" target="_blank" rel="nofollow noopener">prototype</a> I developed, which allows to explore a sample collection of Springer Nature publications using subject tags automatically extracted from DBPedia.</p>
<blockquote><p><a class="jive-link-external-small" href="https://wiki.dbpedia.org/about" target="_blank" rel="nofollow noopener">DBpedia</a> is a crowd-sourced community effort to extract structured content from the information created in various Wikimedia projects. This structured information resembles an open knowledge graph (OKG) which is available for everyone on the Web.</p></blockquote>
<h3>Datasets</h3>
<p>The dataset I used is the result of a <strong>collaboration with <a href="https://twitter.com/beyza__yaman?lang=en">Beyza Yaman</a></strong>, a researcher working with the <a class="jive-link-external-small" href="https://wiki.dbpedia.org/about" target="_blank" rel="nofollow noopener">DBpedia</a> team in Leipzig, who used the <a class="jive-link-external-small" href="https://scigraph.springernature.com/explorer" target="_blank" rel="nofollow noopener">SciGraph datasets</a> as input to the <a class="jive-link-external-small" href="https://www.dbpedia-spotlight.org/" target="_blank" rel="nofollow noopener">DBPedia-Spotlight</a> entity-mining tool.</p>
<p>By using <a class="jive-link-external-small" href="https://www.dbpedia-spotlight.org/" target="_blank" rel="nofollow noopener">DBPedia-Spotlight</a> we automatically associated DBpedia subjects terms to a  subset of abstracts available in the SciGraph dataset (around 90k abstract from 2017 publications).</p>
<p>The prototype allows to search the Springer Nature publications using these subject terms.</p>
<p>Also, DBpedia subjects include definitions and semantic relationships (which we are currently not using, but one can imagine how they could be raw material for generating more thematic &#8216;pathways&#8217;).</p>
<h3>Results: serendipitous discovery of scientific publications</h3>
<p><strong>The results are pretty encouraging: </strong>despite the fact that the concepts extracted sometimes are only marginally relevant (or not relevant at all), the breadth and depth of the DBpedia classification makes the interactive <strong>exploration</strong> quite <strong>interesting</strong> and <strong>serendipitous</strong>.</p>
<p style="padding-left: 30px;">You can judge for yourself: <strong>the tool is available here</strong>: <a class="jive-link-external-small" href="http://hacks2019.michelepasin.org/dbpedialinks/" target="_blank" rel="nofollow noopener">http://hacks2019.michelepasin.org/dbpedialinks</a></p>
<p>The purpose of this prototype is to evaluate the quality of the tagging and generate ideas for future applications. So any kind of feedback or ideas is very welcome!</p>
<p>We are working with Beyza to write up the results of this investigation as a research paper. The data and software is already freely available on <a class="jive-link-external-small" href="https://github.com/dbpedia/sci-graph-links/" target="_blank" rel="nofollow noopener">github</a>.</p>
<h3>A couple of screenshots:</h3>
<p>Eg see the topic &#8216;<a href="http://hacks2019.michelepasin.org/dbpedialinks/entities/63004">artificial intelligence</a>&#8216;</p>
<p><a href="http://www.michelepasin.org/blog/wp-content/uploads/2018/11/Screen-Shot-2018-11-23-at-17.15.07.png"><img class=" wp-image-3255  aligncenter" src="http://www.michelepasin.org/blog/wp-content/uploads/2018/11/Screen-Shot-2018-11-23-at-17.15.07.png" alt="Screen Shot 2018-11-23 at 17.15.07.png" width="557" height="347" srcset="http://www.michelepasin.org/blog/wp-content/uploads/2018/11/Screen-Shot-2018-11-23-at-17.15.07.png 1201w, http://www.michelepasin.org/blog/wp-content/uploads/2018/11/Screen-Shot-2018-11-23-at-17.15.07-300x187.png 300w, http://www.michelepasin.org/blog/wp-content/uploads/2018/11/Screen-Shot-2018-11-23-at-17.15.07-768x478.png 768w, http://www.michelepasin.org/blog/wp-content/uploads/2018/11/Screen-Shot-2018-11-23-at-17.15.07-1024x638.png 1024w" sizes="(max-width: 557px) 100vw, 557px" /></a></p>
<p>One can add more subjects to a search in order to &#8216;zoom in&#8217; into a results set, eg by <a class="jive-link-external-small" href="http://hacks2019.michelepasin.org/dbpedialinks/entities/63004?filters=61361" target="_blank" rel="nofollow noopener">adding &#8216;China&#8217; </a>to the search:</p>
<p><a href="http://www.michelepasin.org/blog/wp-content/uploads/2018/11/Screen-Shot-2018-11-23-at-17.16.38.png"><img class="  wp-image-3256 aligncenter" src="http://www.michelepasin.org/blog/wp-content/uploads/2018/11/Screen-Shot-2018-11-23-at-17.16.38.png" alt="Screen Shot 2018-11-23 at 17.16.38" width="573" height="360" srcset="http://www.michelepasin.org/blog/wp-content/uploads/2018/11/Screen-Shot-2018-11-23-at-17.16.38.png 1217w, http://www.michelepasin.org/blog/wp-content/uploads/2018/11/Screen-Shot-2018-11-23-at-17.16.38-300x189.png 300w, http://www.michelepasin.org/blog/wp-content/uploads/2018/11/Screen-Shot-2018-11-23-at-17.16.38-768x483.png 768w, http://www.michelepasin.org/blog/wp-content/uploads/2018/11/Screen-Shot-2018-11-23-at-17.16.38-1024x644.png 1024w" sizes="(max-width: 573px) 100vw, 573px" /></a></p>
<h3>Implementation details</h3>
<ul>
<li>Main webapp is using Python and <a href="https://www.djangoproject.com/">Django</a>. I&#8217;ve finally found a good excuse to upgrade to the latest release (<a href="https://docs.djangoproject.com/en/2.1/">2.1</a>) so very proud of myself!</li>
<li><a href="https://getbootstrap.com/docs/3.3/components/">Bootstrap</a></li>
<li><a href="https://d3js.org/">d3.js </a>and in particular I&#8217;ve learned a lot from this great <a href="http://bl.ocks.org/eyaler/10586116">force-directed layout example</a></li>
<li>The Springer Nature <a href="https://scigraph.springernature.com">SciGraph</a> dataset of scientific publications</li>
<li>The <a href="https://www.dbpedia-spotlight.org/demo/">DBpedia Spotlight</a> tool</li>
</ul>
<p>&nbsp;</p>
<p>&nbsp;</p>
]]></content:encoded>
									<post-id xmlns="com-wordpress:feed-additions:1">3254</post-id>	</item>
		<item>
		<title>SN SciGraph: latest website release make it easier to discover related content</title>
		<link>http://www.michelepasin.org/blog/2018/08/01/sn-scigraph-latest-website-release-make-it-easier-to-discover-related-content/</link>
				<pubDate>Wed, 01 Aug 2018 08:26:19 +0000</pubDate>
		<dc:creator><![CDATA[mikele]]></dc:creator>
				<category><![CDATA[Information Architecture]]></category>
		<category><![CDATA[Semantic Web]]></category>
		<category><![CDATA[linkeddata]]></category>
		<category><![CDATA[navigation]]></category>
		<category><![CDATA[news]]></category>
		<category><![CDATA[opendata]]></category>
		<category><![CDATA[scigraph]]></category>

		<guid isPermaLink="false">http://www.michelepasin.org/blog/?p=3239</guid>
				<description><![CDATA[The latest release of SN SciGraph Explorer website includes a number of new features that make it easier to navigate the scholarly knowledge graph and discover items of interest. Graphs are essentially composed by two kinds of objects: nodes and edges. Nodes are like the stations in a train map, while edges are the links that [&#8230;]]]></description>
								<content:encoded><![CDATA[<p>The latest release of SN SciGraph Explorer website includes a number of new features that make it easier to navigate the scholarly knowledge graph and discover items of interest.</p>
<p>Graphs are essentially composed by two kinds of objects: <a href="https://en.wikipedia.org/wiki/Graph_theory" target="_blank" rel="noopener">nodes and edges</a>. Nodes are like the stations in a train map, while edges are the links that connect the different stations.</p>
<p>Of course one wants to be able to move from station to station in any direction! Similarly in a graph one wants to be able to jump back and forth from node to node using any of the links provided. That&#8217;s the beauty of it!</p>
<p>Although the underlying data allowed for this, the <a href="https://scigraph.springernature.com/explorer" target="_blank" rel="noopener">SN SciGraph Explorer website</a> wasn&#8217;t fully supporting this kind of navigation. So we&#8217;ve now started to add a number of &#8216;related objects&#8217; sections that reveal these pathways more clearly.</p>
<p>For example, now it&#8217;s much easier to get to the organizations and grants an <a href="https://scigraph.springernature.com/things/articles/847fd3229b37409e49155ed96d3aabd4" target="_blank" rel="noopener">article</a> relates to:</p>
<p><a href="https://scigraph.springernature.com/things/articles/847fd3229b37409e49155ed96d3aabd4" target="_blank" rel="noopener"><img class="  wp-image-3240 aligncenter" src="http://www.michelepasin.org/blog/wp-content/uploads/2018/08/Screen-Shot-2018-07-31-at-18.23.47.png" alt="Screen Shot 2018-07-31 at 18.23.47.png" width="624" height="424" /></a></p>
<p>Or, for a <a href="https://scigraph.springernature.com/things/book-editions/3101c98cc1558e4fd4e533e8b7caad87" target="_blank" rel="noopener">book edition</a>, to see its chapters and related organizations:</p>
<p><a href="https://scigraph.springernature.com/things/book-editions/3101c98cc1558e4fd4e533e8b7caad87" target="_blank" rel="noopener"><img class="  wp-image-3241 aligncenter" src="http://www.michelepasin.org/blog/wp-content/uploads/2018/08/bookEdition-links.png" alt="bookEdition-links.png" width="587" height="389" /></a></p>
<p>And much more..  Take a look at the <a href="https://scigraph.springernature.com/explorer" target="_blank" rel="noopener">site</a> yourself to find out.</p>
<p>Finally, we improved the <em>linked data </em>visualization included in every page by adding distinctive icons to each object type &#8211; so to make it easier to understand the immediate network of an object at a glance. E.g. see this <a href="https://scigraph.springernature.com/things/grants/c741f014f7e20fdd3bd510aac2c16693" target="_blank" rel="noopener">grant</a>:</p>
<p><a href="https://scigraph.springernature.com/things/grants/c741f014f7e20fdd3bd510aac2c16693" target="_blank" rel="noopener"><img class="  wp-image-3242 aligncenter" src="http://www.michelepasin.org/blog/wp-content/uploads/2018/08/grant-diagram.png" alt="grant-diagram.png" width="628" height="385" /></a></p>
<p>SN SciGraph is primarily about opening up new opportunities for open data and metadata enthusiasts who want to do more things with our content, so we hope that these additions will make discovering data items easier and more fun.</p>
<p>Any comments? We&#8217;d love to hear from you. Otherwise, thanks for reading and stay tuned for more updates.</p>
<p style="text-align: right;"><em><span style="color: #800000;">PS: this blog was posted on the <a href="https://researchdata.springernature.com/users/82895-sn-scigraph/posts/37235-sn-scigraph-latest-website-release-make-it-easier-to-discover-related-content">SN Research Data</a> space too.</span></em></p>
<p>&nbsp;</p>
]]></content:encoded>
									<post-id xmlns="com-wordpress:feed-additions:1">3239</post-id>	</item>
		<item>
		<title>SN SciGraph is part of the Linked Open Data Cloud 2018</title>
		<link>http://www.michelepasin.org/blog/2018/05/23/sn-scigraph-is-part-of-the-linked-open-data-cloud-2018/</link>
				<pubDate>Wed, 23 May 2018 11:28:59 +0000</pubDate>
		<dc:creator><![CDATA[mikele]]></dc:creator>
				<category><![CDATA[Semantic Web]]></category>
		<category><![CDATA[linkeddata]]></category>
		<category><![CDATA[lod cloud]]></category>

		<guid isPermaLink="false">http://www.michelepasin.org/blog/?p=3311</guid>
				<description><![CDATA[The latest Linked Open Data (LOD) Cloud has been recently made available by the Insights Centre for Data Analytics. The LOD cloud is a visual representation of the datasets (and the links among them) that have been published according to the Linked Data principles &#8211; a web-friendly methodology for data sharing that encourages open schemas and data reuse. &#160; We&#8217;ve very glad to [&#8230;]]]></description>
								<content:encoded><![CDATA[<p>The latest <a class="jive-link-external-small" href="https://lod-cloud.net/#about" target="_blank" rel="nofollow noopener">Linked Open Data (LOD) Cloud</a> has been recently made available by the <a class="jive-link-external-small" href="http://www.insight-centre.org" target="_blank" rel="nofollow noopener">Insights Centre for Data Analytics</a>. The LOD cloud is a visual representation of the datasets (and the links among them) that have been published according to the <a class="jive-link-external-small" href="https://www.w3.org/DesignIssues/LinkedData.html" target="_blank" rel="nofollow noopener">Linked Data principles</a> &#8211; a web-friendly methodology for data sharing that encourages open schemas and data reuse.</p>
<p><a href="http://www.michelepasin.org/blog/wp-content/uploads/2019/04/ScreenShot2018-05-23at10.10.09.png"><img class="aligncenter size-full wp-image-3312" src="http://www.michelepasin.org/blog/wp-content/uploads/2019/04/ScreenShot2018-05-23at10.10.09.png" alt="Screen+Shot+2018-05-23+at+10.10.09.png" width="596" height="593" srcset="http://www.michelepasin.org/blog/wp-content/uploads/2019/04/ScreenShot2018-05-23at10.10.09.png 596w, http://www.michelepasin.org/blog/wp-content/uploads/2019/04/ScreenShot2018-05-23at10.10.09-150x150.png 150w, http://www.michelepasin.org/blog/wp-content/uploads/2019/04/ScreenShot2018-05-23at10.10.09-300x298.png 300w, http://www.michelepasin.org/blog/wp-content/uploads/2019/04/ScreenShot2018-05-23at10.10.09-144x144.png 144w" sizes="(max-width: 596px) 100vw, 596px" /></a></p>
<p>&nbsp;</p>
<p>We&#8217;ve very glad to say that Sn SciGraph is now part of it! (ps this is its <a class="jive-link-external-small" href="https://lod-cloud.net/dataset/springernaturescigraph" target="_blank" rel="nofollow noopener">JSON record</a>) If you look at the picture above, the two red lines departing from our &#8216;bubble&#8217; indicate that the two main datasets we are linking to are CrossRef and DBpedia.</p>
<p><em><strong>Note</strong> this visualisation unfortunately doesn&#8217;t do justice to the fact that SN SciGraph is one of the largest datasets out there (1 billion + triples and counting). In previous versions, the bubble&#8217;s size would reflect how large a dataset is.. but hopefully that&#8217;ll change in the future!</em></p>
<p>The cloud currently contains 1,184 datasets with 15,993 links (as of April 2018) and it&#8217;s divided into 9 sub-clouds based on their domain.</p>
<p><a href="http://www.michelepasin.org/blog/wp-content/uploads/2019/04/ScreenShot2018-05-22at16.14.11.png"><img class="aligncenter size-full wp-image-3313" src="http://www.michelepasin.org/blog/wp-content/uploads/2019/04/ScreenShot2018-05-22at16.14.11.png" alt="Screen+Shot+2018-05-22+at+16.14.11.png" width="487" height="482" srcset="http://www.michelepasin.org/blog/wp-content/uploads/2019/04/ScreenShot2018-05-22at16.14.11.png 487w, http://www.michelepasin.org/blog/wp-content/uploads/2019/04/ScreenShot2018-05-22at16.14.11-300x297.png 300w, http://www.michelepasin.org/blog/wp-content/uploads/2019/04/ScreenShot2018-05-22at16.14.11-144x144.png 144w" sizes="(max-width: 487px) 100vw, 487px" /></a></p>
<p>SciGraph is part of the <a class="jive-link-external-small" href="https://lod-cloud.net/clouds/publications-lod.svg" target="_blank" rel="nofollow noopener">&#8216;Publications&#8217; sub-cloud</a> (depicted above) alongside other important linked data publishers such as the <a class="jive-link-external-small" href="http://bnb.data.bl.uk" target="_blank" rel="nofollow noopener">British Library</a>, the <a class="jive-link-external-small" href="http://www.dnb.de/EN/Service/DigitaleDienste/LinkedData/linkeddata_node.html" target="_blank" rel="nofollow noopener">German National Library</a>, the <a class="jive-link-external-small" href="https://openlibrary.org" target="_blank" rel="nofollow noopener">Open Library</a>, <a class="jive-link-external-small" href="http://experimental.worldcat.org/fast/" target="_blank" rel="nofollow noopener">OCLC</a> and many others.</p>
<p>It&#8217;s <strong>impressive</strong> to see the growing number of datasets being released using this approach! We&#8217;ve been told that later this year more discovery tools will be made available that allow searching for data publishers, so to make it easier for people and projects to collaborate.</p>
<p>Useful links:</p>
<ul>
<li>The LOD Cloud: <a class="link-titled" href="https://lod-cloud.net/#about" target="_blank" rel="nofollow noopener">The Linked Open Data Cloud</a></li>
<li>The LOD publications cloud: <a class="link-titled" href="https://lod-cloud.net/clouds/publications-lod.svg" target="_blank" rel="nofollow noopener">http://lod-cloud.net/clouds/publications-lod.svg</a></li>
</ul>
<p>&nbsp;</p>
<p>&nbsp;</p>
]]></content:encoded>
									<post-id xmlns="com-wordpress:feed-additions:1">3311</post-id>	</item>
		<item>
		<title>Exploring SciGraph data using JSON-LD, Elastic Search and Kibana</title>
		<link>http://www.michelepasin.org/blog/2017/04/06/exploring-scigraph-data-using-elastic-search-and-kibana/</link>
				<comments>http://www.michelepasin.org/blog/2017/04/06/exploring-scigraph-data-using-elastic-search-and-kibana/#comments</comments>
				<pubDate>Thu, 06 Apr 2017 14:12:05 +0000</pubDate>
		<dc:creator><![CDATA[mikele]]></dc:creator>
				<category><![CDATA[Information Architecture]]></category>
		<category><![CDATA[Semantic Web]]></category>
		<category><![CDATA[data exploration]]></category>
		<category><![CDATA[elasticsearch]]></category>
		<category><![CDATA[graph]]></category>
		<category><![CDATA[jsonld]]></category>
		<category><![CDATA[kibana]]></category>
		<category><![CDATA[linkeddata]]></category>
		<category><![CDATA[nature]]></category>
		<category><![CDATA[scigraph]]></category>
		<category><![CDATA[visualization]]></category>

		<guid isPermaLink="false">http://www.michelepasin.org/blog/?p=2844</guid>
				<description><![CDATA[Hello there data lovers! In this post you can find some information on how to download and make some sense of the scholarly dataset recently made available by the Springer Nature SciGraph project, by using the freely available Elasticsearch suite of software. A few weeks ago the SciGraph dataset was released (full disclosure: I&#8217;m part [&#8230;]]]></description>
								<content:encoded><![CDATA[<p>Hello there data lovers! In this post you can find some information on how to download and make some sense of the scholarly dataset recently made available by the Springer Nature <a href="http://www.springernature.com/scigraph">SciGraph project</a>, by using the freely available <a href="https://en.wikipedia.org/wiki/Elasticsearch">Elasticsearch</a> suite of software.</p>
<p>A few weeks ago the SciGraph dataset was <a href="http://www.springernature.com/gp/group/media/press-releases/springer-nature-scigraph--supporting-open-science-and-the-wider-understanding-of-research/12129614">released</a> (full disclosure: I&#8217;m part of the team who did that!). This is a high quality dataset containing metadata and abstracts about scientific articles published by <a href="https://en.wikipedia.org/wiki/Springer_Nature">Springer Nature</a>, research grants related to them plus other classifications of this content.</p>
<p><img class="alignnone size-full wp-image-3123" src="http://www.michelepasin.org/blog/wp-content/uploads/2017/04/scigraph.png?w=1116" alt="scigraph.png" width="558" height="142" srcset="http://www.michelepasin.org/blog/wp-content/uploads/2017/04/scigraph.png 1888w, http://www.michelepasin.org/blog/wp-content/uploads/2017/04/scigraph-300x76.png 300w, http://www.michelepasin.org/blog/wp-content/uploads/2017/04/scigraph-768x195.png 768w, http://www.michelepasin.org/blog/wp-content/uploads/2017/04/scigraph-1024x260.png 1024w" sizes="(max-width: 558px) 100vw, 558px" /></p>
<p>This release of the dataset includes the last 5 years of content &#8211; that&#8217;s already an impressive <strong>32 gigs of data</strong> you can get your hands on. So in this post I&#8217;m going to show how to do that, in particular by transforming the data from the <a href="https://en.wikipedia.org/wiki/Resource_Description_Framework">RDF graph</a> format they come with, into a <a href="https://en.wikipedia.org/wiki/JSON">JSON format</a> which is more suited for application development and analytics.</p>
<p>We will be using two free-to-download products, <a href="http://ontotext.com/products/graphdb/">GraphDB</a> and <a href="https://www.elastic.co/">Elasticsearch</a>, so you&#8217;ll have to install them if you haven&#8217;t got them already. But no worries, that&#8217;s pretty straighforward, as you&#8217;ll see below.</p>
<h2>1. Hello SciGraph Linked Data</h2>
<p>First things first, we want to get hold of the SciGraph RDF datasets of course. That&#8217;s pretty easy, just head over to the SciGraph <a href="https://github.com/springernature/scigraph/wiki#downloads">downloads page</a> and get the following datasets:</p>
<ul>
<li><strong>Ontologies</strong>: the main <a href="http://s3-service-broker-live-afe45d64-24d0-4a96-b6a8-23b79e885eb7.s3-website.eu-central-1.amazonaws.com/2017-02-15/springernature-scigraph-ontologies.2017-02-15.nt.bz2">schema</a> behind SciGraph.</li>
<li><strong>Articles</strong> <strong>&#8211; 2016</strong>: all the core <a href="http://s3-service-broker-live-afe45d64-24d0-4a96-b6a8-23b79e885eb7.s3-website.eu-central-1.amazonaws.com/2017-02-15/springernature-scigraph-2016-articles.2017-02-15.nt.bz2">articles</a> metadata for one year.</li>
<li><strong>Grants</strong>: <a href="http://s3-service-broker-live-afe45d64-24d0-4a96-b6a8-23b79e885eb7.s3-website.eu-central-1.amazonaws.com/2017-02-15/springernature-scigraph-grants.2017-02-15.nt.bz2">grants</a> metadata related to those articles.</li>
<li><strong>Journals</strong>: full list of Springer Nature <a href="http://s3-service-broker-live-afe45d64-24d0-4a96-b6a8-23b79e885eb7.s3-website.eu-central-1.amazonaws.com/2017-02-15/springernature-scigraph-journals.2017-02-15.nt.bz2">journal catalogue</a>.</li>
<li><strong>Subjects</strong>: classification of <a href="http://s3-service-broker-live-afe45d64-24d0-4a96-b6a8-23b79e885eb7.s3-website.eu-central-1.amazonaws.com/2017-02-15/springernature-scigraph-subjects.2017-02-15.nt.bz2">research areas</a> developed by Springer Nature.</li>
</ul>
<p>That&#8217;s pretty much everything, only thing we&#8217;re getting only one year worth of articles as that&#8217;s enough for the purpose of this exercise (~300k articles from 2016).</p>
<p>Next up, we want to get a couple of other datasets SciGraph depends on:</p>
<ul>
<li><strong>GRID</strong>: a catalogue of the world&#8217;s research organisations. Make sure you get both the <a href="http://www.grid.ac/ontology/">ontology</a> and one of the <a href="https://www.grid.ac/downloads">latest releases</a>, within which you can find an RDF implementation too.</li>
<li><strong>Field Of Research codes</strong>: another classification scheme used in SciGraph, developed by the <a href="https://vocabs.ands.org.au/anzsrc-for">Australian and New Zealand Standard Research Classification</a> organization.</li>
</ul>
<p>That&#8217;s it! Time for a cup of coffee.</p>
<h2>2. Python to the help</h2>
<p>We will be doing a bit of data manipulation  in the next sections and Python is a great language for that sort of thing. Here&#8217;s what we need to get going:</p>
<ol>
<li><strong>Python</strong>. Make sure you have <a href="https://wiki.python.org/moin/BeginnersGuide/Download">Python installed</a> and also <a href="https://packaging.python.org/installing/">Pip</a>, the Python package manager (any Python version <strong>above 2.7</strong> should be ok).</li>
<li><strong>GitHub project</strong>. I&#8217;ve created a few scripts for this tutorial, so head over to the <a href="https://github.com/lambdamusic/hello-scigraph">hello-scigraph project on GitHub</a> and download it to your computer. Note: the project contains all the Python scripts needed to complete this tutorial, but of course you should feel free to modify them or write from scratch if you fancy it!</li>
<li><strong>Libraries</strong>. Install all the dependencies for the hello-scigraph project to run. You can do that by cd-ing into the project folder and running <code style="display: inline; padding: 0px;">pip install -r requirements.txt</code> (ideally within a <a href="http://python-guide-pt-br.readthedocs.io/en/latest/dev/virtualenvs/">virtual environment</a>, but that&#8217;s up to you).</li>
</ol>
<h2>3. Loading the data into GraphDB</h2>
<p>So, you should have by now 8 different files containing data (after step 1 above). Make sure they&#8217;re all in the same folder and that all of them have been unzipped (if needed), then head over to the <a href="http://ontotext.com/graphdb-8-enterprise-linked-data/">GraphDB website</a> and download the free version of the triplestore (you may have to sign up first).</p>
<p>The <a href="http://graphdb.ontotext.com/documentation/free/quick-start-guide.html#run-graphdb-as-a-stand-alone-server">online documentation</a> for GraphDB is pretty good, so it should be easy to get it up and running. In essence, you have to do the following steps:</p>
<ol>
<li><strong>Launch the application</strong>: for me, on a mac, I just had to double click the GraphDB icon &#8211; nice!</li>
<li><strong>Create a</strong> <a href="http://graphdb.ontotext.com/documentation/free/quick-start-guide.html#create-a-repository">new repository</a>: this is the equivalent of a database within the triplestore. Call this repo <span class="pl-pds">&#8220;</span><strong>scigraph-2016</strong><span class="pl-pds">&#8221; so that we&#8217;re all synced for the following steps.</span></li>
</ol>
<p>Next thing, we want a script to load our RDF files into this empty repository. So cd into the directory containg the GitHub project (from step 2) and run the following command:</p>
<pre style="background: lightgray; padding: 5px;">python -m hello-scigraph.loadGraphDB ~/scigraph-downloads/</pre>
<p>The &#8220;loadGraphDB&#8221; script goes through all RDF files in the &#8220;scigraph-downloads&#8221; directory and loads them into the <strong>scigraph-2016 </strong>repository (note: you must replace &#8220;scigraph-downloads&#8221; with the actual path to the folder you downloaded content in step 1 above).</p>
<p><strong>So, to recap:</strong> this script is now loading more than 35 million triples into your local graph database. Don&#8217;t be surprised if it&#8217;ll take some time (in particular the &#8216;articles-2016&#8217; dataset, by far the biggest) so it&#8217;s time to take a break or do something else.</p>
<p>Once the process it&#8217;s finished, you should be able to <a href="http://graphdb.ontotext.com/documentation/free/quick-start-guide.html#explore-your-data-and-class-relationships">explore your data via the GraphDB workbench</a>.  It&#8217;ll look something like this:</p>
<p><img class="aligncenter size-large wp-image-2954" src="http://www.michelepasin.org/blog/wp-content/uploads/2017/04/GraphDB-class-hierarchy-1024x525.png" alt="GraphDB-class-hierarchy" width="1024" height="525" srcset="http://www.michelepasin.org/blog/wp-content/uploads/2017/04/GraphDB-class-hierarchy-1024x525.png 1024w, http://www.michelepasin.org/blog/wp-content/uploads/2017/04/GraphDB-class-hierarchy-300x154.png 300w, http://www.michelepasin.org/blog/wp-content/uploads/2017/04/GraphDB-class-hierarchy-768x394.png 768w" sizes="(max-width: 1024px) 100vw, 1024px" /></p>
<h2>4. Creating an Elasticsearch index</h2>
<p>We&#8217;re almost there. Let&#8217;s head over to the <a href="https://www.elastic.co/downloads/elasticsearch">Elasticsearch website</a> and <strong>download</strong> it. Elasticsearch is a powerful, distributed, JSON-based search and analytics engine so we&#8217;ll be using it to build an analytics dashboard for the SciGraph data.</p>
<p>Make sure Elastic is running (run <code style="display: inline; padding: 0px;">bin/elasticsearch</code> (or <code style="display: inline; padding: 0px;">bin\elasticsearch.bat</code> on Windows), then cd into the hello-scigraph Python project (from step 2) in order to run the following script:</p>
<pre style="background: lightgray; padding: 5px;">python -m hello-scigraph.loadElastic</pre>
<p>If you <a href="https://github.com/lambdamusic/hello-scigraph/blob/master/hello-scigraph/loadElastic.py">take a look at the source code</a>, you&#8217;ll see that the script does the following:</p>
<ol>
<li><strong>Articles loading:</strong> extracts articles references from GraphDB in batches of 200.</li>
<li><strong>Articles metadata extraction:</strong> for each article, we <a href="https://github.com/lambdamusic/hello-scigraph/blob/master/hello-scigraph/queries.py">pull out all relevant metadata</a> (e.g. title, DOI, authors) plus related information (e.g. author GRID organizations, geo locations, funding info etc..).</li>
<li><strong>Articles metadata simplification: </strong> some intermediate nodes coming from the orginal RDF graph are dropped and replaced with a flatter structure which uses a a temporary dummy schema (<code style="display: inline; padding: 0px;">prefix es: &lt;http://elastic-index.scigraph.com/&gt;</code> It doesn&#8217;t matter what we call that schema, but what&#8217;s important is to that we want to simplify the data we put into the Elastic search index. That&#8217;s because while the Graph layer is supposed to facilitate <em>data integration </em>and hence it benefits from a rich semantic representation of information, the <em>search layer</em> is more geared towards performance and retrieval hence a leaner information structure can dramatically speed things up there.</li>
<li><strong>JSON-LD transformation</strong>: the simplified RDF data structure is <a href="http://json-ld.org/">serialized as JSON-LD</a> &#8211; one of the many serializations available for RDF. JSON-LD is of course valid JSON, meaning that we can put that into Elastic right away. This is a bit of a shortcut actually, in fact for a more fine-grained control of how the JSON looks like,  it&#8217;s probably better to transform the data into JSON using some ad-hoc mechanism. But for the purpose of this tutorial it&#8217;s more than enough.</li>
<li><strong>Elastic index creation.</strong> Finally, we can load the data into an Elastic index called &#8211; guess what &#8211; &#8220;hello-scigraph&#8221;.</li>
</ol>
<p>Two more things to point out:</p>
<ul>
<li><strong>Long queries.</strong> The Python script enforces a 60 seconds <a href="https://github.com/lambdamusic/hello-scigraph/blob/master/hello-scigraph/timeout.py">time-out </a>on the GraphDB queries, so in case things go wrong with some articles data the script should keep running.</li>
<li><strong>Memory issues.</strong> The script stops for 10 seconds after each batch of 200 articles (<code style="display: inline; padding: 0px;">time.sleep(10)</code>). Had to do this to prevent GraphDB on my laptop from running out of memory. Time to catch some breath!</li>
</ul>
<p>That&#8217;s it! <strong>Time for another break  now.</strong> A pretty long one actually &#8211; loading all the data took around 10 hours on my (rather averaged spec&#8217;ed) laptop so you may want to do that overnight or get hold of a faster machine/server.</p>
<p>Eventually, once the loading script is finished, you can issue this command from the command line to see how much data you&#8217;ve loaded into the Elastic index  &#8220;hello-scigraph&#8221;. Bravo!</p>
<pre style="background: lightgray; padding: 5px;">curl -XGET 'localhost:9200/_cat/indices/'</pre>
<h2>5. Analyzing the data with Kibana</h2>
<p>Loading the data in Elastic already opens up a number of possibilites &#8211; check out the <a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/search.html">search APIs </a>for some ideas &#8211; however there&#8217;s an even quicker way to analyze the data: <strong>Kibana</strong>. <a href="https://www.elastic.co/products/kibana">Kibana</a> is another free product in the Elastic Search suite, which provides an extensible user interface for configuring and managing all aspects of the Elastic Stack.</p>
<p>So let&#8217;s get started with Kibana: <a href="https://www.elastic.co/downloads/kibana">download it</a> and set it up using the online instructions, then point your browser at <a href="http://localhost:5601">http://localhost:5601 </a>.</p>
<p>You&#8217;ll get to the Kibana dashboard which shows the index we just created. Here you can perform any kind of searches and see the raw data as JSON.</p>
<p>What&#8217;s even more interesting is the <strong>visualization tab</strong>. Results of searches can be rendered as line chart, pie charts etc.. and more dimensions can be added via &#8216;buckets&#8217;. See below for some quick examples, but really, the possibilities are endless!</p>

<a href='http://www.michelepasin.org/blog/2017/04/06/exploring-scigraph-data-using-elastic-search-and-kibana/kibana-super-collider-1/'><img width="300" height="153" src="http://www.michelepasin.org/blog/wp-content/uploads/2017/04/Kibana-super-collider-1-300x153.png" class="attachment-medium size-medium" alt="" srcset="http://www.michelepasin.org/blog/wp-content/uploads/2017/04/Kibana-super-collider-1-300x153.png 300w, http://www.michelepasin.org/blog/wp-content/uploads/2017/04/Kibana-super-collider-1-768x391.png 768w, http://www.michelepasin.org/blog/wp-content/uploads/2017/04/Kibana-super-collider-1-1024x521.png 1024w" sizes="(max-width: 300px) 100vw, 300px" /></a>
<a href='http://www.michelepasin.org/blog/2017/04/06/exploring-scigraph-data-using-elastic-search-and-kibana/kibana-super-collider-2/'><img width="300" height="153" src="http://www.michelepasin.org/blog/wp-content/uploads/2017/04/Kibana-super-collider-2-300x153.png" class="attachment-medium size-medium" alt="" srcset="http://www.michelepasin.org/blog/wp-content/uploads/2017/04/Kibana-super-collider-2-300x153.png 300w, http://www.michelepasin.org/blog/wp-content/uploads/2017/04/Kibana-super-collider-2-768x391.png 768w, http://www.michelepasin.org/blog/wp-content/uploads/2017/04/Kibana-super-collider-2-1024x521.png 1024w" sizes="(max-width: 300px) 100vw, 300px" /></a>
<a href='http://www.michelepasin.org/blog/2017/04/06/exploring-scigraph-data-using-elastic-search-and-kibana/kibana-super-collider-3/'><img width="300" height="171" src="http://www.michelepasin.org/blog/wp-content/uploads/2017/04/Kibana-super-collider-3-300x171.png" class="attachment-medium size-medium" alt="" srcset="http://www.michelepasin.org/blog/wp-content/uploads/2017/04/Kibana-super-collider-3-300x171.png 300w, http://www.michelepasin.org/blog/wp-content/uploads/2017/04/Kibana-super-collider-3-768x438.png 768w, http://www.michelepasin.org/blog/wp-content/uploads/2017/04/Kibana-super-collider-3-1024x584.png 1024w" sizes="(max-width: 300px) 100vw, 300px" /></a>
<a href='http://www.michelepasin.org/blog/2017/04/06/exploring-scigraph-data-using-elastic-search-and-kibana/kibana-super-collider-4/'><img width="300" height="173" src="http://www.michelepasin.org/blog/wp-content/uploads/2017/04/Kibana-super-collider-4-300x173.png" class="attachment-medium size-medium" alt="" srcset="http://www.michelepasin.org/blog/wp-content/uploads/2017/04/Kibana-super-collider-4-300x173.png 300w, http://www.michelepasin.org/blog/wp-content/uploads/2017/04/Kibana-super-collider-4-768x444.png 768w, http://www.michelepasin.org/blog/wp-content/uploads/2017/04/Kibana-super-collider-4-1024x592.png 1024w" sizes="(max-width: 300px) 100vw, 300px" /></a>

<h2>Conclusion</h2>
<p>This post should have given you enough to realise that:</p>
<ol>
<li>The <strong>SciGraph dataset</strong> contain an impressive amount of high-quality scholarly publications metadata which can be used for things like literature search, research statistics etc..</li>
<li>Even though you&#8217;re not familiar with Linked Data and the RDF family of languages, it&#8217;s not hard to get going with a triplestore and then <strong>transform the data</strong> into a more widely used format like JSON.</li>
<li>Finally, <strong>Elasticsearch</strong> and especially <strong>Kibana</strong> are fantastic tools for data analysis and exploration! Needless to say, in this post I&#8217;ve just scratched the surface of what could be done with it.</li>
</ol>
<p>Hope this was fun, any questions or comments, you know the drill :-)</p>
]]></content:encoded>
							<wfw:commentRss>http://www.michelepasin.org/blog/2017/04/06/exploring-scigraph-data-using-elastic-search-and-kibana/feed/</wfw:commentRss>
		<slash:comments>4</slash:comments>
						<post-id xmlns="com-wordpress:feed-additions:1">2844</post-id>	</item>
		<item>
		<title>Recent projects from CrossRef.org</title>
		<link>http://www.michelepasin.org/blog/2015/06/14/recent-projects-from-crossref-org/</link>
				<pubDate>Sun, 14 Jun 2015 22:21:55 +0000</pubDate>
		<dc:creator><![CDATA[mikele]]></dc:creator>
				<category><![CDATA[Just Blogging]]></category>
		<category><![CDATA[Semantic Web]]></category>
		<category><![CDATA[doi]]></category>
		<category><![CDATA[linkeddata]]></category>
		<category><![CDATA[publishing]]></category>

		<guid isPermaLink="false">http://www.michelepasin.org/blog/?p=2638</guid>
				<description><![CDATA[We spent the day with the CrossRef team in Oxford last week, talking about our recent work in the linked data space (see the nature ontologies portal) and their recent initiatives in the scholarly publishing area. So here&#8217;s a couple of interesting follow ups from the meeting. ps. If you want to know more about [&#8230;]]]></description>
								<content:encoded><![CDATA[<p>We spent the day with the CrossRef team in Oxford last week, talking about our recent work in the linked data space (see the <a href="https://campus.macmillan.com/groups/data-information-architecture/blog/2015/04/23/new-release-of-naturecomontologies">nature ontologies portal</a>) and their recent initiatives in the scholarly publishing area.</p>
<p>So here&#8217;s a couple of interesting follow ups from the meeting.<br />
ps. If you want to know more about CrossRef, make sure you take a look at their <a href="http://www.crossref.org/">website</a> and in particular the labs section: <a href="http://labs.crossref.org/">http://labs.crossref.org/</a>.</p>
<h2>Opening up article level metrics</h2>
<p><a href="http://det.labs.crossref.org/">http://det.labs.crossref.org/</a></p>
<p>CrossRef is using the open source <strong>Lagotto</strong> application (developed by PLOS <a href="https://github.com/articlemetrics/lagotto">https://github.com/articlemetrics/lagotto</a>) to <strong>retrieve article metrics data from a variety of sources</strong> (e.g. wikipedia, twitter etc. see the full list <a href="http://det.labs.crossref.org/docs/sources">here</a>).</p>
<p>The model used for storing this data follows an agreed ontology containing for example a classification of &#8216;mentions&#8217; actions (viewed/saved/discussed/recommended/cited &#8211; see this <a href="http://www.niso.org/publications/isq/2013/v25no2/lin/">paper</a> for more details).</p>
<p>In a nutshell, CrossRef is planning to collect and make the metrics (raw) data for all the DOIs they track in the form of &#8216;<a href="http://events.labs.crossref.org/">DOI events</a>&#8216;</p>
<p>An interesting demo application shows the <a href="http://events.labs.crossref.org/">stream of DOIs citations coming from Wikipedia</a> (one of the top referrers of DOIs, unsurprisingly). More discussions on this <a href="http://crosstech.crossref.org/2015/03/real-time-stream-of-dois-being-cited-in-the-wikipedia.html">blog post</a>.</p>
<p><a href="http://www.michelepasin.org/blog/wp-content/uploads/2015/06/Screen-Shot-2015-05-20-at-16.30.00-1024x760.png"><img src="http://www.michelepasin.org/blog/wp-content/uploads/2015/06/Screen-Shot-2015-05-20-at-16.30.00-1024x760.png"  alt="Screen Shot 2015 05 20 at 16 30 00 1024x760" border="0" width="600" style="width: 600px; margin-left:auto; margin-right:auto; " /></a></p>
<h2>Linking dataset DOIs and publications DOIs</h2>
<p> <a href="http://www.crosscite.org/">http://www.crosscite.org/</a></p>
<p>CrossRef has been working with <a href="https://www.datacite.org/">Datacite</a> to the goal of <strong>harmonising their databases</strong>. Datacite is the second major register of DOIs (after CrossRef) and it has been focusing on assigning persistent identifiers to <strong>datasets</strong>.</p>
<p>This work is now gaining more momentum as Datacite is <a href="https://www.datacite.org/news/job-offer-technical-architect-thor-project-datacite.html">enlarging its team</a>. So in theory it won&#8217;t be long before we see a service that allows to interlink publications and datasets, which is great news. </p>
<h2>Linking publications and funding sources</h2>
<p> <a href="http://www.crossref.org/fundref/">http://www.crossref.org/fundref/</a></p>
<p>FundRef provides a <strong>standard way to report funding sources for published scholarly research</strong>. This is increasingly becoming a fundamental <strong>requirement</strong> for all publicly funded research, so several publishers have agreed to help extracting funding information and sending it to CrossRef.</p>
<p>A recent platform built on top of Fundref is Chorus <a href="http://www.chorusaccess.org/">http://www.chorusaccess.org/</a>, which enables users to discover articles reporting on funded research. Furthermore it provides <strong>dashboards</strong> which can b used by funders, institutions, researchers, publishers, and the public for monitoring and tracking public-access compliance for articles reporting on funded research.</p>
<p>For example see <a href="http://dashboard.chorusaccess.org/ahrq#/breakdown">http://dashboard.chorusaccess.org/ahrq#/breakdown</a></p>
<p><a href="http://www.michelepasin.org/blog/wp-content/uploads/2015/06/Screen-Shot-2015-06-11-at-12.57.39.png"><img src="http://www.michelepasin.org/blog/wp-content/uploads/2015/06/Screen-Shot-2015-06-11-at-12.57.39.png"  alt="Screen Shot 2015 06 11 at 12 57 39" border="0" width="600" style="width: 600px; margin-left:auto; margin-right:auto; " /></a></p>
<h2>Miscellaneous news &#038; links</h2>
<p>&#8211; <a href="http://json-ld.org/">JSON-LD</a> (an RDF version of JSON) is being considered as a candidate data format for the next generation of the <a href="https://github.com/CrossRef/rest-api-doc/blob/master/rest_api.md">CrossRef REST API</a>.</p>
<p>&#8211; The prototype <a href="http://www.yamz.net/">http://www.yamz.net/</a> came up in discussion; a quite interesting stack-overflow meets ontology-engineering kind of tool. Def worth a look, I&#8217;d say.</p>
<p>&#8211; <a href="http://www.wikidata.org/wiki/Wikidata:Main_Page">Wikidata</a> (a queryable structured <a href="http://www.wikidata.org/wiki/Wikidata:Data_access">data version</a> of wikipedia) seems to be gaining a lot of momentum after it&#8217;s <a href="http://searchengineland.com/google-close-freebase-helped-feed-knowledge-graph-211103">taken over Freebase from Google</a>. Will it eventually replace its main rival <a href="https://en.wikipedia.org/wiki/DBpedia">DBpedia</a>?  </p>
<p><a href="http://www.michelepasin.org/blog/wp-content/uploads/2015/06/Screen-Shot-2015-06-11-at-12.58.20.png"><img src="http://www.michelepasin.org/blog/wp-content/uploads/2015/06/Screen-Shot-2015-06-11-at-12.58.20.png"  alt="Screen Shot 2015 06 11 at 12 58 20" border="0" width="600" style="width: 600px; margin-left:auto; margin-right:auto; " /></a></p>
<p>&nbsp;</p>
]]></content:encoded>
									<post-id xmlns="com-wordpress:feed-additions:1">2638</post-id>	</item>
		<item>
		<title>Nature.com ontologies portal available online</title>
		<link>http://www.michelepasin.org/blog/2015/04/30/nature-com-ontologies-portal-available-online/</link>
				<pubDate>Thu, 30 Apr 2015 21:46:42 +0000</pubDate>
		<dc:creator><![CDATA[mikele]]></dc:creator>
				<category><![CDATA[Just Blogging]]></category>
		<category><![CDATA[Semantic Web]]></category>
		<category><![CDATA[linkeddata]]></category>
		<category><![CDATA[ontology]]></category>

		<guid isPermaLink="false">http://www.michelepasin.org/blog/?p=2618</guid>
				<description><![CDATA[The Nature ontologies portal is new section of the nature.com site that describes our involvement with semantic technologies and also makes available to the wider public several models and datasets as RDF linked data. We launched the portal nearly a month ago, to the purpose of sharing our experiences with semantic technologies and more generally [&#8230;]]]></description>
								<content:encoded><![CDATA[<p>The Nature <a href="http://www.nature.com/ontologies/">ontologies portal</a> is new section of the nature.com site that describes our involvement with semantic technologies and also makes available to the wider public several models and datasets as RDF linked data.</p>
<p>We launched the portal nearly a month ago, to the purpose of sharing our experiences with semantic technologies and more generally to contribute to the wider linked data community with our data models and datasets.</p>
<p><a href="http://www.michelepasin.org/blog/wp-content/uploads/2015/04/Screen-Shot-2015-04-30-at-17.35.39.png"><img src="http://www.michelepasin.org/blog/wp-content/uploads/2015/04/Screen-Shot-2015-04-30-at-17.35.39.png"  alt="Screen Shot 2015 04 30 at 17 35 39" border="0" width="600" style="width: 600px; margin-left:auto; margin-right:auto; " /></a></p>
<p>This April 2015 release doubles the number and size of our published data models. This now spans more completely the various things that our world contains, from <a href="http://www.nature.com/ontologies/core/publications/">publication things</a> – articles, figures, etc. – to classification things – <a href="http://www.nature.com/ontologies/models/article-types/">article-types</a>, <a href="http://www.nature.com/ontologies/models/subjects/">subjects</a>, etc. – and additional things used to manage our content publishing operation – assets, <a href="http://www.nature.com/ontologies/core/events/">events</a>, etc. Also included is a <a href="http://www.nature.com/ontologies/releases/">release page</a> for the latest data release and a separate page for archival data releases.</p>
<p><a href="http://www.michelepasin.org/blog/wp-content/uploads/2015/04/npg-models-hierarchy-v2-alt.png"><img src="http://www.michelepasin.org/blog/wp-content/uploads/2015/04/npg-models-hierarchy-v2-alt.png"  alt="Npg models hierarchy v2 alt" border="0" width="600" style="width: 600px; margin-left:auto; margin-right:auto; " /></a></p>
<h3>Background</h3>
<p>Is this the first time you&#8217;ve heard about <em>semantic web</em> and <em>ontologies</em>?<br />
 <br />
Then you should know that even though internally at <a href="http://se.macmillan.com/">Macmillan Science and Education</a> XML remains the main technology used to represent and store the things we publish, the <em>metadata</em> about these documents (e.g. publication details, subject categories etc..) are normally encoded also using a more abstract, <strong>graph-oriented information model</strong>.<br />
 <br />
This is called <a href="http://en.wikipedia.org/wiki/Resource_Description_Framework">RDF</a> and has two key characteristics:<br />
&#8211; it encodes all information in the form of triples e.g. <span style="font-family:monospace;color:#000000; ">&lt;subject&gt;&lt;predicate&gt;&lt;object&gt;</span><br />
&#8211; it was built with the web in mind: broadly speaking, each of the items in a triple can be accessed via the internet i.e. it is a <a href="http://www.ltg.ed.ac.uk/~ht/WhatAreURIs/">URIs</a> (a generalised notion of a URL).<br />
 <br />
<strong>So why using RDF?</strong></p>
<p>The RDF model makes it easier to maintain a <strong>shared yet scalable schema</strong> (aka an &#8216;ontology&#8217;) of the data types in use within our organization . A bit like a common language which is spoken by increasingly more data stores and thus allows to join things up more easily whenever needed.<br />
 <br />
At the same time &#8211; since the RDF model is native to the web &#8211; it facilitates the &#8216;semantic&#8217; integration of our data with the increasing number of other organisations that publish their data using compatible models.<br />
 <br />
For example the <a href="http://www.bbc.co.uk/ontologies">BBC</a>, <a href="http://www.elsevier.com/about/content-innovation/database-linking">Elsevier</a> or more recently <a href="https://campus.macmillan.com/message/34445#34445">Springer</a>  are among the many organisations that contribute to the <a href="http://lod-cloud.net/">Linked Data Cloud</a>.</p>
<h3>What&#8217;s next</h3>
<p>We&#8217;ll continue improving these ontologies and releasing new ones as they are created. But probably most interestingly for many people, we&#8217;re working a new release of the whole <strong>NPG articles dataset</strong> (~1M articles). </p>
<p>So stay tuned for more!</p>
<p>&nbsp;</p>
]]></content:encoded>
									<post-id xmlns="com-wordpress:feed-additions:1">2618</post-id>	</item>
		<item>
		<title>A few useful Linked Data resources</title>
		<link>http://www.michelepasin.org/blog/2011/03/17/a-few-useful-linked-data-resources/</link>
				<pubDate>Thu, 17 Mar 2011 11:32:00 +0000</pubDate>
		<dc:creator><![CDATA[mikele]]></dc:creator>
				<category><![CDATA[Semantic Web]]></category>
		<category><![CDATA[linkeddata]]></category>
		<category><![CDATA[rdf]]></category>
		<category><![CDATA[semanticweb]]></category>
		<category><![CDATA[sparql]]></category>

		<guid isPermaLink="false">http://www.michelepasin.org/blog/?p=1135</guid>
				<description><![CDATA[Done a bit of semantic web work in the last couple of weeks, which gave me a chance to explore better the current web-scenario around this topic. I&#8217;m working on some example applications myself, but in the meanwhile I thought I&#8217;d share here a couple of quite useful links I ran into. Development Tools: Quick [&#8230;]]]></description>
								<content:encoded><![CDATA[<p>Done a bit of semantic web work in the last couple of weeks, which gave me a chance to explore better the current web-scenario around this topic. I&#8217;m working on some example applications myself, but in the meanwhile I thought I&#8217;d share here a couple of quite useful links I ran into.</p>
<h3>Development Tools:</h3>
<p></p>
<li><a href="http://graphite.ecs.soton.ac.uk/browser/">Quick and Dirty RDF browser</a>. It does just what is says: you pass it an rdf file and it helps you making sense of it. For example, check out the rdf graph describing the city of Southampton on DbPedia: <a href="http://graphite.ecs.soton.ac.uk/browser/?uri=http://dbpedia.org/resource/Southampton">http://dbpedia.org/resource/Southampton</a>. Minimal, fast and useful! </li>
<li><a href="http://prefix.cc/">Namespace lookup service for RDF developers</a>. The intention of this service is to simplify a common task in the work of RDF developers: remembering and looking up URI prefixes.You can look up prefixes from the search box on the homepage, or directly by typing URLs into your browser bar, such as <a href="http://prefix.cc/foaf">http://prefix.cc/foaf</a> or <a href="http://prefix.cc/foaf,dc,owl.ttl">http://prefix.cc/foaf,dc,owl.ttl</a>.</li>
<li><a href="http://www.knoodl.com/ui/home.html">Knoodl</a> Knoodl is an online tool for creating, managing, and analyzing RDF/OWL descriptions. It has several features that support collaboration in all stages of these activities (eg it lets you create quite easily discussion forums around ontological modeling decisions). It&#8217;s hosted in the Amazon EC2 cloud and can be used for free. </li>
<li><a href="https://chrome.google.com/extensions/search?itemlang=&#038;hl=en&#038;q=rdf">Rdf Goole chrome extensions</a>. Just a list of extensions for Google Chrome that make working with rdf much simpler, for example by detecting rdf annotations embedded in HTML.</li>
<li><a href="http://getthedata.org/">Get the data</a>. Ask and answer questions about getting, using and sharing data! A StackOverflow clone that crowd-sources the task of finding out whether the data you need are available, and where.</a></li>
<p>&nbsp;</p>
<h3>Articles / Tutorials</h3>
<p></p>
<li><a href="http://openorg.ecs.soton.ac.uk/index.php/Linked_Data_Guide_for_Newbies">Linked Data Guide for Newbies</a>. It&#8217;s primarily aimed at &#8220;<em>people who&#8217;re tasked with creating RDF and don&#8217;t have time to faff around.</em>&#8221; It&#8217;s a brief and practical introduction to some of the concepts and technical issues behind Linked Data &#8211; simple and effective, although it obviously hides all the most difficult aspects.</li>
<li><a href="http://blogs.ecs.soton.ac.uk/webteam/2010/11/08/what-you-need-to-know-about-rdfxml/">What you need to know about RDF+XML</a>. Again, another gentle and practical intro.</li>
<li><a href="http://www.w3.org/DesignIssues/LinkedData.html">Linked Data: design issues</a>. One of the original articles by Berners Lee. It goes a little deeper into the theoretical issues involved with the Linked Data approach.</li>
<li><a href="http://linkeddatabook.com/editions/1.0/">Linked Data: Evolving the Web into a Global Data Space</a>. Large and thorough resource: this book is freely available online and contains all that you need to become a Linked Data expert &#8211; whatever that means!</li>
<li><a href="http://memespring.co.uk/2011/01/linked-data-rdfsparql-documentation-challenge/">Linked Data/RDF/SPARQL Documentation Challenge</a>. A recent initiative aimed at pushing people to document the &#8216;path to rdf&#8217; with as many languages and environments as possible. The idea is to move away from some kind of academic-circles-only culture and create something &#8220;<em>closer to the Django introduction tutorial or the MongoDB quick start guide than an academic white paper</em>&#8220;. This blog post is definitely worth checking out imho, especially because of the wealth of responses it has elicited!</li>
<li><a href="http://www.xml.com/pub/a/2005/11/16/introducing-sparql-querying-semantic-web-tutorial.html?page=1">Introducing SPARQL: Querying the Semantic Web</a>. An in-depth article at XML.com that introduces SPARQL &#8211; the query language and data access protocol for the Semantic Web.</li>
<li><a href="http://www.pezholio.co.uk/2011/01/a-beginners-guide-to-sparqling-linked-data-part-1/">A beginner’s guide to SPARQLing linked data</a>. A more hands-on description of what SPARQL can do for you.</li>
<li><a href="http://richard.cyganiak.de/2007/10/lod/">Linked Data: how to get your dataset in the diagram</a>. So you&#8217;ve noticed the Linked Data bubbles growing bigger and bigger. Next step is &#8211; how to contribute and get in there? This article gives you all the info you need to know. </li>
<li><a href="http://answers.semanticweb.com/"><strike>Semantic Overflow</strike> Answers.semanticweb.com</a>. If you run out of ideas, this is the place where to ask for help!</li>
<p>&nbsp;</p>
]]></content:encoded>
									<post-id xmlns="com-wordpress:feed-additions:1">1135</post-id>	</item>
		<item>
		<title>Survey of Pythonic tools for RDF and Linked Data programming</title>
		<link>http://www.michelepasin.org/blog/2011/02/24/survey-of-pythonic-tools-for-rdf-and-linked-data-programming/</link>
				<comments>http://www.michelepasin.org/blog/2011/02/24/survey-of-pythonic-tools-for-rdf-and-linked-data-programming/#comments</comments>
				<pubDate>Thu, 24 Feb 2011 15:21:27 +0000</pubDate>
		<dc:creator><![CDATA[mikele]]></dc:creator>
				<category><![CDATA[Semantic Web]]></category>
		<category><![CDATA[TechLife]]></category>
		<category><![CDATA[linkeddata]]></category>
		<category><![CDATA[python]]></category>
		<category><![CDATA[rdf]]></category>
		<category><![CDATA[rdflib]]></category>
		<category><![CDATA[semanticweb]]></category>

		<guid isPermaLink="false">http://www.michelepasin.org/blog/?p=1110</guid>
				<description><![CDATA[In this post I&#8217;m reporting on a recent survey I made in the context of a Linked Data project I&#8217;m working on, SAILS. The Resource Description Framework (RDF) is a data model and language which is quickly gaining momentum in the open-data and data-integration worlds. In SAILS we&#8217;re developing a prototype for rdf-data manipulation and [&#8230;]]]></description>
								<content:encoded><![CDATA[<p>In this post I&#8217;m reporting on a recent survey I made in the context of a <a href="http://en.wikipedia.org/wiki/Linked_Data">Linked Data</a> project I&#8217;m working on, <a href="http://sailsproject.cerch.kcl.ac.uk/">SAILS</a>. The <a href="http://en.wikipedia.org/wiki/Resource_Description_Framework">Resource Description Framework</a> (RDF) is a data model and language which is quickly gaining momentum in the open-data and data-integration worlds. In SAILS we&#8217;re developing a prototype for rdf-data manipulation and querying, but since the final application (of which the rdf-components is part of) will be written in Python and Django, in what follows I tried to gather information about all the existing libraries and frameworks for doing rdf-programming using python. </p>
<h2>1. Python libraries for working with Rdf</h2>
<h3> RdfLib <a style="font-size: 14px;" href="http://www.rdflib.net/">http://www.rdflib.net/</a></h3>
<p>RdfLib (<a href="http://www.rdflib.net/rdflib-3.0.0.tar.gz">download</a>) is a pretty solid and extensive rdf-programming kit for python. It contains parsers and serializers for RDF/XML, N3, NTriples, Turtle, TriX and RDFa. The library presents a Graph interface which can be backed by any one of a number of store implementations, including, memory, MySQL, Redland, SQLite, Sleepycat, ZODB and SQLObject.</p>
<p>The latest release is <strong> RdfLib 3.0</strong>, although I have the feeling that many are still using the previous release, <strong>2.4</strong>. One big difference between the two is that in 3.0 some libraries have been separated into another package (called <a href="http://code.google.com/p/rdfextras/">rdfextras</a>); among these libraries there&#8217;s also the one you need for processing <a href="http://en.wikipedia.org/wiki/SPARQL">sparql</a> queries (the rdf query language), so it&#8217;s likely that you want to install that too.<br />
A short overview of the difference between these two recent releases of RdfLib can be found <a href="http://code.google.com/p/rdflib/wiki/UpgradingToVersion3">here</a>. The APIs documentation for RdfLib 2.4 is available <a href="http://www.rdflib.net/rdflib-2.4.0/html/index.html">here</a>, while the one for RdfLib 3.0 can be found <a href="http://code.alcidesfonseca.com/docs/rdflib/index.html">here</a>. Finally, there are also some other (a bit older, but possibly useful) docs on the <a href="http://code.google.com/p/rdflib/w/list">wiki</a>.</p>
<p>Next thing, you might want to check out these tutorials:</p>
<li><a href="http://semanticweb.org/wiki/Getting_data_from_the_Semantic_Web.html">Getting data from the Semantic Web</a>: a nice example of how to use RdfLib and python in order to get data from  <a href="http://dbpedia.org/">DBPedia</a>, the Semantic Web version of Wikipedia.</li>
<li><a href="http://johngoodwin225.wordpress.com/2011/01/18/how-can-i-use-the-ordnance-survey-linked-data-a-python-rdflib-example/">How can I use the Ordnance Survey Linked Data</a>: shows how to install RdfLib and query the linked data offered by <a href="http://blog.ordnancesurvey.co.uk/2011/01/how-linked-data-can-reap-benefits/">Ordnance Survey</a>.</li>
<li><a href="http://gromgull.net/blog/2011/01/a-quick-and-dirty-guide-to-your-first-time-with-rdf/">A quick and dirty guide to YOUR first time with RDF</a>: another example of querying Uk government data found on <a href="http://data.gov.uk/">data.gov.uk</a> using RdfLib and Berkely/Sleepycat DB.</li>
<h3>RdfAlchemy <a style="font-size: 14px;" href="http://www.openvest.com/trac/wiki/RDFAlchemy">http://www.openvest.com/trac/wiki/RDFAlchemy</a></h3>
<p>The goal of RDFAlchemy (<a href="http://www.openvest.com/trac/wiki/RDFAlchemy#Installation">install</a> | <a href="http://www.openvest.com/public/docs/rdfalchemy/api/">apidocs</a> | <a href="http://groups.google.com/group/rdfalchemy-dev">usergroup</a>) is to allow anyone who uses  python to have a object type API access to an RDF Triplestore. In a nutshell, the same way that <a href="http://www.sqlalchemy.org/">SQLAlchemy</a> is an ORM (Object Relational Mapper) for relational database users, RDFAlchemy is an ORM (Object RDF Mapper) for semantic web users.</p>
<p>RdfAlchemy can also work in conjunction with other datastores, including rdflib, Sesame, and Jena. Support for SPARQL is present, although it seems less stable than the rest of the library.</p>
<h3>Fuxi <a style="font-size: 14px;" href="http://code.google.com/p/fuxi/">http://code.google.com/p/fuxi/</a></h3>
<p>FuXi is a Python-based, bi-directional logical reasoning system for the semantic web. It <strong>requires</strong> rdflib 2.4.1 or 2.4.2 and it is <strong>not</strong> compatible with rdflib 3. FuXi aims to be the &#8216;engine for contemporary expert systems based on the Semantic Web technologies&#8217;. The documentation can be found <a href="http://fuxi.googlecode.com/hg/documentation/html/index.html">here</a>; it might be useful also to look at the <a href="http://code.google.com/p/fuxi/wiki/FuXiUserManual">user-manual</a> and the <a href="http://groups.google.com/group/fuxi-discussion">discussion group</a>.  </p>
<p>In general, it looks as if Fuxi can offer a complete solution for knowledge representation and reasoning over the semantic web; it is quite sophisticated and well documented (partly via several academic articles). The downside is that to the end of hacking together a linked data application.. well Fuxi is probably just too complex and difficult to learn.</p>
<li><a href="http://blog.okfn.org/2010/08/02/about-inferencing/">About Inferencing</a>: a very short introduction to what Fuxi inferencing capabilities can do in the context of an rdf application.</li>
<h3>ORDF <a style="font-size: 14px;" href="http://ordf.org/">ordf.org</a></h3>
<p>ORDF (<a href="http://packages.python.org/ordf/administration.html#installation">download</a> | <a href="http://packages.python.org/ordf/index.html">docs</a>) is the <a href="http://okfn.org/">Open Knowledge Foundation</a>‘s library of support infrastructure for RDF. It is <strong>based</strong> on RDFLib and contains an object-description mapper, support for multiple back-end indices, message passing, revision history and provenance, a namespace library and a variety of helper functions and modules to ease integration with the <a href="http://pylonshq.com/">Pylons</a> framework.</p>
<p>The current version of this library is 0.35. You can have a peek at some of its key functionalities by checking out the &#8216;<a href="http://packages.python.org/ordf/odm.html">Object Description Mapper</a>&#8216; &#8211; an equivalent to what an Object-Relational Mapper would give you in the context of a relational database. The library seems to be pretty solid; for an example of a system built on top of ORDF you can see <a href="http://bibliographica.org/">Bibliographica</a>, an online open catalogue of cultural works.</p>
<li>Why using RDF? The <a href="http://packages.python.org/ordf/design_considerations.html">Design Considerations</a> section in the ORDF documentation discusses the reasons that led to the development of this library in a clear and practical fashion.</li>
<h3>Django-rdf <a style="font-size: 14px;" href="http://code.google.com/p/django-rdf/">http://code.google.com/p/django-rdf/</a></h3>
<p>Django-RDF (<a href="http://code.google.com/p/django-rdf/downloads/list">download</a> | <a href="http://code.google.com/p/django-rdf/wiki/FAQ">faq</a> | <a href="http://groups.google.com/group/django-rdf">discussiongroup</a>) is an RDF engine implemented in a generic, reusable <a href="http://www.djangoproject.com/">Django</a> app, providing complete RDF support to Django projects without requiring any modifications to existing framework or app source code. The philosophy is simple: do your web development using Django just like you&#8217;re used to, then turn the knob and &#8211; with no additional effort &#8211; expose your project on the semantic web.</p>
<p>Django-RDF can expose models from any other app as RDF data. This makes it easy to write new views that return RDF/XML data, and/or query existing models in terms of RDFS or OWL classes and properties using (a variant of) the SPARQL query language. SPARQL in, RDF/XML out &#8211; two basic semantic web necessities. Django-RDF also implements an RDF store using its internal models such as Concept, Predicate, Resource, Statement, Literal, Ontology, Namespace, etc. The SPARQL query engine returns query sets that can freely mix data in the RDF store with data from existing Django models.</p>
<p>The major <strong>downside</strong> of this library is that it doesn&#8217;t seem to be maintained anymore; the last release is from 2008, and there seem to be various conflicts with recent versions of Django. A real shame!</p>
<h3>Djubby <a style="font-size: 14px;" href="http://code.google.com/p/djubby/">http://code.google.com/p/djubby/</a></h3>
<p>Djubby (<a href="http://code.google.com/p/djubby/downloads/list">download</a> | <a href="http://code.google.com/p/djubby/wiki/GettingStarted">docs</a>) is a Linked Data frontend for SPARQL endpoints for the <a href="http://www.djangoproject.com/">Django</a> Web framework, adding a Linked Data interface to any existing SPARQL-capable triple stores. </p>
<p>Djubby is quite inspired by Richard Cyganiak&#8217;s <a href="http://www4.wiwiss.fu-berlin.de/pubby/">Pubby</a> (written in Java): it provides a Linked Data interface to local or remote SPARQL protocol servers, it provides dereferenceable URIs by rewriting URIs found in the SPARQL-exposed dataset into the djubby server&#8217;s namespace, and it provides a simple HTML interface showing the data available about each resource, taking care of handling 303 redirects and content negotiation. </p>
<h3>Redland <a style="font-size: 14px;" href="http://librdf.org/">http://librdf.org/</a></h3>
<p>Redland (<a href="http://download.librdf.org/">download</a> | <a href="http://librdf.org/docs/">docs</a> | <a href="http://lists.usefulinc.com/pipermail/redland-dev/">discussiongroup</a>) is an RDF library written in C and including several high-level language APIs providing RDF manipulation and storage. Redland makes available also a Python interface (<a href="http://librdf.org/docs/python.html">intro</a> | <a href="http://librdf.org/docs/pydoc/RDF.html">apidocs</a>) that can be used to manipulate RDF triples. </p>
<p>This library seems to be quite complete and is actively maintained; only potential downside is the installation process. In order to use the python bindings you need to install the C library too (which in turns depends on other C libraries), so (depending on your programming experience and operating system used) just getting up and running might become a challenge.</p>
<h3>SuRF <a style="font-size: 14px;" href="http://packages.python.org/SuRF/">http://packages.python.org/SuRF/</a></h3>
<p>SuRF (<a href="http://packages.python.org/SuRF/install.html#installing-rdflib">install</a> | <a href="http://packages.python.org/SuRF/#documentation">docs</a>) is an Object &#8211; RDF Mapper based on the RDFLIB python library. It exposes the RDF triple sets as sets of resources and seamlessly integrates them into the Object Oriented paradigm of python in a similar manner as ActiveRDF does for ruby.</p>
<h3>Other smaller (but possibly useful) python libraries for rdf:</h3>
<li><a href="http://ivan-herman.name/2007/07/06/sparql-endpoint-interface-to-python/">Sparql Interface to python</a>: a minimalistic solution for querying sparql endpoints using python (<a href="http://www.ivan-herman.net/Misc/PythonStuff/SPARQL/">download</a> | <a href="http://www.ivan-herman.net/Misc/PythonStuff/SPARQL/Doc-SPARQL/">apidocs</a>). <em>UPDATE: Ivan Herman pointed out that this library has been discontinued and merged with the &#8216;SPARQL Endpoint interface to Python&#8217; below.</em></li>
<li><a href="http://sparql-wrapper.sourceforge.net/">SPARQL Endpoint interface to Python</a> another little utility for talking to a SPARQL endpoint, including having select-results mapped to rdflib terms or returned in JSON format (<a href="http://sourceforge.net/projects/sparql-wrapper/">download</a>)</li>
<li><a href="http://code.google.com/p/pysparql/source/browse/trunk/src/sparql.py">PySparql</a>: again, a minimal library that does SELECT and ASK queries on an endpoint which implements the HTTP (GET or POST) bindings of the SPARQL Protocol (<a href="http://code.google.com/p/pysparql/source/browse/trunk/src/sparql.py">code page</a>)</li>
<li><a href="https://github.com/mnot/sparta/">Sparta</a>: Sparts is a simple, resource-centric API for RDF graphs, built on top of RDFLIB. </li>
<li><a href="http://oort.to/">Oort</a>: another Python toolkit for accessing RDF graphs as plain objects, based on RDFLIB. The project homepage hasn&#8217;t been updated for a while, although there is trace of recent activity on its <a href="http://code.google.com/p/oort/">google project</a> page.</li>
<p>&nbsp;</p>
<h2>2. RDF Triplestores that are python-friendly</h2>
<p>An important component of a linked-data application is the <a href="http://en.wikipedia.org/wiki/Triplestore">triplestore</a> (that is, an RDF database): many commercial and non-commercial triplestores are available, but only a few offer out-of-the-box python interfaces. Here&#8217;s a list of them:</p>
<h3>Allegro Graph <a style="font-size: 14px;" href="http://www.franz.com/agraph/allegrograph/">http://www.franz.com/agraph/allegrograph/</a></h3>
<p><a href="http://www.franz.com/agraph/allegrograph/">AllegroGraph</a> RDFStore is a high-performance, persistent RDF graph database. AllegroGraph uses disk-based storage, enabling it to scale to billions of triples while maintaining superior performance. Unfortunately, the official version of AllegroGraph is not free, but it is possible to get a <a href="http://www.franz.com/agraph/allegrograph/ag_commercial_edition.lhtml">free version</a> of it (it limits the DB to 50 million triples, so although useful for testing or development it doesn&#8217;t seem a good solution for a production environment).</p>
<p>The Allegro Graph Python API (<a href="http://www.franz.com/agraph/allegrograph/clients.lhtml">download</a> | <a href="http://www.franz.com/agraph/support/documentation/current/python-tutorial/python-tutorial-40.html">docs</a> | <a href="http://www.franz.com/agraph/support/documentation/v4/python-tutorial/python-API-40.html">reference</a>) offers convenient and efficient access to an <a href="http://www.franz.com/agraph/allegrograph/">AllegroGraph</a> server from a Python-based application. This API provides methods for creating, querying and maintaining RDF data, and for managing the stored triples.</p>
<li>A hands-on overview of what&#8217;s like to work with AllegroGraph and python can be found here: <a href="http://www.snee.com/bobdc.blog/2009/04/getting-started-with-allegrogr.html">Getting started with AllegroGraph</a>.</li>
<h3>Open Link Virtuoso <a style="font-size: 14px;" href="http://virtuoso.openlinksw.com/">http://virtuoso.openlinksw.com/</a></h3>
<p><a href="http://en.wikipedia.org/wiki/Virtuoso_Universal_Server">Virtuoso Universal Server</a> is a middleware and database engine hybrid that combines the functionality of a traditional RDBMS, ORDBMS, virtual database, RDF, XML, free-text, web application server and file server functionality in a single system. Rather than have dedicated servers for each of the aforementioned functionality realms, Virtuoso is a &#8220;universal server&#8221;; it enables a single multithreaded server process that implements multiple protocols. The open source edition of Virtuoso Universal Server is also known as <a href="http://virtuoso.openlinksw.com/dataspace/dav/wiki/Main/VOSIntro">OpenLink Virtuoso</a>.</p>
<p><a href="http://packages.python.org/virtuoso/">Virtuoso from Python</a> is intended to be a collection of modules for interacting with OpenLink Virtuoso from python. The goal is to provide drivers for `SQLAlchemy` and `RDFLib`. The package is installable from the <a href="http://pypi.python.org/pypi/virtuoso">Python Package Index</a> and source code for development is available in a mercurial repository on <a href="http://bitbucket.org/ww/virtuoso">BitBucket</a>. </p>
<li>A possibly useful example of using Virtuoso from python: <a href="http://www.openlinksw.com/dataspace/kidehen@openlinksw.com/weblog/kidehen@openlinksw.com%27s%20BLOG%20%5B127%5D/1651">SPARQL Guide for Python Developer</a>.</li>
<h3>Sesame <a style="font-size: 14px;" href="http://www.openrdf.org/">http://www.openrdf.org/</a></h3>
<p><a href="http://en.wikipedia.org/wiki/Sesame_(framework)">Sesame</a> is an open-source framework for querying and analyzing RDF data (<a href="http://www.openrdf.org/download.jsp">download</a> | <a href="http://www.openrdf.org/documentation.jsp">documentation</a>). Sesame supports two query languages: SeRQL and Sparql. Sesame&#8217;s API differs from comparable solutions in that it offers a (stackable) interface through wich functionality can be added, and the storage engine is abstracted from the query interface (many other Triplestores can in fact be used through the Sesame API).</p>
<p>It looks as if the best way to interact with Sesame is by using Java; however there is also a pythonic API called <a href="http://pysesame.projects.semwebcentral.org/">pySesame</a>. This is essentially a python wrapper for Sesame&#8217;s REST HTTP API, so the range of operations supported (Log in, Log out, Request a list of available repositories, Evaluate a SeRQL-select, RQL or RDQL query, Extract/upload/remove RDF from a repository) are somehow limited (for example, there does not seem to be any native SPARQL support).</p>
<li>A nice introduction to using Sesame with Python (without pySesame though) can be found in this article: <a href="http://www.jenitennison.com/blog/node/153">Getting Started with RDF and SPARQL Using Sesame and Python</a>.</li>
<h3>Talis platform <a style="font-size: 14px;" href="http://www.talis.com/platform/">http://www.talis.com/platform/</a></h3>
<p>The Talis Platform (<a href="http://docs.api.talis.com/getting-started/platform-faq#TOC-What-is-the-Talis-Platform-">faq</a> | <a href="http://docs.api.talis.com/">docs</a>)is an environment for building next generation applications and services based on Semantic Web technologies. It is a <strong>hosted</strong> system which provides an efficient, robust storage infrastructure. Both arbitrary documents and RDF-based semantic content are supported, with sophisticated query, indexing and search features. Data uploaded on the Talis platform are organized into stores: a <strong>store</strong> is a grouping of related data and metadata. For convenience each store is assigned one or more owners who are the people who have rights to configure the access controls over that data and metadata. Each store provides a uniform REST interface to the data and metadata it manages.</p>
<p>Stores don&#8217;t come free of charge, but through the <a href="http://www.talis.com/platform/cc/">Talis Connected Commons scheme</a> it is possible have quite large amounts of store space for free. The scheme is intended to support a wide range of different forms of data publishing. For example scientific researchers seeking to share their research data; dissemination of public domain data from a variety of different charitable, public sector or volunteer organizations; open data enthusiasts compiling data sets to be shared with the web community.</p>
<p>Good news for pythonistas too: <a href="http://code.google.com/p/pynappl/">pynappl</a> is a simple client library for the Talis Platform. It relies on <a href="http://code.google.com/p/rdflib/">rdflib 3.0</a> and draws inspiration from other similar client libraries.  Currently it is focussed mainly on managing data loading and manipulation of Talis Platform stores (<a href="http://blogs.talis.com/n2/archives/887">this blog post</a> says more about it). </p>
<li>Before trying out the Talis platform you might find useful this blog post: <a href="http://www.jenitennison.com/blog/node/109">Publishing Linked Data on the Talis Platform</a>.</li>
<h3>4store <a style="font-size: 14px;" href="http://4store.org/">http://4store.org/</a></h3>
<p>4store (<a href="http://4store.org/trac/wiki/Download">download</a> | <a href="http://4store.org/about">features</a> | <a href="http://4store.org/trac/wiki/Documentation">docs</a>) is a database storage and query engine that holds RDF data. It has been used by <a href="http://www.garlik.com/">Garlik</a> as their primary RDF platform for three years, and has proved itself to be robust and secure.<br />
4store&#8217;s main strengths are its performance, scalability and stability. It does not provide many features over and above RDF storage and SPARQL queries, but if your are looking for a scalable, secure, fast and efficient RDF store, then 4store should be on your shortlist.</p>
<p>4store offers a number of <a href="http://4store.org/trac/wiki/ClientLibraries">client libraries</a>, among them there are two for python: first, <a href="http://pypi.python.org/pypi/HTTP4Store/0.2">HTTP4Store</a> is a client for the 4Store httpd service &#8211; allowing for easy handling of sparql results, and adding, appending and deleting graphs. Second, <a href="https://github.com/wwaites/py4s">py4s</a>, although this seems to be a much more experimental library (geared towards multi process queries).<br />
Furthemore, there is also an application for the Django web framework called <a href="https://github.com/66laps/django-4store#readme">django-4store</a> that makes it easier to query and load rdf data into 4store when running Django. The application offers some support for constructing sparql-based Django <a href="https://github.com/66laps/django-4store/blob/master/src/fourstore/views.py">views</a>.</p>
<li>This blog post shows how to install 4store: <a href="http://www.jenitennison.com/blog/node/152">Getting Started with RDF and SPARQL Using 4store and RDF.rb </a>.</li>
<p>&nbsp;</p>
<p>End of the survey.. <em>have I missed out on something</em>? Please let me know if I did &#8211; I&#8217;ll try to keep adding stuff to this list as I move on with my project work!</p>
<p>&nbsp;</p>
]]></content:encoded>
							<wfw:commentRss>http://www.michelepasin.org/blog/2011/02/24/survey-of-pythonic-tools-for-rdf-and-linked-data-programming/feed/</wfw:commentRss>
		<slash:comments>24</slash:comments>
						<post-id xmlns="com-wordpress:feed-additions:1">1110</post-id>	</item>
		<item>
		<title>Roman Port Networks project</title>
		<link>http://www.michelepasin.org/blog/2009/07/21/roman-port-networks-project/</link>
				<comments>http://www.michelepasin.org/blog/2009/07/21/roman-port-networks-project/#comments</comments>
				<pubDate>Tue, 21 Jul 2009 13:07:50 +0000</pubDate>
		<dc:creator><![CDATA[mikele]]></dc:creator>
				<category><![CDATA[Cultural Informatics]]></category>
		<category><![CDATA[archeology]]></category>
		<category><![CDATA[history]]></category>
		<category><![CDATA[linkeddata]]></category>
		<category><![CDATA[project]]></category>

		<guid isPermaLink="false">http://magicrebirth.wordpress.com/?p=225</guid>
				<description><![CDATA[The Roman Port Networks Project is a collaboration between 30 European partners, examining the connections between Roman ports across the Mediterranean. The project has received financial support from the British Academy (BASIS) and the University of Southampton (School of Humanities, Department of Archaeology and School of Electronics and Computing Science). From the website (the bold [&#8230;]]]></description>
								<content:encoded><![CDATA[<p>The <a href="http://www.romanportnetworks.org/index.html">Roman Port Networks Project</a> is a collaboration between 30 European partners, <strong>examining the connections between Roman ports across the Mediterranean</strong>. The project has received financial support from the British Academy (<a href="http://www.britac.ac.uk:80/institutes/index.cfm">BASIS</a>) and the University of Southampton (School of Humanities, Department of Archaeology and School of Electronics and Computing Science).</p>
<p><img src="http://www.michelepasin.org/blog/wp-content/uploads/2009/07/picture-1.png" alt="Picture 1" title="Picture 1" width="464" height="151" class="alignnone size-full wp-image-227" srcset="http://www.michelepasin.org/blog/wp-content/uploads/2009/07/picture-1.png 464w, http://www.michelepasin.org/blog/wp-content/uploads/2009/07/picture-1-300x97.png 300w" sizes="(max-width: 464px) 100vw, 464px" /></p>
<p>From the website (the bold font is mine):</p>
<blockquote><p>The project will use an innovative new approach to data management in order to bring together the many separate sources of information that we have about ports in the Roman Mediterranean. The <strong>Semantic Web is a way of linking data by storing it as statements rather than in tables</strong>. Because the statements are composed of the same URIs that you use in the address bar of an internet browser, they can be accessed by other computers so <strong>different datasets can be connected together more easily</strong>. It also means that we can see all the information related to a given concept, whether it&#8217;s a thing, a property or a class of objects. [some interesting papers about this approach <a href="http://leifuss.wordpress.com/publications/">can be found here</a>]</p>
<p>We hope that by using this methodology we might soon be able to ask questions such as &#8216;where are all the known finds of Dressel 20 amphorae on the Mediterranean coast?&#8217;, or &#8216;which other towns have used the same types of marble as those employed in Tarragona?&#8217; <strong>It is with this kind of knowledge that we can start building theoretical networks of trade and mobility.</strong></p></blockquote>
<p>&nbsp;</p>
]]></content:encoded>
							<wfw:commentRss>http://www.michelepasin.org/blog/2009/07/21/roman-port-networks-project/feed/</wfw:commentRss>
		<slash:comments>1</slash:comments>
						<post-id xmlns="com-wordpress:feed-additions:1">225</post-id>	</item>
	</channel>
</rss>
