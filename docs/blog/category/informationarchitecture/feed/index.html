<?xml version="1.0" encoding="UTF-8"?><rss version="2.0"
	xmlns:content="http://purl.org/rss/1.0/modules/content/"
	xmlns:wfw="http://wellformedweb.org/CommentAPI/"
	xmlns:dc="http://purl.org/dc/elements/1.1/"
	xmlns:atom="http://www.w3.org/2005/Atom"
	xmlns:sy="http://purl.org/rss/1.0/modules/syndication/"
	xmlns:slash="http://purl.org/rss/1.0/modules/slash/"
	
	xmlns:georss="http://www.georss.org/georss"
	xmlns:geo="http://www.w3.org/2003/01/geo/wgs84_pos#"
	>

<channel>
	<title>Information Architecture &#8211; Parerga und Paralipomena</title>
	<atom:link href="http://www.michelepasin.org/blog/category/informationarchitecture/feed/" rel="self" type="application/rss+xml" />
	<link>http://www.michelepasin.org/blog</link>
	<description>At the core of all well-founded belief lies belief that is unfounded - Wittgenstein</description>
	<lastBuildDate>Tue, 19 Feb 2019 22:03:38 +0000</lastBuildDate>
	<language>en-US</language>
	<sy:updatePeriod>
	hourly	</sy:updatePeriod>
	<sy:updateFrequency>
	1	</sy:updateFrequency>
	<generator>https://wordpress.org/?v=5.2.11</generator>
<site xmlns="com-wordpress:feed-additions:1">13825966</site>	<item>
		<title>Zero Hunger Hack Day: surfacing research about the Sustainable Development Goals program</title>
		<link>http://www.michelepasin.org/blog/2019/02/11/zero-hunger-hack-day/</link>
				<pubDate>Mon, 11 Feb 2019 12:09:58 +0000</pubDate>
		<dc:creator><![CDATA[mikele]]></dc:creator>
				<category><![CDATA[Information Architecture]]></category>
		<category><![CDATA[Just Blogging]]></category>

		<guid isPermaLink="false">http://www.michelepasin.org/blog/?p=3282</guid>
				<description><![CDATA[This post is about a little dashboard idea that aims at helping policy makers discover research relevant to the &#8216;zero hunger&#8216; topic, one of the themes of the Sustainable Development Goals program. The 2030 Agenda for Sustainable Development, adopted by all United Nations Member States in 2015, provides a shared blueprint for peace and prosperity for people [&#8230;]]]></description>
								<content:encoded><![CDATA[<p>This post is about a little dashboard idea that aims at helping policy makers discover research relevant to the &#8216;<a href="https://en.wikipedia.org/wiki/Sustainable_Development_Goals#Goal_2:_Zero_hunger">zero hunger</a>&#8216; topic, one of the themes of the <a href="https://sustainabledevelopment.un.org/sdgs">Sustainable Development Goals</a> program.</p>
<blockquote><p><a href="https://sustainabledevelopment.un.org/post2015/transformingourworld">The 2030 Agenda for Sustainable Development,</a> adopted by all United Nations Member States in 2015, provides a shared blueprint for peace and prosperity for people and the planet, now and into the future. At its heart are the 17 Sustainable Development Goals (SDGs), which are an urgent call for action by all countries &#8211; developed and developing &#8211; in a global partnership. They recognize that ending poverty and other deprivations must go hand-in-hand with strategies that improve health and education, reduce inequality, and spur economic growth – all while tackling climate change and working to preserve our oceans and forests.</p></blockquote>
<p>For more background about this project, see also its wikipedia page <a href="https://en.wikipedia.org/wiki/Sustainable_Development_Goals">https://en.wikipedia.org/wiki/Sustainable_Development_Goals</a></p>
<p><a href="https://sustainabledevelopment.un.org/sdgs" target="_blank" rel="noopener"><img class="  wp-image-3292 aligncenter" src="http://www.michelepasin.org/blog/wp-content/uploads/2019/02/Screenshot-2019-02-19-at-21.18.42.png" alt="Screenshot 2019-02-19 at 21.18.42.png" width="557" height="377" /></a></p>
<p><a href="https://www.springernature.com">Springer Nature</a> is among the many organizations who are taking <a href="https://grandchallenges.springernature.com/">an active role</a> in developing scenarios and solutions to tackle these global challenges. A couple of months ago Springer Nature organized a hack day which brought together people with different backgrounds and expertise in order to come up with ideas and prototypes that could lead to further research. In particular, the focus of the hack day was on the <a href="https://en.wikipedia.org/wiki/Sustainable_Development_Goals#Goal_2:_Zero_hunger">&#8216;zero hunger&#8217;</a> theme.</p>
<p>The team I was working with developed a <strong>concept</strong> around the idea of an easy-to-use <strong>dashboard-like tool</strong> which could be <strong>used by busy policy makers</strong> in order to quickly gather infos about researchers or institutions they&#8217;d want to consult with.</p>
<p><a href="http://www.michelepasin.org/blog/wp-content/uploads/2019/02/Screenshot-2019-02-19-at-21.46.02.jpg" target="_blank" rel="noopener"><em><img class="aligncenter size-full wp-image-3296" src="https://i0.wp.com/www.michelepasin.org/blog/wp-content/uploads/2019/02/Screenshot-2019-02-19-at-21.46.02.jpg?w=1224" alt="Screenshot 2019-02-19 at 21.46.02.jpg" width="612" height="369" /></em></a></p>
<p>In order to make this idea more tangible I ended up building a little <a href="http://hacks2019.michelepasin.org/zerohunger/">prototype,</a> which allows to scan scholarly documents in order to pull out information (potentially) related to the &#8216;zero hunger&#8217; topic and sub-topics, essentially following the keywords-structure specified in the Sustainable Development Goals document.</p>
<blockquote><p>The prototype is available here: <a href="http://hacks2019.michelepasin.org/zerohunger/">http://hacks2019.michelepasin.org/zerohunger/</a></p></blockquote>
<p>&nbsp;</p>
<p><a href="http://www.michelepasin.org/blog/wp-content/uploads/2019/02/Screen-Shot-2018-11-02-at-16.22.42.png" target="_blank" rel="noopener"><img class="  wp-image-3284 aligncenter" src="http://www.michelepasin.org/blog/wp-content/uploads/2019/02/Screen-Shot-2018-11-02-at-16.22.42.png" alt="Screen Shot 2018-11-02 at 16.22.42" width="642" height="336" /></a></p>
<p>&nbsp;</p>
<p><a href="http://www.michelepasin.org/blog/wp-content/uploads/2019/02/Screen-Shot-2018-11-02-at-16.22.51.png" target="_blank" rel="noopener"><img class="  wp-image-3285 aligncenter" src="http://www.michelepasin.org/blog/wp-content/uploads/2019/02/Screen-Shot-2018-11-02-at-16.22.51.png" alt="Screen Shot 2018-11-02 at 16.22.51" width="642" height="334" /></a></p>
<p>&nbsp;</p>
<p>This experiment also gave me an opportunity to learn about the <strong>Dimensions.ai API</strong>, a domain specific language (DSL) which allows to query the <a href="https://www.dimensions.ai/">Dimensions database,</a> a state-of-the-art scholarly platform containing  millions of linked metadata records about publications, grants, patents, clinical trials and policy documents (for more background about Dimensions, see this <a href="https://www.digital-science.com/press-releases/digital-science-launches-dimensions-next-generation-research-discovery-platform-linking-124-million-documents-providing-free-search-citation-data-across-86-million-articles/">blog post </a>and this <a href="http://www.library.spbu.ru/blog/wp-content/uploads/2018/01/Guide-to-Dimensions-Data-Approach-2018.pdf">white paper</a>).</p>
<p><a href="https://www.dimensions.ai/" target="_blank" rel="noopener"><img class="aligncenter  wp-image-3297" src="http://www.michelepasin.org/blog/wp-content/uploads/2019/02/Screenshot-2019-02-19-at-21.50.33.jpg" alt="Screenshot 2019-02-19 at 21.50.33.jpg" width="655" height="402" srcset="http://www.michelepasin.org/blog/wp-content/uploads/2019/02/Screenshot-2019-02-19-at-21.50.33.jpg 2424w, http://www.michelepasin.org/blog/wp-content/uploads/2019/02/Screenshot-2019-02-19-at-21.50.33-300x185.jpg 300w, http://www.michelepasin.org/blog/wp-content/uploads/2019/02/Screenshot-2019-02-19-at-21.50.33-768x473.jpg 768w, http://www.michelepasin.org/blog/wp-content/uploads/2019/02/Screenshot-2019-02-19-at-21.50.33-1024x630.jpg 1024w" sizes="(max-width: 655px) 100vw, 655px" /></a></p>
<p>The API itself is being a paywall, but if you are curious about it, the <a href="https://docs.dimensions.ai/dsl/index.html">documentation is available online</a>.</p>
<p>It&#8217;s a fantastic resource, intuitive and easy to use yet powerful and features-rich, so I am pretty sure I&#8217;ll be writing more about it.</p>
<p>Stay tuned for more!</p>
<p>&nbsp;</p>
<p>&nbsp;</p>
]]></content:encoded>
									<post-id xmlns="com-wordpress:feed-additions:1">3282</post-id>	</item>
		<item>
		<title>Ontospy 1.9.8 released</title>
		<link>http://www.michelepasin.org/blog/2019/01/03/ontospy-1-9-8-released/</link>
				<pubDate>Thu, 03 Jan 2019 11:55:14 +0000</pubDate>
		<dc:creator><![CDATA[mikele]]></dc:creator>
				<category><![CDATA[Information Architecture]]></category>
		<category><![CDATA[Semantic Web]]></category>
		<category><![CDATA[ontospy]]></category>
		<category><![CDATA[python]]></category>
		<category><![CDATA[rdf]]></category>
		<category><![CDATA[semantic web]]></category>

		<guid isPermaLink="false">http://www.michelepasin.org/blog/?p=3279</guid>
				<description><![CDATA[Ontospy version 1.9.8 has been just released and it contains tons of improvements and new features. Ontospy is a lightweight open-source Python library and command line tool for working with vocabularies encoded in the RDF family of languages. Over the past month I&#8217;ve been working on a new version of Ontospy, which is now available for download [&#8230;]]]></description>
								<content:encoded><![CDATA[<p>Ontospy version 1.9.8 has been just released and it contains tons of improvements and new features. <a href="http://lambdamusic.github.io/Ontospy/">Ontospy</a> is a lightweight open-source Python library and command line tool for working with vocabularies encoded in the <a href="https://en.wikipedia.org/wiki/Resource_Description_Framework">RDF</a> family of languages.</p>
<p>Over the past month I&#8217;ve been working on a new version of Ontospy, which is now available for download on <a href="https://pypi.org/project/ontospy/">Pypi</a>.</p>
<iframe class='youtube-player' type='text/html' width='560' height='315' src='https://www.youtube.com/embed/MkKrtVHi_Ks?version=3&#038;rel=1&#038;fs=1&#038;autohide=2&#038;showsearch=0&#038;showinfo=1&#038;iv_load_policy=1&#038;wmode=transparent' allowfullscreen='true' style='border:0;'></iframe>
<p> &nbsp;</p>
<h3>What&#8217;s new in this version</h3>
<li>The library to generate <strong>ontology documentation</strong> (as html or markdown) is now included within the main Ontospy distribution. Previously this library was distributed separately under the name <code style="display: inline; padding: 0px;">ontodocs</code>.  The main problem with this approach is that keeping the two projects in sync was becoming too time-consuming for me, so I&#8217;ve decided to merge them. NOTE one can still choose whether or not to include this extra library <a href="http://lambdamusic.github.io/Ontospy/#installation">when installing</a>.</li>
<li>You can print out the <strong>raw RDF data</strong> being returned via command line argument.</li>
<li>One can decided whether or not to include <strong>&#8216;inferred&#8217; schema definitions</strong> extracted from an RDF payload. The inferences are pretty basic for now (eg the object of <code style="display: inline; padding: 0px;">rdf:type</code> statements is taken to be a type) but this allows for example to quickly dereference a DBpedia URI and pull out all types/predicates being used.</li>
<li>The online <strong>documentation</strong> are now hosted on <a href="http://lambdamusic.github.io/Ontospy/">github pages</a> and available within the <code style="display: inline; padding: 0px;">/docs</code> folder of the project.</li>
<li>Improved support for <b>JSON-LD</b> and a new utility for quickly sending JSON-LD data to the <a href="https://json-ld.org/playground/">online playground tool</a>.</li>
<li>Several other bug fixes and improvements, in particular to the <b>interactive</b> ontology exploration mode (<code style="display: inline; padding: 0px;">shell</code> command), the visualization library (<b>new visualizations</b> are available &#8211; albeit still in alpha state).</li>
<p> &nbsp;</p>
]]></content:encoded>
									<post-id xmlns="com-wordpress:feed-additions:1">3279</post-id>	</item>
		<item>
		<title>Exploring scholarly publications using DBPedia concepts: an experiment</title>
		<link>http://www.michelepasin.org/blog/2018/11/23/exploring-scholarly-publications-via-dbpedia/</link>
				<pubDate>Fri, 23 Nov 2018 17:18:48 +0000</pubDate>
		<dc:creator><![CDATA[mikele]]></dc:creator>
				<category><![CDATA[Information Architecture]]></category>
		<category><![CDATA[Semantic Web]]></category>
		<category><![CDATA[d3.js]]></category>
		<category><![CDATA[dbpedia]]></category>
		<category><![CDATA[linkeddata]]></category>

		<guid isPermaLink="false">http://www.michelepasin.org/blog/?p=3254</guid>
				<description><![CDATA[This post is about a recent prototype I developed, which allows to explore a sample collection of Springer Nature publications using subject tags automatically extracted from DBPedia. DBpedia is a crowd-sourced community effort to extract structured content from the information created in various Wikimedia projects. This structured information resembles an open knowledge graph (OKG) which is available for everyone [&#8230;]]]></description>
								<content:encoded><![CDATA[<p>This post is about a recent <a class="jive-link-external-small" href="http://hacks2019.michelepasin.org/dbpedialinks" target="_blank" rel="nofollow noopener">prototype</a> I developed, which allows to explore a sample collection of Springer Nature publications using subject tags automatically extracted from DBPedia.</p>
<blockquote><p><a class="jive-link-external-small" href="https://wiki.dbpedia.org/about" target="_blank" rel="nofollow noopener">DBpedia</a> is a crowd-sourced community effort to extract structured content from the information created in various Wikimedia projects. This structured information resembles an open knowledge graph (OKG) which is available for everyone on the Web.</p></blockquote>
<h3>Datasets</h3>
<p>The dataset I used is the result of a <strong>collaboration with <a href="https://twitter.com/beyza__yaman?lang=en">Beyza Yaman</a></strong>, a researcher working with the <a class="jive-link-external-small" href="https://wiki.dbpedia.org/about" target="_blank" rel="nofollow noopener">DBpedia</a> team in Leipzig, who used the <a class="jive-link-external-small" href="https://scigraph.springernature.com/explorer" target="_blank" rel="nofollow noopener">SciGraph datasets</a> as input to the <a class="jive-link-external-small" href="https://www.dbpedia-spotlight.org/" target="_blank" rel="nofollow noopener">DBPedia-Spotlight</a> entity-mining tool.</p>
<p>By using <a class="jive-link-external-small" href="https://www.dbpedia-spotlight.org/" target="_blank" rel="nofollow noopener">DBPedia-Spotlight</a> we automatically associated DBpedia subjects terms to a  subset of abstracts available in the SciGraph dataset (around 90k abstract from 2017 publications).</p>
<p>The prototype allows to search the Springer Nature publications using these subject terms.</p>
<p>Also, DBpedia subjects include definitions and semantic relationships (which we are currently not using, but one can imagine how they could be raw material for generating more thematic &#8216;pathways&#8217;).</p>
<h3>Results: serendipitous discovery of scientific publications</h3>
<p><strong>The results are pretty encouraging: </strong>despite the fact that the concepts extracted sometimes are only marginally relevant (or not relevant at all), the breadth and depth of the DBpedia classification makes the interactive <strong>exploration</strong> quite <strong>interesting</strong> and <strong>serendipitous</strong>.</p>
<p style="padding-left: 30px;">You can judge for yourself: <strong>the tool is available here</strong>: <a class="jive-link-external-small" href="http://hacks2019.michelepasin.org/dbpedialinks/" target="_blank" rel="nofollow noopener">http://hacks2019.michelepasin.org/dbpedialinks</a></p>
<p>The purpose of this prototype is to evaluate the quality of the tagging and generate ideas for future applications. So any kind of feedback or ideas is very welcome!</p>
<p>We are working with Beyza to write up the results of this investigation as a research paper. The data and software is already freely available on <a class="jive-link-external-small" href="https://github.com/dbpedia/sci-graph-links/" target="_blank" rel="nofollow noopener">github</a>.</p>
<h3>A couple of screenshots:</h3>
<p>Eg see the topic &#8216;<a href="http://hacks2019.michelepasin.org/dbpedialinks/entities/63004">artificial intelligence</a>&#8216;</p>
<p><a href="http://www.michelepasin.org/blog/wp-content/uploads/2018/11/Screen-Shot-2018-11-23-at-17.15.07.png"><img class=" wp-image-3255  aligncenter" src="http://www.michelepasin.org/blog/wp-content/uploads/2018/11/Screen-Shot-2018-11-23-at-17.15.07.png" alt="Screen Shot 2018-11-23 at 17.15.07.png" width="557" height="347" srcset="http://www.michelepasin.org/blog/wp-content/uploads/2018/11/Screen-Shot-2018-11-23-at-17.15.07.png 1201w, http://www.michelepasin.org/blog/wp-content/uploads/2018/11/Screen-Shot-2018-11-23-at-17.15.07-300x187.png 300w, http://www.michelepasin.org/blog/wp-content/uploads/2018/11/Screen-Shot-2018-11-23-at-17.15.07-768x478.png 768w, http://www.michelepasin.org/blog/wp-content/uploads/2018/11/Screen-Shot-2018-11-23-at-17.15.07-1024x638.png 1024w" sizes="(max-width: 557px) 100vw, 557px" /></a></p>
<p>One can add more subjects to a search in order to &#8216;zoom in&#8217; into a results set, eg by <a class="jive-link-external-small" href="http://hacks2019.michelepasin.org/dbpedialinks/entities/63004?filters=61361" target="_blank" rel="nofollow noopener">adding &#8216;China&#8217; </a>to the search:</p>
<p><a href="http://www.michelepasin.org/blog/wp-content/uploads/2018/11/Screen-Shot-2018-11-23-at-17.16.38.png"><img class="  wp-image-3256 aligncenter" src="http://www.michelepasin.org/blog/wp-content/uploads/2018/11/Screen-Shot-2018-11-23-at-17.16.38.png" alt="Screen Shot 2018-11-23 at 17.16.38" width="573" height="360" srcset="http://www.michelepasin.org/blog/wp-content/uploads/2018/11/Screen-Shot-2018-11-23-at-17.16.38.png 1217w, http://www.michelepasin.org/blog/wp-content/uploads/2018/11/Screen-Shot-2018-11-23-at-17.16.38-300x189.png 300w, http://www.michelepasin.org/blog/wp-content/uploads/2018/11/Screen-Shot-2018-11-23-at-17.16.38-768x483.png 768w, http://www.michelepasin.org/blog/wp-content/uploads/2018/11/Screen-Shot-2018-11-23-at-17.16.38-1024x644.png 1024w" sizes="(max-width: 573px) 100vw, 573px" /></a></p>
<h3>Implementation details</h3>
<ul>
<li>Main webapp is using Python and <a href="https://www.djangoproject.com/">Django</a>. I&#8217;ve finally found a good excuse to upgrade to the latest release (<a href="https://docs.djangoproject.com/en/2.1/">2.1</a>) so very proud of myself!</li>
<li><a href="https://getbootstrap.com/docs/3.3/components/">Bootstrap</a></li>
<li><a href="https://d3js.org/">d3.js </a>and in particular I&#8217;ve learned a lot from this great <a href="http://bl.ocks.org/eyaler/10586116">force-directed layout example</a></li>
<li>The Springer Nature <a href="https://scigraph.springernature.com">SciGraph</a> dataset of scientific publications</li>
<li>The <a href="https://www.dbpedia-spotlight.org/demo/">DBpedia Spotlight</a> tool</li>
</ul>
<p>&nbsp;</p>
<p>&nbsp;</p>
]]></content:encoded>
									<post-id xmlns="com-wordpress:feed-additions:1">3254</post-id>	</item>
		<item>
		<title>SN SciGraph: latest website release make it easier to discover related content</title>
		<link>http://www.michelepasin.org/blog/2018/08/01/sn-scigraph-latest-website-release-make-it-easier-to-discover-related-content/</link>
				<pubDate>Wed, 01 Aug 2018 08:26:19 +0000</pubDate>
		<dc:creator><![CDATA[mikele]]></dc:creator>
				<category><![CDATA[Information Architecture]]></category>
		<category><![CDATA[Semantic Web]]></category>
		<category><![CDATA[linkeddata]]></category>
		<category><![CDATA[navigation]]></category>
		<category><![CDATA[news]]></category>
		<category><![CDATA[opendata]]></category>
		<category><![CDATA[scigraph]]></category>

		<guid isPermaLink="false">http://www.michelepasin.org/blog/?p=3239</guid>
				<description><![CDATA[The latest release of SN SciGraph Explorer website includes a number of new features that make it easier to navigate the scholarly knowledge graph and discover items of interest. Graphs are essentially composed by two kinds of objects: nodes and edges. Nodes are like the stations in a train map, while edges are the links that [&#8230;]]]></description>
								<content:encoded><![CDATA[<p>The latest release of SN SciGraph Explorer website includes a number of new features that make it easier to navigate the scholarly knowledge graph and discover items of interest.</p>
<p>Graphs are essentially composed by two kinds of objects: <a href="https://en.wikipedia.org/wiki/Graph_theory" target="_blank" rel="noopener">nodes and edges</a>. Nodes are like the stations in a train map, while edges are the links that connect the different stations.</p>
<p>Of course one wants to be able to move from station to station in any direction! Similarly in a graph one wants to be able to jump back and forth from node to node using any of the links provided. That&#8217;s the beauty of it!</p>
<p>Although the underlying data allowed for this, the <a href="https://scigraph.springernature.com/explorer" target="_blank" rel="noopener">SN SciGraph Explorer website</a> wasn&#8217;t fully supporting this kind of navigation. So we&#8217;ve now started to add a number of &#8216;related objects&#8217; sections that reveal these pathways more clearly.</p>
<p>For example, now it&#8217;s much easier to get to the organizations and grants an <a href="https://scigraph.springernature.com/things/articles/847fd3229b37409e49155ed96d3aabd4" target="_blank" rel="noopener">article</a> relates to:</p>
<p><a href="https://scigraph.springernature.com/things/articles/847fd3229b37409e49155ed96d3aabd4" target="_blank" rel="noopener"><img class="  wp-image-3240 aligncenter" src="http://www.michelepasin.org/blog/wp-content/uploads/2018/08/Screen-Shot-2018-07-31-at-18.23.47.png" alt="Screen Shot 2018-07-31 at 18.23.47.png" width="624" height="424" /></a></p>
<p>Or, for a <a href="https://scigraph.springernature.com/things/book-editions/3101c98cc1558e4fd4e533e8b7caad87" target="_blank" rel="noopener">book edition</a>, to see its chapters and related organizations:</p>
<p><a href="https://scigraph.springernature.com/things/book-editions/3101c98cc1558e4fd4e533e8b7caad87" target="_blank" rel="noopener"><img class="  wp-image-3241 aligncenter" src="http://www.michelepasin.org/blog/wp-content/uploads/2018/08/bookEdition-links.png" alt="bookEdition-links.png" width="587" height="389" /></a></p>
<p>And much more..  Take a look at the <a href="https://scigraph.springernature.com/explorer" target="_blank" rel="noopener">site</a> yourself to find out.</p>
<p>Finally, we improved the <em>linked data </em>visualization included in every page by adding distinctive icons to each object type &#8211; so to make it easier to understand the immediate network of an object at a glance. E.g. see this <a href="https://scigraph.springernature.com/things/grants/c741f014f7e20fdd3bd510aac2c16693" target="_blank" rel="noopener">grant</a>:</p>
<p><a href="https://scigraph.springernature.com/things/grants/c741f014f7e20fdd3bd510aac2c16693" target="_blank" rel="noopener"><img class="  wp-image-3242 aligncenter" src="http://www.michelepasin.org/blog/wp-content/uploads/2018/08/grant-diagram.png" alt="grant-diagram.png" width="628" height="385" /></a></p>
<p>SN SciGraph is primarily about opening up new opportunities for open data and metadata enthusiasts who want to do more things with our content, so we hope that these additions will make discovering data items easier and more fun.</p>
<p>Any comments? We&#8217;d love to hear from you. Otherwise, thanks for reading and stay tuned for more updates.</p>
<p style="text-align: right;"><em><span style="color: #800000;">PS: this blog was posted on the <a href="https://researchdata.springernature.com/users/82895-sn-scigraph/posts/37235-sn-scigraph-latest-website-release-make-it-easier-to-discover-related-content">SN Research Data</a> space too.</span></em></p>
<p>&nbsp;</p>
]]></content:encoded>
									<post-id xmlns="com-wordpress:feed-additions:1">3239</post-id>	</item>
		<item>
		<title>Exploring SciGraph data using JSON-LD, Elastic Search and Kibana</title>
		<link>http://www.michelepasin.org/blog/2017/04/06/exploring-scigraph-data-using-elastic-search-and-kibana/</link>
				<comments>http://www.michelepasin.org/blog/2017/04/06/exploring-scigraph-data-using-elastic-search-and-kibana/#comments</comments>
				<pubDate>Thu, 06 Apr 2017 14:12:05 +0000</pubDate>
		<dc:creator><![CDATA[mikele]]></dc:creator>
				<category><![CDATA[Information Architecture]]></category>
		<category><![CDATA[Semantic Web]]></category>
		<category><![CDATA[data exploration]]></category>
		<category><![CDATA[elasticsearch]]></category>
		<category><![CDATA[graph]]></category>
		<category><![CDATA[jsonld]]></category>
		<category><![CDATA[kibana]]></category>
		<category><![CDATA[linkeddata]]></category>
		<category><![CDATA[nature]]></category>
		<category><![CDATA[scigraph]]></category>
		<category><![CDATA[visualization]]></category>

		<guid isPermaLink="false">http://www.michelepasin.org/blog/?p=2844</guid>
				<description><![CDATA[Hello there data lovers! In this post you can find some information on how to download and make some sense of the scholarly dataset recently made available by the Springer Nature SciGraph project, by using the freely available Elasticsearch suite of software. A few weeks ago the SciGraph dataset was released (full disclosure: I&#8217;m part [&#8230;]]]></description>
								<content:encoded><![CDATA[<p>Hello there data lovers! In this post you can find some information on how to download and make some sense of the scholarly dataset recently made available by the Springer Nature <a href="http://www.springernature.com/scigraph">SciGraph project</a>, by using the freely available <a href="https://en.wikipedia.org/wiki/Elasticsearch">Elasticsearch</a> suite of software.</p>
<p>A few weeks ago the SciGraph dataset was <a href="http://www.springernature.com/gp/group/media/press-releases/springer-nature-scigraph--supporting-open-science-and-the-wider-understanding-of-research/12129614">released</a> (full disclosure: I&#8217;m part of the team who did that!). This is a high quality dataset containing metadata and abstracts about scientific articles published by <a href="https://en.wikipedia.org/wiki/Springer_Nature">Springer Nature</a>, research grants related to them plus other classifications of this content.</p>
<p><img class="alignnone size-full wp-image-3123" src="http://www.michelepasin.org/blog/wp-content/uploads/2017/04/scigraph.png?w=1116" alt="scigraph.png" width="558" height="142" srcset="http://www.michelepasin.org/blog/wp-content/uploads/2017/04/scigraph.png 1888w, http://www.michelepasin.org/blog/wp-content/uploads/2017/04/scigraph-300x76.png 300w, http://www.michelepasin.org/blog/wp-content/uploads/2017/04/scigraph-768x195.png 768w, http://www.michelepasin.org/blog/wp-content/uploads/2017/04/scigraph-1024x260.png 1024w" sizes="(max-width: 558px) 100vw, 558px" /></p>
<p>This release of the dataset includes the last 5 years of content &#8211; that&#8217;s already an impressive <strong>32 gigs of data</strong> you can get your hands on. So in this post I&#8217;m going to show how to do that, in particular by transforming the data from the <a href="https://en.wikipedia.org/wiki/Resource_Description_Framework">RDF graph</a> format they come with, into a <a href="https://en.wikipedia.org/wiki/JSON">JSON format</a> which is more suited for application development and analytics.</p>
<p>We will be using two free-to-download products, <a href="http://ontotext.com/products/graphdb/">GraphDB</a> and <a href="https://www.elastic.co/">Elasticsearch</a>, so you&#8217;ll have to install them if you haven&#8217;t got them already. But no worries, that&#8217;s pretty straighforward, as you&#8217;ll see below.</p>
<h2>1. Hello SciGraph Linked Data</h2>
<p>First things first, we want to get hold of the SciGraph RDF datasets of course. That&#8217;s pretty easy, just head over to the SciGraph <a href="https://github.com/springernature/scigraph/wiki#downloads">downloads page</a> and get the following datasets:</p>
<ul>
<li><strong>Ontologies</strong>: the main <a href="http://s3-service-broker-live-afe45d64-24d0-4a96-b6a8-23b79e885eb7.s3-website.eu-central-1.amazonaws.com/2017-02-15/springernature-scigraph-ontologies.2017-02-15.nt.bz2">schema</a> behind SciGraph.</li>
<li><strong>Articles</strong> <strong>&#8211; 2016</strong>: all the core <a href="http://s3-service-broker-live-afe45d64-24d0-4a96-b6a8-23b79e885eb7.s3-website.eu-central-1.amazonaws.com/2017-02-15/springernature-scigraph-2016-articles.2017-02-15.nt.bz2">articles</a> metadata for one year.</li>
<li><strong>Grants</strong>: <a href="http://s3-service-broker-live-afe45d64-24d0-4a96-b6a8-23b79e885eb7.s3-website.eu-central-1.amazonaws.com/2017-02-15/springernature-scigraph-grants.2017-02-15.nt.bz2">grants</a> metadata related to those articles.</li>
<li><strong>Journals</strong>: full list of Springer Nature <a href="http://s3-service-broker-live-afe45d64-24d0-4a96-b6a8-23b79e885eb7.s3-website.eu-central-1.amazonaws.com/2017-02-15/springernature-scigraph-journals.2017-02-15.nt.bz2">journal catalogue</a>.</li>
<li><strong>Subjects</strong>: classification of <a href="http://s3-service-broker-live-afe45d64-24d0-4a96-b6a8-23b79e885eb7.s3-website.eu-central-1.amazonaws.com/2017-02-15/springernature-scigraph-subjects.2017-02-15.nt.bz2">research areas</a> developed by Springer Nature.</li>
</ul>
<p>That&#8217;s pretty much everything, only thing we&#8217;re getting only one year worth of articles as that&#8217;s enough for the purpose of this exercise (~300k articles from 2016).</p>
<p>Next up, we want to get a couple of other datasets SciGraph depends on:</p>
<ul>
<li><strong>GRID</strong>: a catalogue of the world&#8217;s research organisations. Make sure you get both the <a href="http://www.grid.ac/ontology/">ontology</a> and one of the <a href="https://www.grid.ac/downloads">latest releases</a>, within which you can find an RDF implementation too.</li>
<li><strong>Field Of Research codes</strong>: another classification scheme used in SciGraph, developed by the <a href="https://vocabs.ands.org.au/anzsrc-for">Australian and New Zealand Standard Research Classification</a> organization.</li>
</ul>
<p>That&#8217;s it! Time for a cup of coffee.</p>
<h2>2. Python to the help</h2>
<p>We will be doing a bit of data manipulation  in the next sections and Python is a great language for that sort of thing. Here&#8217;s what we need to get going:</p>
<ol>
<li><strong>Python</strong>. Make sure you have <a href="https://wiki.python.org/moin/BeginnersGuide/Download">Python installed</a> and also <a href="https://packaging.python.org/installing/">Pip</a>, the Python package manager (any Python version <strong>above 2.7</strong> should be ok).</li>
<li><strong>GitHub project</strong>. I&#8217;ve created a few scripts for this tutorial, so head over to the <a href="https://github.com/lambdamusic/hello-scigraph">hello-scigraph project on GitHub</a> and download it to your computer. Note: the project contains all the Python scripts needed to complete this tutorial, but of course you should feel free to modify them or write from scratch if you fancy it!</li>
<li><strong>Libraries</strong>. Install all the dependencies for the hello-scigraph project to run. You can do that by cd-ing into the project folder and running <code style="display: inline; padding: 0px;">pip install -r requirements.txt</code> (ideally within a <a href="http://python-guide-pt-br.readthedocs.io/en/latest/dev/virtualenvs/">virtual environment</a>, but that&#8217;s up to you).</li>
</ol>
<h2>3. Loading the data into GraphDB</h2>
<p>So, you should have by now 8 different files containing data (after step 1 above). Make sure they&#8217;re all in the same folder and that all of them have been unzipped (if needed), then head over to the <a href="http://ontotext.com/graphdb-8-enterprise-linked-data/">GraphDB website</a> and download the free version of the triplestore (you may have to sign up first).</p>
<p>The <a href="http://graphdb.ontotext.com/documentation/free/quick-start-guide.html#run-graphdb-as-a-stand-alone-server">online documentation</a> for GraphDB is pretty good, so it should be easy to get it up and running. In essence, you have to do the following steps:</p>
<ol>
<li><strong>Launch the application</strong>: for me, on a mac, I just had to double click the GraphDB icon &#8211; nice!</li>
<li><strong>Create a</strong> <a href="http://graphdb.ontotext.com/documentation/free/quick-start-guide.html#create-a-repository">new repository</a>: this is the equivalent of a database within the triplestore. Call this repo <span class="pl-pds">&#8220;</span><strong>scigraph-2016</strong><span class="pl-pds">&#8221; so that we&#8217;re all synced for the following steps.</span></li>
</ol>
<p>Next thing, we want a script to load our RDF files into this empty repository. So cd into the directory containg the GitHub project (from step 2) and run the following command:</p>
<pre style="background: lightgray; padding: 5px;">python -m hello-scigraph.loadGraphDB ~/scigraph-downloads/</pre>
<p>The &#8220;loadGraphDB&#8221; script goes through all RDF files in the &#8220;scigraph-downloads&#8221; directory and loads them into the <strong>scigraph-2016 </strong>repository (note: you must replace &#8220;scigraph-downloads&#8221; with the actual path to the folder you downloaded content in step 1 above).</p>
<p><strong>So, to recap:</strong> this script is now loading more than 35 million triples into your local graph database. Don&#8217;t be surprised if it&#8217;ll take some time (in particular the &#8216;articles-2016&#8217; dataset, by far the biggest) so it&#8217;s time to take a break or do something else.</p>
<p>Once the process it&#8217;s finished, you should be able to <a href="http://graphdb.ontotext.com/documentation/free/quick-start-guide.html#explore-your-data-and-class-relationships">explore your data via the GraphDB workbench</a>.  It&#8217;ll look something like this:</p>
<p><img class="aligncenter size-large wp-image-2954" src="http://www.michelepasin.org/blog/wp-content/uploads/2017/04/GraphDB-class-hierarchy-1024x525.png" alt="GraphDB-class-hierarchy" width="1024" height="525" srcset="http://www.michelepasin.org/blog/wp-content/uploads/2017/04/GraphDB-class-hierarchy-1024x525.png 1024w, http://www.michelepasin.org/blog/wp-content/uploads/2017/04/GraphDB-class-hierarchy-300x154.png 300w, http://www.michelepasin.org/blog/wp-content/uploads/2017/04/GraphDB-class-hierarchy-768x394.png 768w" sizes="(max-width: 1024px) 100vw, 1024px" /></p>
<h2>4. Creating an Elasticsearch index</h2>
<p>We&#8217;re almost there. Let&#8217;s head over to the <a href="https://www.elastic.co/downloads/elasticsearch">Elasticsearch website</a> and <strong>download</strong> it. Elasticsearch is a powerful, distributed, JSON-based search and analytics engine so we&#8217;ll be using it to build an analytics dashboard for the SciGraph data.</p>
<p>Make sure Elastic is running (run <code style="display: inline; padding: 0px;">bin/elasticsearch</code> (or <code style="display: inline; padding: 0px;">bin\elasticsearch.bat</code> on Windows), then cd into the hello-scigraph Python project (from step 2) in order to run the following script:</p>
<pre style="background: lightgray; padding: 5px;">python -m hello-scigraph.loadElastic</pre>
<p>If you <a href="https://github.com/lambdamusic/hello-scigraph/blob/master/hello-scigraph/loadElastic.py">take a look at the source code</a>, you&#8217;ll see that the script does the following:</p>
<ol>
<li><strong>Articles loading:</strong> extracts articles references from GraphDB in batches of 200.</li>
<li><strong>Articles metadata extraction:</strong> for each article, we <a href="https://github.com/lambdamusic/hello-scigraph/blob/master/hello-scigraph/queries.py">pull out all relevant metadata</a> (e.g. title, DOI, authors) plus related information (e.g. author GRID organizations, geo locations, funding info etc..).</li>
<li><strong>Articles metadata simplification: </strong> some intermediate nodes coming from the orginal RDF graph are dropped and replaced with a flatter structure which uses a a temporary dummy schema (<code style="display: inline; padding: 0px;">prefix es: &lt;http://elastic-index.scigraph.com/&gt;</code> It doesn&#8217;t matter what we call that schema, but what&#8217;s important is to that we want to simplify the data we put into the Elastic search index. That&#8217;s because while the Graph layer is supposed to facilitate <em>data integration </em>and hence it benefits from a rich semantic representation of information, the <em>search layer</em> is more geared towards performance and retrieval hence a leaner information structure can dramatically speed things up there.</li>
<li><strong>JSON-LD transformation</strong>: the simplified RDF data structure is <a href="http://json-ld.org/">serialized as JSON-LD</a> &#8211; one of the many serializations available for RDF. JSON-LD is of course valid JSON, meaning that we can put that into Elastic right away. This is a bit of a shortcut actually, in fact for a more fine-grained control of how the JSON looks like,  it&#8217;s probably better to transform the data into JSON using some ad-hoc mechanism. But for the purpose of this tutorial it&#8217;s more than enough.</li>
<li><strong>Elastic index creation.</strong> Finally, we can load the data into an Elastic index called &#8211; guess what &#8211; &#8220;hello-scigraph&#8221;.</li>
</ol>
<p>Two more things to point out:</p>
<ul>
<li><strong>Long queries.</strong> The Python script enforces a 60 seconds <a href="https://github.com/lambdamusic/hello-scigraph/blob/master/hello-scigraph/timeout.py">time-out </a>on the GraphDB queries, so in case things go wrong with some articles data the script should keep running.</li>
<li><strong>Memory issues.</strong> The script stops for 10 seconds after each batch of 200 articles (<code style="display: inline; padding: 0px;">time.sleep(10)</code>). Had to do this to prevent GraphDB on my laptop from running out of memory. Time to catch some breath!</li>
</ul>
<p>That&#8217;s it! <strong>Time for another break  now.</strong> A pretty long one actually &#8211; loading all the data took around 10 hours on my (rather averaged spec&#8217;ed) laptop so you may want to do that overnight or get hold of a faster machine/server.</p>
<p>Eventually, once the loading script is finished, you can issue this command from the command line to see how much data you&#8217;ve loaded into the Elastic index  &#8220;hello-scigraph&#8221;. Bravo!</p>
<pre style="background: lightgray; padding: 5px;">curl -XGET 'localhost:9200/_cat/indices/'</pre>
<h2>5. Analyzing the data with Kibana</h2>
<p>Loading the data in Elastic already opens up a number of possibilites &#8211; check out the <a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/search.html">search APIs </a>for some ideas &#8211; however there&#8217;s an even quicker way to analyze the data: <strong>Kibana</strong>. <a href="https://www.elastic.co/products/kibana">Kibana</a> is another free product in the Elastic Search suite, which provides an extensible user interface for configuring and managing all aspects of the Elastic Stack.</p>
<p>So let&#8217;s get started with Kibana: <a href="https://www.elastic.co/downloads/kibana">download it</a> and set it up using the online instructions, then point your browser at <a href="http://localhost:5601">http://localhost:5601 </a>.</p>
<p>You&#8217;ll get to the Kibana dashboard which shows the index we just created. Here you can perform any kind of searches and see the raw data as JSON.</p>
<p>What&#8217;s even more interesting is the <strong>visualization tab</strong>. Results of searches can be rendered as line chart, pie charts etc.. and more dimensions can be added via &#8216;buckets&#8217;. See below for some quick examples, but really, the possibilities are endless!</p>

<a href='http://www.michelepasin.org/blog/2017/04/06/exploring-scigraph-data-using-elastic-search-and-kibana/kibana-super-collider-1/'><img width="300" height="153" src="http://www.michelepasin.org/blog/wp-content/uploads/2017/04/Kibana-super-collider-1-300x153.png" class="attachment-medium size-medium" alt="" srcset="http://www.michelepasin.org/blog/wp-content/uploads/2017/04/Kibana-super-collider-1-300x153.png 300w, http://www.michelepasin.org/blog/wp-content/uploads/2017/04/Kibana-super-collider-1-768x391.png 768w, http://www.michelepasin.org/blog/wp-content/uploads/2017/04/Kibana-super-collider-1-1024x521.png 1024w" sizes="(max-width: 300px) 100vw, 300px" /></a>
<a href='http://www.michelepasin.org/blog/2017/04/06/exploring-scigraph-data-using-elastic-search-and-kibana/kibana-super-collider-2/'><img width="300" height="153" src="http://www.michelepasin.org/blog/wp-content/uploads/2017/04/Kibana-super-collider-2-300x153.png" class="attachment-medium size-medium" alt="" srcset="http://www.michelepasin.org/blog/wp-content/uploads/2017/04/Kibana-super-collider-2-300x153.png 300w, http://www.michelepasin.org/blog/wp-content/uploads/2017/04/Kibana-super-collider-2-768x391.png 768w, http://www.michelepasin.org/blog/wp-content/uploads/2017/04/Kibana-super-collider-2-1024x521.png 1024w" sizes="(max-width: 300px) 100vw, 300px" /></a>
<a href='http://www.michelepasin.org/blog/2017/04/06/exploring-scigraph-data-using-elastic-search-and-kibana/kibana-super-collider-3/'><img width="300" height="171" src="http://www.michelepasin.org/blog/wp-content/uploads/2017/04/Kibana-super-collider-3-300x171.png" class="attachment-medium size-medium" alt="" srcset="http://www.michelepasin.org/blog/wp-content/uploads/2017/04/Kibana-super-collider-3-300x171.png 300w, http://www.michelepasin.org/blog/wp-content/uploads/2017/04/Kibana-super-collider-3-768x438.png 768w, http://www.michelepasin.org/blog/wp-content/uploads/2017/04/Kibana-super-collider-3-1024x584.png 1024w" sizes="(max-width: 300px) 100vw, 300px" /></a>
<a href='http://www.michelepasin.org/blog/2017/04/06/exploring-scigraph-data-using-elastic-search-and-kibana/kibana-super-collider-4/'><img width="300" height="173" src="http://www.michelepasin.org/blog/wp-content/uploads/2017/04/Kibana-super-collider-4-300x173.png" class="attachment-medium size-medium" alt="" srcset="http://www.michelepasin.org/blog/wp-content/uploads/2017/04/Kibana-super-collider-4-300x173.png 300w, http://www.michelepasin.org/blog/wp-content/uploads/2017/04/Kibana-super-collider-4-768x444.png 768w, http://www.michelepasin.org/blog/wp-content/uploads/2017/04/Kibana-super-collider-4-1024x592.png 1024w" sizes="(max-width: 300px) 100vw, 300px" /></a>

<h2>Conclusion</h2>
<p>This post should have given you enough to realise that:</p>
<ol>
<li>The <strong>SciGraph dataset</strong> contain an impressive amount of high-quality scholarly publications metadata which can be used for things like literature search, research statistics etc..</li>
<li>Even though you&#8217;re not familiar with Linked Data and the RDF family of languages, it&#8217;s not hard to get going with a triplestore and then <strong>transform the data</strong> into a more widely used format like JSON.</li>
<li>Finally, <strong>Elasticsearch</strong> and especially <strong>Kibana</strong> are fantastic tools for data analysis and exploration! Needless to say, in this post I&#8217;ve just scratched the surface of what could be done with it.</li>
</ol>
<p>Hope this was fun, any questions or comments, you know the drill :-)</p>
]]></content:encoded>
							<wfw:commentRss>http://www.michelepasin.org/blog/2017/04/06/exploring-scigraph-data-using-elastic-search-and-kibana/feed/</wfw:commentRss>
		<slash:comments>4</slash:comments>
						<post-id xmlns="com-wordpress:feed-additions:1">2844</post-id>	</item>
		<item>
		<title>Ontospy v. 1.6.7</title>
		<link>http://www.michelepasin.org/blog/2016/06/12/ontospy-v-1-6-7/</link>
				<pubDate>Sun, 12 Jun 2016 18:05:51 +0000</pubDate>
		<dc:creator><![CDATA[mikele]]></dc:creator>
				<category><![CDATA[Information Architecture]]></category>
		<category><![CDATA[Semantic Web]]></category>
		<category><![CDATA[ontology]]></category>
		<category><![CDATA[ontospy]]></category>
		<category><![CDATA[python]]></category>

		<guid isPermaLink="false">http://www.michelepasin.org/blog/?p=2783</guid>
				<description><![CDATA[A new and improved version of OntoSpy (1.6.7) is available online. OntoSpy is a lightweight Python library and command line tool for inspecting and visualizing vocabularies encoded in the RDF family of languages. This update includes support for Python 3, plus various other improvements that make it easier to query semantic web vocabularies using OntoSpy&#8217;s [&#8230;]]]></description>
								<content:encoded><![CDATA[<p>A new and improved version of <a href="https://github.com/lambdamusic/ontospy">OntoSpy</a> (1.6.7) is available online. OntoSpy is a lightweight Python library and command line tool for inspecting and visualizing vocabularies encoded in the <a href="https://en.wikipedia.org/wiki/Resource_Description_Framework">RDF</a> family of languages.</p>
<p>This update includes support for Python 3, plus various other improvements that make it easier to query semantic web vocabularies using OntoSpy&#8217;s interactive shell module. To find out more about Ontospy:</p>
<li>Docs: <a href="http://ontospy.readthedocs.org">http://ontospy.readthedocs.org</a></li>
<li>CheeseShop: <a href="https://pypi.python.org/pypi/ontospy">https://pypi.python.org/pypi/ontospy</a></li>
<li>Github: <a href="https://github.com/lambdamusic/ontospy">https://github.com/lambdamusic/ontospy</a></li>
<p> <br />
Here&#8217;s a short video showing a typical sessions with the OntoSpy <a href="https://en.wikipedia.org/wiki/Read%E2%80%93eval%E2%80%93print_loop">repl</a>: </p>
<p><iframe src="https://player.vimeo.com/video/169707591" width="640" height="360" frameborder="0" webkitallowfullscreen mozallowfullscreen allowfullscreen></iframe></p>
<h3>What&#8217;s new in this release</h3>
<p>The main new features of version 1.6.7:<br />
 </p>
<li>added support for Python 3.0 (thanks to a pull request from <a href="https://github.com/T-002">https://github.com/T-002</a>)</li>
<li>the <span style="font-family:monospace;color:#000000; ">import [file | uri | repo | starter-pack]</span> command that makes it easier to load models into the local repository. You can import a local RDF file or a web resource via its URI. The <span style="font-family:monospace;color:#000000; ">repo</span> option allows to select an ontology by listing the one available in a couple of online public repositories; finally the <span style="font-family:monospace;color:#000000; ">starter-pack</span> option can be used to download automatically a few widely used vocabularies (e.g. FOAF,DC etc..) into the local repository &#8211; mostly useful after a fresh installation in order to get started</li>
<li>the <span style="font-family:monospace;color:#000000; ">info [toplayer | parents | children | ancestors | descendants]</span> command allows to print more detailed info about entities</li>
<li>added an incremental search mode based on text patterns e.g. to reduce the options returned by the <span style="font-family:monospace;color:#000000; ">ls</span> command </li>
<li>calling the <span style="font-family:monospace;color:#000000; ">serialize</span> command at ontology level now serializes the whole graph</li>
<li>made the caching functionality version-dependent </li>
<li>added json serialization option (via <a href="https://github.com/RDFLib/rdflib-jsonld">rdflib-jsonld</a>)</li>
<p> </p>
<p>Install/update simply by typing <span style="font-family:monospace;color:#000000; ">pip install ontospy -U</span> in your terminal window (see this <a href="http://ontospy.readthedocs.io/en/latest/installation.html">page</a> for more info).</p>
<h3>Coming up next</h3>
<p>I&#8217;d really like to add more output visualisations e.g. <a href="https://github.com/anvaka/VivaGraphJS">VivaGraphJS</a> or one of the <a href="http://philogb.github.io/jit/demos.html">JavaScript InfoVis Toolkit</a>.</p>
<p>Probably even more interesting, I&#8217;d like to refactor the code generating visualisations so that it allows people to develop their own via a standard API and then publishing them on GitHub. </p>
<p>Lastly, more support for instance management: querying and creating instances from any loaded ontology.</p>
<p>Of course, any comments or suggestions are welcome as usual &#8211;  either using the form below or via <a href="https://github.com/lambdamusic/ontospy/issues">GitHub</a>. Cheers!</p>
<p>&nbsp;</p>
]]></content:encoded>
									<post-id xmlns="com-wordpress:feed-additions:1">2783</post-id>	</item>
		<item>
		<title>Nature.com Subjects Stream Graph</title>
		<link>http://www.michelepasin.org/blog/2016/01/03/nature-com-subjects-stream-graph/</link>
				<pubDate>Sun, 03 Jan 2016 00:28:08 +0000</pubDate>
		<dc:creator><![CDATA[mikele]]></dc:creator>
				<category><![CDATA[Information Architecture]]></category>
		<category><![CDATA[d3]]></category>
		<category><![CDATA[infographics]]></category>
		<category><![CDATA[nature]]></category>
		<category><![CDATA[visualization]]></category>

		<guid isPermaLink="false">http://www.michelepasin.org/blog/?p=2750</guid>
				<description><![CDATA[The nature.com subjects stream graph displays the distribution of content across the subject areas covered by the nature.com portal. This is an experimental interactive visualisation based on a freely available dataset from the nature.com linked data platform, which I&#8217;ve been working on in the last few months. The main visualization provides an overview of selected [&#8230;]]]></description>
								<content:encoded><![CDATA[<p>The <a href="http://www.nature.com/developers/hacks/articles/stream/by-subject/">nature.com subjects stream graph</a> displays the distribution of content across the subject areas covered by the nature.com portal. </p>
<p>This is an experimental interactive visualisation based on a freely available dataset from the <a href="http://www.nature.com/ontologies/datasets/">nature.com linked data platform</a>, which I&#8217;ve been working on in the last few months.</p>
<p><a href="http://www.michelepasin.org/blog/wp-content/uploads/2014/10/streamgraph6.png"><img src="http://www.michelepasin.org/blog/wp-content/uploads/2014/10/streamgraph6-1024x657.png" alt="streamgraph" width="600" class="alignnone size-large wp-image-2761" srcset="http://www.michelepasin.org/blog/wp-content/uploads/2014/10/streamgraph6-1024x657.png 1024w, http://www.michelepasin.org/blog/wp-content/uploads/2014/10/streamgraph6-300x192.png 300w, http://www.michelepasin.org/blog/wp-content/uploads/2014/10/streamgraph6-900x577.png 900w" sizes="(max-width: 1024px) 100vw, 1024px" /></a></p>
<p>The main visualization provides an overview of selected content within the level 2 disciplines of the <a href="http://www.nature.com/ontologies/models/subjects/">NPG Subjects Ontology</a>. By clicking on these, it is then possible to explore more specific subdisciplines and their related articles.</p>
<p><span style='text-decoration:underline;'>For those of you who are not familiar with the Subjects Ontology</span>: this is a categorization of scholarly subject areas which are used for the indexing of content on nature.com. It includes subject terms of varying levels of specificity such as Biological sciences (top level), Cancer (level 2), or B-2 cells (level 7). In total there are more than 2500 subject terms, organized into a <a href="http://www.nature.com/developers/hacks/subjects/tree">polyhierarchical tree</a>.</p>
<p>Starting in 2010, the various journals published on nature.com have adopted the subject ontology to tag their articles (note: different journals have started doing this at different times, hence some variations in the graph starting dates).</p>
<p><a href="http://www.michelepasin.org/blog/wp-content/uploads/2014/10/streamgraph22.png"><img src="http://www.michelepasin.org/blog/wp-content/uploads/2014/10/streamgraph22.png" alt="streamgraph2" width="600"  class="alignnone size-full wp-image-2764" srcset="http://www.michelepasin.org/blog/wp-content/uploads/2014/10/streamgraph22.png 864w, http://www.michelepasin.org/blog/wp-content/uploads/2014/10/streamgraph22-300x271.png 300w" sizes="(max-width: 864px) 100vw, 864px" /></a></p>
<p><a href="http://www.michelepasin.org/blog/wp-content/uploads/2014/10/streamgraph33.png"><img src="http://www.michelepasin.org/blog/wp-content/uploads/2014/10/streamgraph33-1024x472.png" alt="streamgraph3" width="600"  class="alignnone size-large wp-image-2763" srcset="http://www.michelepasin.org/blog/wp-content/uploads/2014/10/streamgraph33-1024x472.png 1024w, http://www.michelepasin.org/blog/wp-content/uploads/2014/10/streamgraph33-300x138.png 300w, http://www.michelepasin.org/blog/wp-content/uploads/2014/10/streamgraph33-900x415.png 900w, http://www.michelepasin.org/blog/wp-content/uploads/2014/10/streamgraph33.png 1591w" sizes="(max-width: 1024px) 100vw, 1024px" /></a></p>
<p>The visualization makes use of various <a href="http://d3js.org/">d3.js</a> modules, plus some simple customizations here and there. The hardest part of the work was putting the different page components together, to the effect of a more fluent &#8216;narrative&#8217; achieved by gradually zooming into the data. </p>
<p>The back end is a <a href="https://www.djangoproject.com/">Django</a> web application with a relational database. The original dataset is <a href="http://www.nature.com/ontologies/downloads/">published as RDF</a>, so in order to use the Django APIs I&#8217;ve recreated it as a relational model. That let me also add a few extra data fields containing search indexes (e.g. article counts per month), so to make the stream graph load faster. </p>
<p>Comments or suggestions, as always very welcome. </p>
<p>&nbsp;</p>
]]></content:encoded>
									<post-id xmlns="com-wordpress:feed-additions:1">2750</post-id>	</item>
		<item>
		<title>Another experiment with Wittgenstein&#8217;s Tractatus</title>
		<link>http://www.michelepasin.org/blog/2015/09/21/another-experiment-with-wittgensteins-tractatus/</link>
				<comments>http://www.michelepasin.org/blog/2015/09/21/another-experiment-with-wittgensteins-tractatus/#comments</comments>
				<pubDate>Mon, 21 Sep 2015 18:58:46 +0000</pubDate>
		<dc:creator><![CDATA[mikele]]></dc:creator>
				<category><![CDATA[Cultural Informatics]]></category>
		<category><![CDATA[Information Architecture]]></category>
		<category><![CDATA[digitalhumanities]]></category>
		<category><![CDATA[philosophy]]></category>
		<category><![CDATA[wittgenstein]]></category>

		<guid isPermaLink="false">http://www.michelepasin.org/blog/?p=2717</guid>
				<description><![CDATA[Spent some time hacking over the weekend. And here&#8217;s the result: a minimalist interactive version of Wittgenstein&#8217;s Tractatus. The Tractatus Logico-Philosophicus is a text I&#8217;ve worked with already in the past. This time I was intrigued by the simple yet super cool typed.js javascript library, which simulates animated typing. After testing it out a bit [&#8230;]]]></description>
								<content:encoded><![CDATA[<p>Spent some time hacking over the weekend. And here&#8217;s the result: a minimalist <a href="http://hacks.michelepasin.org/witt/onesentence">interactive</a> version of Wittgenstein&#8217;s Tractatus. </p>
<p><a href="http://hacks.michelepasin.org/witt/onesentence"><img src="http://www.michelepasin.org/blog/wp-content/uploads/2015/09/Screen-Shot-2015-09-21-at-19.52.31.png"  alt="Screen Shot 2015 09 21 at 19 52 31" border="0" width="600" style="width: 600px; margin-left:auto; margin-right:auto; " /></a></p>
<p>The Tractatus Logico-Philosophicus is a text I&#8217;ve worked with already in the <a href="http://www.michelepasin.org/blog/2012/07/08/wittgenstein-and-the-javascript-infovis-toolkit/">past</a>. </p>
<p>This time I was intrigued by the simple yet super cool <a href="http://www.mattboldt.com/demos/typed-js/">typed.js</a> javascript library, which simulates animated typing.</p>
<p><a href="http://hacks.michelepasin.org/witt/onesentence/1.11/ogden/"><img src="http://www.michelepasin.org/blog/wp-content/uploads/2015/09/Screen-Shot-2015-09-21-at-19.54.12.png"  alt="Screen Shot 2015 09 21 at 19 54 12" border="0" width="600" style="width: 600px; margin-left:auto; margin-right:auto; " /></a></p>
<p>After testing it out a bit I realised that this approach allows to focus on the text with more attention that having it all displayed at once. </p>
<p>Since the words appear one at a time, it feels more like a verbal dialogue than reading. As a consequence, also the way the meaning of the text gets perceived changes. </p>
<p>Slower, deeper. Almost like meditating. <a href="http://hacks.michelepasin.org/witt/onesentence">Try it out here</a>.</p>
<p><strong>Credits</strong></p>
<li>the <a href="http://www.mattboldt.com/demos/typed-js/">typed.js</a> javascript library.</li>
<li>the <a href="http://en.wikipedia.org/wiki/Tractatus_Logico-Philosophicus">Tractatus Logico-Philosophicus</a> by Wittgenstein</li>
<p>&nbsp;</p>
]]></content:encoded>
							<wfw:commentRss>http://www.michelepasin.org/blog/2015/09/21/another-experiment-with-wittgensteins-tractatus/feed/</wfw:commentRss>
		<slash:comments>1</slash:comments>
						<post-id xmlns="com-wordpress:feed-additions:1">2717</post-id>	</item>
		<item>
		<title>Is wikipedia a valid source of scientific knowledge?</title>
		<link>http://www.michelepasin.org/blog/2015/09/02/is-wikipedia-a-valid-source-of-scientific-knowledge/</link>
				<comments>http://www.michelepasin.org/blog/2015/09/02/is-wikipedia-a-valid-source-of-scientific-knowledge/#comments</comments>
				<pubDate>Wed, 02 Sep 2015 13:15:25 +0000</pubDate>
		<dc:creator><![CDATA[mikele]]></dc:creator>
				<category><![CDATA[Information Architecture]]></category>
		<category><![CDATA[Semantic Web]]></category>
		<category><![CDATA[citation]]></category>
		<category><![CDATA[d3]]></category>
		<category><![CDATA[nature]]></category>
		<category><![CDATA[wikipedia]]></category>

		<guid isPermaLink="false">http://www.michelepasin.org/blog/?p=2689</guid>
				<description><![CDATA[Is wikipedia a valid source of scientific knowledge? Many would say yes. Others are still quite skeptical, or maybe just cautious about it. What seems to be the case though &#8211; and this is what this post is about &#8211; is that wikipedians are increasingly including references to scientific literature, and when they do it [&#8230;]]]></description>
								<content:encoded><![CDATA[<p>Is wikipedia a valid source of scientific knowledge? Many would say <a href="http://www.nature.com/news/2008/081216/full/news.2008.1312.html">yes</a>. Others are still quite skeptical, or maybe just <a href="http://www.quora.com/How-reliable-is-Wikipedia-as-a-source-of-information-and-why">cautious</a> about it. What seems to be the case though &#8211; and this is what this post is about &#8211; is that wikipedians are increasingly including references to scientific literature, and when they do it they do it right. </p>
<p>Based on data we&#8217;ve recently <a href="http://www.nature.com/ontologies/linksets/articles/">extracted</a> from Wikipedia, it looks like that the vast majority of <strong>citations to nature.com</strong> content have been done according to the established scientific practice (i.e. using <a href="https://en.wikipedia.org/wiki/Digital_object_identifier">DOIs</a>). Which makes you think that whoever added those citations is either a scientist or has some familiarity with science. </p>
<p>In the context of the <a href="http://www.nature.com/ontologies/">nature.com ontologies portal</a> we&#8217;ve done some work aimed at surfacing links between our articles and other datasets. Wikipedia and <a href="http://wiki.dbpedia.org/">DBpedia</a> (an RDF database version of wikipedia) have come to our attention quite soon: <em>how much do wikipedia articles cite scientific content published on nature.com</em>? Also, <em>how well</em> do they cite it? </p>
<p>So here&#8217;s an <a href="http://www.nature.com/developers/hacks/wikilinks/">interactive visualization</a> that lets you see all incoming references from Wikipedia to the nature.com archive. The actual dataset is encoded in RDF and can be downloaded <a href="http://data.nature.com/downloads/latest/linksets/nq/">here</a> (look for the <em>npg-articles-dbpedia-linkset.2015-08-24.nq.tar.gz</em> file).</p>
<p><a href="http://www.nature.com/developers/hacks/wikilinks/" target="_blank"><img src="http://www.michelepasin.org/blog/wp-content/uploads/2015/09/NewImage.png" alt="NewImage" title="NewImage.png" border="0" width="599" height="338" style="" /></a></p>
<p>&nbsp;</p>
<h2>About the data</h2>
<p>In a nutshell, what we&#8217;ve done was simply extracting all mentions of either NPG DOIs or nature.com links using the wikipedia APIs (for example, see <a href="https://en.wikipedia.org/w/index.php?title=Special%3ASearch&#038;profile=default&#038;search=%2210.1038%2Fng1285%22&#038;fulltext=Search">all references to the DOI &#8220;10.1038/ng1285&#8221;</a>). </p>
<p>These links have then been validated against the nature.com articles database and encoded in RDF in two ways: a <a href="http://www.essepuntato.it/lode/http://purl.org/spar/cito">cito:isCitedBy</a> relationship links the article URI to the citing Wikipedia page, and a <a href="http://xmlns.com/foaf/spec/">foaf:topic</a> relationship links the same article URI to the corresponding DBpedia page.</p>
<p><a href="http://www.michelepasin.org/blog/wp-content/uploads/2015/09/Screen-Shot-2015-09-03-at-12.37.33-PM.png"><img src="http://www.michelepasin.org/blog/wp-content/uploads/2015/09/Screen-Shot-2015-09-03-at-12.37.33-PM.png" alt="Screen Shot 2015 09 03 at 12 37 33 PM" title="Screen Shot 2015-09-03 at 12.37.33 PM.png" border="0" width="600" /></a></p>
<p>In total there are <strong>51309 links over 145 years</strong>. </p>
<p>Quite interestingly, the vast majority of these links are explicit DOI references (only ~900 were links to nature.com without a DOI). So, it seems that people do recognize the importance of DOIs even within a loosely controlled context like wikipedia.</p>
<h2>Using the dataset</h2>
<p>Considering that for many wikipedia is become the <em>de facto</em> largest and most cited encyclopedia out there (see the articles below), this may be an interesting dataset to <strong>analyze</strong> e.g. to highlight citation patters of influential articles.</p>
<p>Also, this could become quite useful as a data source for <strong>content enrichment</strong>: the wikipedia links could be used to drive subject tagging, or they could even be presented to readers on article pages e.g. as contextual information.</p>
<p><a href="http://www.michelepasin.org/blog/wp-content/uploads/2015/09/toparticles.png"><img src="http://www.michelepasin.org/blog/wp-content/uploads/2015/09/toparticles.png" alt="Toparticles" title="toparticles.png" border="0" width="700"  /></a></p>
<p>We haven&#8217;t really had time to explore any follow up on this work, but hopefully we&#8217;ll do that soon.  </p>
<p>All of this data is open source and freely available on <a href="http://www.nature.com/ontologies/">nature.com/ontologies</a>. So if you&#8217;re reading this and have more ideas about potential uses or just want to collaborate, please do <a href="http://www.michelepasin.org/contact/">get in touch</a>!</p>
<h2>Caveats</h2>
<p>This dataset is obviously just a <strong>snaphot</strong> of wikipedia links at a specific moment in time. </p>
<p>If one were to use these data within a real-world application he&#8217;d probably want to come up with some strategy to keep it up to date (e.g. monitoring the <a href="https://meta.wikimedia.org/wiki/IRC/Channels#Recent_changes">Wikipedia IRC recent changes channel</a>).</p>
<p>Good news is, work is already happening in this space:</p>
<li><strong>CrossRef</strong> is looking at collecting citation events from Wikipedia in real time and release these data freely as part of their service e.g. see <a href="http://crosstech.crossref.org/2015/05/coming-to-you-live-from-wikipedia.html">http://crosstech.crossref.org/2015/05/coming-to-you-live-from-wikipedia.html</a></li>
<li><strong>Altmetric</strong> scans wikipedia for references too e.g. see <a href="http://nature.altmetric.com/details/961190/wikipedia">http://nature.altmetric.com/details/961190/wikipedia</a> and <a href="http://www.altmetric.com/blog/new-source-alert-wikipedia/">http://www.altmetric.com/blog/new-source-alert-wikipedia/</a>, however the source data is not freely available.</li>
<p>&nbsp;</p>
<h2>Readings</h2>
<p>Finally, here are a couple of interesting background readings I&#8217;ve found in the nature.com archive:</p>
<li><em>Wikipedia rival calls in the experts</em> (2006) <a href="http://www.nature.com/nature/journal/v443/n7111/full/443493a.html">http://www.nature.com/nature/journal/v443/n7111/full/443493a.html</a></li>
<li><em>Publish in Wikipedia or perish</em> (2008) <a href="http://www.nature.com/news/2008/081216/full/news.2008.1312.html">http://www.nature.com/news/2008/081216/full/news.2008.1312.html</a></li>
<li><em>Time to underpin Wikipedia wisdom</em> (2010) <a href="http://www.nature.com/nature/journal/v468/n7325/full/468765c.html">http://www.nature.com/nature/journal/v468/n7325/full/468765c.html</a></li>
<p><em>Enjoy</em>!</p>
<p>&nbsp;</p>
]]></content:encoded>
							<wfw:commentRss>http://www.michelepasin.org/blog/2015/09/02/is-wikipedia-a-valid-source-of-scientific-knowledge/feed/</wfw:commentRss>
		<slash:comments>2</slash:comments>
						<post-id xmlns="com-wordpress:feed-additions:1">2689</post-id>	</item>
		<item>
		<title>Italian public spending data: a review</title>
		<link>http://www.michelepasin.org/blog/2014/12/22/italian-public-spending-data/</link>
				<pubDate>Mon, 22 Dec 2014 21:45:13 +0000</pubDate>
		<dc:creator><![CDATA[mikele]]></dc:creator>
				<category><![CDATA[Information Architecture]]></category>
		<category><![CDATA[Semantic Web]]></category>
		<category><![CDATA[italy]]></category>
		<category><![CDATA[open data]]></category>
		<category><![CDATA[open democracy]]></category>
		<category><![CDATA[transparency]]></category>

		<guid isPermaLink="false">http://michelepasin.org/blog/?p=2561</guid>
				<description><![CDATA[The Italian government recently announced a new portal containing data on public spending: http://soldipubblici.gov.it. This is obviously great news; the website is still in beta though so in what follows I&#8217;d like to put forward a few (hopefully constructive) comments and desires for how it could/should be developed further. Incidentally, I recently ran into Ian [&#8230;]]]></description>
								<content:encoded><![CDATA[<p>The Italian government recently announced a new portal containing data on public spending: <a href="http://soldipubblici.gov.it">http://soldipubblici.gov.it</a>. This is obviously great news; the website is still in beta though so in what follows I&#8217;d like to put forward a few (hopefully constructive) comments and desires for how it could/should be developed further. </p>
<p>Incidentally, I recently ran into <a href="https://twitter.com/ianmakgill">Ian Makgill</a> from <a href="http://spendnetwork.com/">spendnetwork.com/</a>, a London startup funded by the <a href="http://opendatainstitute.org/start-ups/spend-network">Open Data Institute</a> which looks at using <em>open public data to create the first comprehensive and publicly available repository for government transaction data</em>.  </p>
<p><iframe src="//player.vimeo.com/video/101522232" width="500" height="281" frameborder="0" webkitallowfullscreen mozallowfullscreen allowfullscreen></iframe></p>
<p>We ended up chatting about the situation with <strong>open government data in Italy</strong>. To be honest I&#8217;m no expert on the matter but a couple of names quickly came to mind. </p>
<p><strong>First</strong>, the excellent <a href="http://www.openpolis.it/eng/">OpenPolis</a> association. Their mission is <em>to enable free access to public information on political candidates, elected representatives, and legislative activity thus promoting <a href="http://blog.openpolis.it/2014/09/29/foia-4-italy-ricordiamo-renzi-gli-impegni-preso-diritto-accesso/#more-2981">transparency</a> and the democratic participation of Italian citizens</em>. </p>
<p>One of their most successful projects is <a href="http://parlamento.openpolis.it/">Open Parliament</a> (similar in scope to <a href="http://www.theyworkforyou.com/">theyworkforyou.com</a> in the UK). More recently the <a href="http://www.openbilanci.it/">Open Bilanci</a> platform was created so to allow citizens to search&#038;compare the budgets and expenses of municipalities (local boroughs) in Italy.    </p>
<p><a href="http://michelepasin.org/blog/wp-content/uploads/2014/12/Screen-Shot-2014-12-22-at-22.44.34.png"><img src="http://michelepasin.org/blog/wp-content/uploads/2014/12/Screen-Shot-2014-12-22-at-22.44.34.png" alt="Screen Shot 2014 12 22 at 22 44 34" style="width: 600px; " border="0" width="600"  /></a></p>
<p><a href="http://michelepasin.org/blog/wp-content/uploads/2014/12/Screen-Shot-2014-12-23-at-10.52.45.png"><img src="http://michelepasin.org/blog/wp-content/uploads/2014/12/Screen-Shot-2014-12-23-at-10.52.45.png"  alt="Screen Shot 2014 12 23 at 10 52 45" border="0" width="600" style="width: 600px; margin-left:auto; margin-right:auto; " /></a></p>
<p><strong>Second</strong>, the ongoing work done by the <a href="https://okfn.org/">Open Knowledge Foundation</a>, which also has an <a href="http://it.okfn.org/">Italian charter</a>. For example one of its long-standing projects, <a href="https://openspending.org">openspending.org</a>, contains references to several <a href="https://openspending.org/search?q=italy">datasets about Italy&#8217;s public spending</a>. </p>
<p>Another useful resource is the <a href="http://it-city.census.okfn.org/">Italia open data census</a>, a community driven initiative to <em>compare the progress made by different cities and local areas in releasing Open Data</em>. </p>
<p><a href="http://michelepasin.org/blog/wp-content/uploads/2014/12/Screen-Shot-2014-12-22-at-22.42.47.png"><img src="http://michelepasin.org/blog/wp-content/uploads/2014/12/Screen-Shot-2014-12-22-at-22.42.47.png" alt="Screen Shot 2014 12 22 at 22 42 47" style="width: 600px; " border="0" width="600"  /></a></p>
<p>&nbsp;</p>
<h3>Soldipubblici.gov.it: a first look</h3>
<p>It should be clear by now that the people behind <a href="http://soldipubblici.gov.it/it/home">soldipubblici.gov.it</a> are not the only ones looking at increasing transparency and democracy by releasing open data. </p>
<p>What&#8217;s <strong>not clear at all</strong> though, is whether these different groups are <strong>talking to each other</strong> &#8211; which would seem the most obvious thing to do before embarking on a new enterprise like this. Especially since soldipubblici.gov.it is strikingly <strong>similar</strong> (in scope) to the aforementioned <a href="http://www.openbilanci.it/">Open Bilanci</a> portal. I&#8217;m sure that both the folks at the OKFN and openpolis.org would be interested in getting their hands on these data so to integrate them with their existing services.</p>
<p>Nonetheless, it&#8217;s great to hear that more is happening in this space. Even more so because it&#8217;s the Italian government who&#8217;s taking responsibility for it this (as it <span style='text-decoration:underline;'>should</span> be).  </p>
<p><a href="http://michelepasin.org/blog/wp-content/uploads/2014/12/Screen-Shot-2014-12-23-at-11.14.52.png"><img src="http://michelepasin.org/blog/wp-content/uploads/2014/12/Screen-Shot-2014-12-23-at-11.14.52.png"  alt="Screen Shot 2014 12 23 at 11 14 52" border="0" width="600" style="width: 600px; margin-left:auto; margin-right:auto; " /></a></p>
<p>That&#8217;s the good news. The bad news is that, from a data perspective, <strong>there isn&#8217;t much you can do with the <a href="http://soldipubblici.gov.it/">soldipubblici.gov.it</a></strong> in this beta release. If one wants to put these data to use there are various key elements missing I believe. Here&#8217;s a few ideas:  </p>
<li><strong>There is no way to <span style='text-decoration:underline;'>browse</span>/<span style='text-decoration:underline;'>review</span> the data</strong>. Search is good, but if you have no idea what to search for (e.g. simply because you don&#8217;t know what&#8217;s it called), then you&#8217;re fundamentally stuck. The system actually features a more advanced &#8216;semantic&#8217; search, which essentially augments the scope of the keywords you put in via synonyms and related terms. That&#8217;s nice, but that&#8217;s also no substitute for a good old days yellow-pages-like categories browser. You know, just to get the hang of what&#8217;s in the box before opening it.</li>
<li><strong>You can&#8217;t <span style='text-decoration:underline;'>download</span> the data</strong>. To be fair, the FAQs clearly state that this feature is still being worked on. Fine &#8211; I guess they&#8217;re talking about some nifty mechanism to select-collect-&#038;-download specific datasets one is interested in. However I do wonder why one cannot download the entire dataset already.  At the end of the day, that data is <strong>A)</strong> already made available via the current user interface; <strong>B)</strong> public and (in theory) already available on a different website called <a href="https://www.siope.it/opendata/siopeopendata.htm">SIOPE</a> (available is a big word though &#8211; I should probably say <a href="http://blog.openpolis.it/2014/06/06/opensiope-cogliamo-unopportunita-non-puo-essere-sprecata/">buried</a>).</li>
<li><strong>The visualisation app is nice but very <span style='text-decoration:underline;'>limited</span></strong>. Data doesn&#8217;t become information unless you give it some meaningful context. This tool is a great idea but it&#8217;d be enormously more useful if you could decide yourself what to plot on the graph (e.g. which years, of which data sets) depending on your research questions i.e. your context. Moreover, you want to be able to make comparisons between different datasets etc etc.. All things that <a href="http://www.openbilanci.it/confronti/milano-comune-mi/napoli-comune-na/entrate/consuntivo-entrate-cassa-imposte-e-tasse">Open Bilanci does already pretty well</a>.</li>
<li><strong>There&#8217;s no data about the <span style='text-decoration:underline;'>beneficiaries</span> of the public expenses</strong>. Not sure what the challenges are here, or whether this is feasible at all. But it&#8217;d be great to have this extra piece of information, for transparency&#8217;s sake. For example, on spendnetwork.com you can easily see which are the list of <a href="https://spendnetwork.com/entity_spend/E5036_ELBC_gov">suppliers for the London Borough Council of Ealing expenses</a>.</li>
<p>&nbsp;</p>
<h3>Conclusion: how serious do you want to be about open data? </h3>
<p>This is an inspiring start and I can&#8217;t wait to see it being developed further. Especially if it gets developed with real end-users in mind!<br />
To that end, it&#8217;s useful to bring up what the OKFN has declared to be the key features of <a href="https://okfn.org/opendata/">data openness</a>:</p>
<blockquote>
<li><strong>Availability and access</strong>: the data must be available as a whole and at no more than a reasonable reproduction cost, preferably by downloading over the internet. The data must also be available in a convenient and modifiable form.</li>
<li><strong>Reuse and redistribution</strong>: the data must be provided under terms that permit reuse and redistribution including the intermixing with other datasets. The data must be machine-readable.
</li>
<li><strong>Universal participation</strong>: everyone must be able to use, reuse and redistribute — there should be no discrimination against fields of endeavour or against persons or groups. For example, ‘non-commercial’ restrictions that would prevent ‘commercial’ use, or restrictions of use for certain purposes (e.g. only in education), are not allowed.</li>
</blockquote>
<p>&nbsp;</p>
<p>Personally, I can&#8217;t stress strongly enough how useful it&#8217;d be being able to access the <strong>raw data</strong>. Excel, CSV, a <a href="http://en.wikipedia.org/wiki/Representational_state_transfer">REST API</a> or even better a <a href="http://linkeddata.org/guides-and-tutorials">Linked Data API</a>.<br />
A data-level access point would turn this nice-looking but essentially <a href="http://en.wikipedia.org/wiki/Information_silo">siloed website</a> into an open resource which thousands of <a href="http://en.wikipedia.org/wiki/Data_journalism">data journalists</a> or data scientists (of <a href="http://www.oreilly.com/data/free/">any kind</a>) could build upon.</p>
<p>Are you looking forward to see this happen? I do!</p>
<p>&nbsp;</p>
]]></content:encoded>
									<post-id xmlns="com-wordpress:feed-additions:1">2561</post-id>	</item>
	</channel>
</rss>
