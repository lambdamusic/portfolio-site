#!/usr/bin/env bash


SECONDS=0

echo "================="
echo "PREREQUISITE"
echo "Ensure site is running on 127.0.0.1:8000..."
echo "================="


if lsof -Pi :8000 -sTCP:LISTEN -t >/dev/null ; then
    echo ">>>>>>> DJANGO is RUNNING! "
    echo "================="
else
    echo "********* Django not running **********"
    echo ".. Exiting process. Please run the django server first."
    exit 1
fi



#
# Folders ignored from build:
# - assets: managed manually for static files 
# - archived: contains legacy content which is unchanged 
#



#
# prep
#


DIR_BUILD="build"
DIR_DOCS="docs"
DIR_MD="/Users/michele.pasin/Dropbox/Apps/NVALT/001/Blog"

DIR_ROOT_BACKUP="backups/$(date +"%Y_%m_%d__%I_%M_%S_%p")"
DIR_DOCS_BACKUP="$DIR_ROOT_BACKUP/docs"
DIR_MD_BACKUP="$DIR_ROOT_BACKUP/md"
DIR_DB_BACKUP="$DIR_ROOT_BACKUP/data"


#
# backup DOCS SITE
#
echo -e "\nCREATE BACKUP DIRS in: $DIR_ROOT_BACKUP/" 
echo "+++++++++++++++++"

mkdir $DIR_ROOT_BACKUP
mkdir $DIR_DOCS_BACKUP
mkdir $DIR_MD_BACKUP
mkdir $DIR_DB_BACKUP

echo -e "\nBACKING UP SITE....RSYNC /DOCS TO /$DIR_DOCS_BACKUP"
echo "+++++++++++++++++"
# the legacy /blog static files don't need to be refreshed
rsync -av --delete --exclude archived --exclude assets $DIR_DOCS/ $DIR_DOCS_BACKUP/



#
# backup DB data
#

echo -e "\nBACKING UP DB DATA ....src/manage.py dumpdata TO /$DIR_DB_BACKUP"
echo "+++++++++++++++++"

FILENAME="$DIR_DB_BACKUP/dump.json"
src/manage.py dumpdata --exclude auth.permission --exclude contenttypes --exclude sessions --indent=4 > $FILENAME

echo -e "\nDONE: $FILENAME\n"



#
# backup MD source files data
#

echo -e "\nBACKING UP MARKDOWN FILES from $DIR_MD TO /$DIR_MD_BACKUP"
echo "+++++++++++++++++"

cp -R $DIR_MD/* $DIR_MD_BACKUP/
echo -e "\nDONE: $DIR_MD_BACKUP\n"




#
# WGET HTML site from local Django 
#

echo -e "\nCLEAN UP TEMP BUILD DIR.."
echo "+++++++++++++++++"
rm -rf $DIR_BUILD/*

#
# WGET
# https://www.gnu.org/software/wget/manual/wget.html
# opt: -m, --mirror: Makes (among other things) the download recursive.
# opt: -k, --convert-links: convert all the links (also to stuff like CSS stylesheets) to relative, so it will be suitable for offline viewing.
# opt: -E, --adjust-extension: Adds suitable extensions to filenames (`html` or `css`) depending on their content-type.
# opt: -p, --page-requisites: Download things like CSS style-sheets and images required to properly display the page offline.
# opt: -np, --no-parent: When recursing do not ascend to the parent directory. It useful for restricting the download to only a portion of the site.
# opt: -nH, --no-host-directories Disable generation of host-prefixed directories. By default, invoking Wget with ‘-r http://fly.srk.fer.hr/’ will create a structure of directories beginning with fly.srk.fer.hr/. This option disables such behavior.
# opt: -w, --wait: Waiting time between calls. Set reasonable or random waiting times between two downloads to avoid the Connection closed by peer error.
#
echo -e "\nWGET SITE INTO TEMP BUILD DIRECTORY.." 
echo "+++++++++++++++++"
wget  -P $DIR_BUILD -mkEpnp -nH http://127.0.0.1:8000/ --wait=0.1 --exclude-domains www.michelepasin.org

# add a log file
cd $DIR_BUILD
touch "$(date +"%Y_%m_%d_%I_%M_%p").log"
cd ..


#
# RSync HTML site to docs folder
# https://download.samba.org/pub/rsync/rsync.html
# https://devhints.io/rsync
# opt: -av: recursive and verbose
# opt: --delete: delete files in destination that are not in source
# opt: --checksum: checksum files to see if they have changed (instead of comparing file sizes / modification times)
# opt: --exclude: exclude files from sync
#
echo -e "\nRSYNC TEMP BUILD DIRECTORY INTO FINAL LOCATION:  /$DIR_DOCS"
echo "+++++++++++++++++"
rsync -av --delete --checksum --exclude '.nojekyll' --exclude 'CNAME' --exclude archived --exclude assets $DIR_BUILD/ $DIR_DOCS/





# https://stackoverflow.com/questions/8903239/how-to-calculate-time-elapsed-in-bash-script
echo "+++++++++++++++++ SITE DUMP COMPLETED"
duration=$SECONDS
echo "$(($duration / 60)) minutes and $(($duration % 60)) seconds elapsed."